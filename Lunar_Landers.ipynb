{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8f3FxonzCOl3wOl1hHC5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarathKumarpm/Multi_Agent_Lunar_Landing_Sim/blob/main/Lunar_Landers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium==0.29.1 box2d-py==2.3.5 pygame==2.5.0 pettingzoo==1.24.3 stable-baselines3==2.2.1 supersuit==3.9.1 opencv-python==4.9.0.80"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG26h_HEOIF2",
        "outputId": "e5dfac52-1615-4580-ef91-6f9021aae7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.5.0\n",
            "  Downloading pygame-2.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting pettingzoo==1.24.3\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting stable-baselines3==2.2.1\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting supersuit==3.9.1\n",
            "  Downloading SuperSuit-3.9.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting opencv-python==4.9.0.80\n",
            "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.2.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.2.1) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.2.1) (3.10.0)\n",
            "Collecting tinyscaler>=1.2.6 (from supersuit==3.9.1)\n",
            "  Downloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->stable-baselines3==2.2.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3==2.2.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3==2.2.1) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.2.1) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.2.1) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.2.1) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.2.1) (3.0.2)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SuperSuit-3.9.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Box2D swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2dCc1jWUPgr",
        "outputId": "160f9cdc-f434-44d2-8d5b-97e34d2cd05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig, Box2D\n",
            "Successfully installed Box2D-2.3.10 swig-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDzY44esUYmv",
        "outputId": "7dcffa7d-79d8-4b07-bb61-a0300c68f4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pettingzoo\n",
            "Successfully installed pettingzoo-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OzNg_3VUetS",
        "outputId": "ec14d510-9306-4f77-9f35-4756d236611d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gymnasium-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3qXZEMxUnMU",
        "outputId": "5eb4617e-7dd6-4995-fe4f-025f71c9f362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supersuit\n",
            "  Downloading SuperSuit-3.9.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.0.0)\n",
            "Collecting tinyscaler>=1.2.6 (from supersuit)\n",
            "  Downloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n",
            "Downloading SuperSuit-3.9.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tinyscaler, supersuit\n",
            "Successfully installed supersuit-3.9.3 tinyscaler-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aJ7TDItzNxCp",
        "outputId": "705db0e4-2f5e-4f55-9f8d-0d7118c7e63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Starting training...\n",
            "Saving video to /content/videos/dual-lunar-lander-step-0-to-step-300.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-0-to-step-300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-0-to-step-300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-0-to-step-300.mp4\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 298  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 6    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024590092 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.000712     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.82e+06     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00225     |\n",
            "|    value_loss           | 3.82e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-5000-to-step-5300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-5000-to-step-5300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-5000-to-step-5300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-5000-to-step-5300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 355          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051873326 |\n",
            "|    clip_fraction        | 0.0121       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -7.99e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.05e+06     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 4.12e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 373          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021894386 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.76        |\n",
            "|    explained_variance   | 7.74e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.99e+06     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    value_loss           | 3.96e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 382          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031094782 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -0.000182    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.99e+06     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00171     |\n",
            "|    value_loss           | 3.95e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-10000-to-step-10300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-10000-to-step-10300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-10000-to-step-10300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-10000-to-step-10300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008085533 |\n",
            "|    clip_fraction        | 0.00679     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.76       |\n",
            "|    explained_variance   | 2.09e-05    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.91e+06    |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00503    |\n",
            "|    value_loss           | 3.96e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 382          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035763734 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.75        |\n",
            "|    explained_variance   | -0.000633    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.99e+06     |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.0016      |\n",
            "|    value_loss           | 3.97e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-15000-to-step-15300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-15000-to-step-15300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-15000-to-step-15300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-15000-to-step-15300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 350          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043335725 |\n",
            "|    clip_fraction        | 0.022        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.73        |\n",
            "|    explained_variance   | 1.78e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.87e+06     |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00355     |\n",
            "|    value_loss           | 3.83e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 362          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048356764 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.72        |\n",
            "|    explained_variance   | 9.05e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.9e+06      |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00215     |\n",
            "|    value_loss           | 3.86e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-20000-to-step-20300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-20000-to-step-20300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-20000-to-step-20300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-20000-to-step-20300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 350          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018029265 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 0.000167     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.04e+06     |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000852    |\n",
            "|    value_loss           | 4.04e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 358         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 62          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005223307 |\n",
            "|    clip_fraction        | 0.00547     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.71       |\n",
            "|    explained_variance   | -1.69e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.95e+06    |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00296    |\n",
            "|    value_loss           | 3.91e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074338447 |\n",
            "|    clip_fraction        | 0.00566      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.69        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.98e+06     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00488     |\n",
            "|    value_loss           | 3.99e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-25000-to-step-25300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-25000-to-step-25300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-25000-to-step-25300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-25000-to-step-25300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 357          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043958155 |\n",
            "|    clip_fraction        | 0.00962      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 1.39e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03e+06     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00315     |\n",
            "|    value_loss           | 4.12e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 364         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002259626 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.72       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.96e+06    |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.000978   |\n",
            "|    value_loss           | 3.88e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-30000-to-step-30300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-30000-to-step-30300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-30000-to-step-30300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-30000-to-step-30300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 356         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 86          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005574913 |\n",
            "|    clip_fraction        | 0.021       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.73       |\n",
            "|    explained_variance   | 4.41e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.04e+06    |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    value_loss           | 4.08e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 363         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 90          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004223191 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.72       |\n",
            "|    explained_variance   | 0.000185    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.06e+06    |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0017     |\n",
            "|    value_loss           | 4.13e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004492455 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.7        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.91e+06    |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00193    |\n",
            "|    value_loss           | 3.87e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-35000-to-step-35300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-35000-to-step-35300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-35000-to-step-35300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-35000-to-step-35300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 362         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 101         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005130076 |\n",
            "|    clip_fraction        | 0.00669     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.68       |\n",
            "|    explained_variance   | 3.58e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.91e+06    |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00343    |\n",
            "|    value_loss           | 3.83e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 105         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001388527 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.66       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.92e+06    |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00043    |\n",
            "|    value_loss           | 3.9e+06     |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-40000-to-step-40300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-40000-to-step-40300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-40000-to-step-40300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-40000-to-step-40300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 362          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058202557 |\n",
            "|    clip_fraction        | 0.0172       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.65        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.96e+06     |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -0.00398     |\n",
            "|    value_loss           | 3.93e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 117          |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043253996 |\n",
            "|    clip_fraction        | 0.00273      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.63        |\n",
            "|    explained_variance   | 4.17e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95e+06     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00207     |\n",
            "|    value_loss           | 4.01e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 122          |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018689199 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.94e+06     |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.00108     |\n",
            "|    value_loss           | 3.8e+06      |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-45000-to-step-45300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-45000-to-step-45300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-45000-to-step-45300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-45000-to-step-45300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 365          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 128          |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028853253 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95e+06     |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.000862    |\n",
            "|    value_loss           | 3.9e+06      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 133          |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060089747 |\n",
            "|    clip_fraction        | 0.023        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.62        |\n",
            "|    explained_variance   | 4.17e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.94e+06     |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.004       |\n",
            "|    value_loss           | 3.95e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-50000-to-step-50300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-50000-to-step-50300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-50000-to-step-50300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-50000-to-step-50300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 365          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071108053 |\n",
            "|    clip_fraction        | 0.00854      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.1e+06      |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 4.23e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 144          |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034482703 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.59        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.91e+06     |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    value_loss           | 3.78e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006117792 |\n",
            "|    clip_fraction        | 0.00596     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.57       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.9e+06     |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    value_loss           | 3.8e+06     |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-55000-to-step-55300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-55000-to-step-55300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-55000-to-step-55300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-55000-to-step-55300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 155         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006573065 |\n",
            "|    clip_fraction        | 0.00806     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.58       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.88e+06    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0032     |\n",
            "|    value_loss           | 3.82e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 160         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008124283 |\n",
            "|    clip_fraction        | 0.0579      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.57       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.01e+06    |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.00751    |\n",
            "|    value_loss           | 3.94e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-60000-to-step-60300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-60000-to-step-60300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-60000-to-step-60300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-60000-to-step-60300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 167          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005220302 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.55        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76e+06     |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.000107    |\n",
            "|    value_loss           | 3.56e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 172          |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020634057 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.54        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.86e+06     |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.000885    |\n",
            "|    value_loss           | 3.73e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-65000-to-step-65300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-65000-to-step-65300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-65000-to-step-65300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-65000-to-step-65300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 365         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 179         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003979141 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.51       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.03e+06    |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00129    |\n",
            "|    value_loss           | 4e+06       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 184         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008296193 |\n",
            "|    clip_fraction        | 0.00854     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.45       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.9e+06     |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00274    |\n",
            "|    value_loss           | 3.81e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 188         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004248225 |\n",
            "|    clip_fraction        | 0.0119      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.46       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.89e+06    |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00255    |\n",
            "|    value_loss           | 3.81e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-70000-to-step-70300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-70000-to-step-70300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-70000-to-step-70300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-70000-to-step-70300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 195          |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075010434 |\n",
            "|    clip_fraction        | 0.0021       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.46        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.91e+06     |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 3.76e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 199          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064443923 |\n",
            "|    clip_fraction        | 0.0153       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.38        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.86e+06     |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.00397     |\n",
            "|    value_loss           | 3.75e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-75000-to-step-75300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-75000-to-step-75300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-75000-to-step-75300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-75000-to-step-75300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 206         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005605803 |\n",
            "|    clip_fraction        | 0.0208      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.35       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.99e+06    |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    value_loss           | 4e+06       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 211          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010091062 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.33        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.94e+06     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.00029     |\n",
            "|    value_loss           | 3.87e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 215          |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026854693 |\n",
            "|    clip_fraction        | 0.00488      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.34        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.93e+06     |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.00175     |\n",
            "|    value_loss           | 3.84e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-80000-to-step-80300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-80000-to-step-80300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-80000-to-step-80300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-80000-to-step-80300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 222          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021469237 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.35        |\n",
            "|    explained_variance   | -3.58e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95e+06     |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.00113     |\n",
            "|    value_loss           | 3.94e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 226          |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019967076 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.35        |\n",
            "|    explained_variance   | 4.17e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.92e+06     |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    value_loss           | 3.78e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-85000-to-step-85300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-85000-to-step-85300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-85000-to-step-85300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-85000-to-step-85300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 233          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041902615 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.38        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.97e+06     |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.00338     |\n",
            "|    value_loss           | 3.97e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 237          |\n",
            "|    total_timesteps      | 88064        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039877966 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.99e+06     |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | -0.0036      |\n",
            "|    value_loss           | 4.03e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 242         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008920935 |\n",
            "|    clip_fraction        | 0.0106      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.44       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.99e+06    |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00319    |\n",
            "|    value_loss           | 4e+06       |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-90000-to-step-90300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-90000-to-step-90300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-90000-to-step-90300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-90000-to-step-90300.mp4\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 369           |\n",
            "|    iterations           | 45            |\n",
            "|    time_elapsed         | 249           |\n",
            "|    total_timesteps      | 92160         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00092829566 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.46         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.79e+06      |\n",
            "|    n_updates            | 440           |\n",
            "|    policy_gradient_loss | -0.000146     |\n",
            "|    value_loss           | 3.53e+06      |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 371        |\n",
            "|    iterations           | 46         |\n",
            "|    time_elapsed         | 253        |\n",
            "|    total_timesteps      | 94208      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00934891 |\n",
            "|    clip_fraction        | 0.03       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.43      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.88e+06   |\n",
            "|    n_updates            | 450        |\n",
            "|    policy_gradient_loss | -0.00446   |\n",
            "|    value_loss           | 3.76e+06   |\n",
            "----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-95000-to-step-95300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-95000-to-step-95300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-95000-to-step-95300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-95000-to-step-95300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 261         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004921012 |\n",
            "|    clip_fraction        | 0.0172      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.39       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.86e+06    |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    value_loss           | 3.79e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 265         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003764095 |\n",
            "|    clip_fraction        | 0.00132     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.33       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.88e+06    |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00163    |\n",
            "|    value_loss           | 3.79e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-100000-to-step-100300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-100000-to-step-100300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-100000-to-step-100300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-100000-to-step-100300.mp4\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 366        |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 273        |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00590299 |\n",
            "|    clip_fraction        | 0.00928    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.27      |\n",
            "|    explained_variance   | 2.98e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.86e+06   |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.00305   |\n",
            "|    value_loss           | 3.82e+06   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 368        |\n",
            "|    iterations           | 50         |\n",
            "|    time_elapsed         | 277        |\n",
            "|    total_timesteps      | 102400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00601382 |\n",
            "|    clip_fraction        | 0.0125     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.2       |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.86e+06   |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.00284   |\n",
            "|    value_loss           | 3.69e+06   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 281          |\n",
            "|    total_timesteps      | 104448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031235474 |\n",
            "|    clip_fraction        | 0.00527      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.18        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.75e+06     |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    value_loss           | 3.55e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-105000-to-step-105300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-105000-to-step-105300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-105000-to-step-105300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-105000-to-step-105300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 289          |\n",
            "|    total_timesteps      | 106496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011845047 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.19        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.78e+06     |\n",
            "|    n_updates            | 510          |\n",
            "|    policy_gradient_loss | -0.000623    |\n",
            "|    value_loss           | 3.63e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 293          |\n",
            "|    total_timesteps      | 108544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048130173 |\n",
            "|    clip_fraction        | 0.011        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.16        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.91e+06     |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    value_loss           | 3.82e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-110000-to-step-110300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-110000-to-step-110300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-110000-to-step-110300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-110000-to-step-110300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 300         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004140714 |\n",
            "|    clip_fraction        | 0.00767     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.17       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.82e+06    |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0021     |\n",
            "|    value_loss           | 3.63e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 304         |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009012853 |\n",
            "|    clip_fraction        | 0.028       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.17       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.81e+06    |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00617    |\n",
            "|    value_loss           | 3.65e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 309          |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045936015 |\n",
            "|    clip_fraction        | 0.00239      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.15        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.84e+06     |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 3.66e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-115000-to-step-115300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-115000-to-step-115300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-115000-to-step-115300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-115000-to-step-115300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 316          |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032773092 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.15        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.86e+06     |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 3.62e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 320          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050570155 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.17        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.88e+06     |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 3.93e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-120000-to-step-120300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-120000-to-step-120300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-120000-to-step-120300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-120000-to-step-120300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 327         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008768968 |\n",
            "|    clip_fraction        | 0.0329      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.21       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.85e+06    |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00511    |\n",
            "|    value_loss           | 3.71e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 331          |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015511492 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.84e+06     |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.000734    |\n",
            "|    value_loss           | 3.66e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 336          |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009163971 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.2         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76e+06     |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.000634    |\n",
            "|    value_loss           | 3.68e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-125000-to-step-125300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-125000-to-step-125300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-125000-to-step-125300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-125000-to-step-125300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 342         |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008262088 |\n",
            "|    clip_fraction        | 0.00239     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.19       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.8e+06     |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00316    |\n",
            "|    value_loss           | 3.59e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 347         |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004333644 |\n",
            "|    clip_fraction        | 0.000439    |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.12       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.72e+06    |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00156    |\n",
            "|    value_loss           | 3.5e+06     |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-130000-to-step-130300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-130000-to-step-130300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-130000-to-step-130300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-130000-to-step-130300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 354          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034866931 |\n",
            "|    clip_fraction        | 0.00449      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.1         |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.78e+06     |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.00218     |\n",
            "|    value_loss           | 3.57e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 358          |\n",
            "|    total_timesteps      | 133120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020168861 |\n",
            "|    clip_fraction        | 0.000146     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.09        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.83e+06     |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 3.63e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 363         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004258447 |\n",
            "|    clip_fraction        | 0.00601     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.1        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.84e+06    |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00165    |\n",
            "|    value_loss           | 3.74e+06    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-135000-to-step-135300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-135000-to-step-135300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-135000-to-step-135300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-135000-to-step-135300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 369         |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002621011 |\n",
            "|    clip_fraction        | 0.00283     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.17       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.91e+06    |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.00165    |\n",
            "|    value_loss           | 3.84e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 374          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027687063 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.81e+06     |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.000841    |\n",
            "|    value_loss           | 3.63e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-140000-to-step-140300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-140000-to-step-140300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-140000-to-step-140300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-140000-to-step-140300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 380          |\n",
            "|    total_timesteps      | 141312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044076657 |\n",
            "|    clip_fraction        | 0.0135       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.28        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.77e+06     |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    value_loss           | 3.61e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 385          |\n",
            "|    total_timesteps      | 143360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011753011 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.27        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95e+06     |\n",
            "|    n_updates            | 690          |\n",
            "|    policy_gradient_loss | -0.000327    |\n",
            "|    value_loss           | 3.85e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-145000-to-step-145300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-145000-to-step-145300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-145000-to-step-145300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-145000-to-step-145300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 391          |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024629931 |\n",
            "|    clip_fraction        | 0.00859      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.29        |\n",
            "|    explained_variance   | -3.58e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.86e+06     |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.00206     |\n",
            "|    value_loss           | 3.68e+06     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 72         |\n",
            "|    time_elapsed         | 395        |\n",
            "|    total_timesteps      | 147456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00541941 |\n",
            "|    clip_fraction        | 0.00283    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.31      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.74e+06   |\n",
            "|    n_updates            | 710        |\n",
            "|    policy_gradient_loss | -0.00246   |\n",
            "|    value_loss           | 3.55e+06   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 373          |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 400          |\n",
            "|    total_timesteps      | 149504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012173217 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.29        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.73e+06     |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.000655    |\n",
            "|    value_loss           | 3.49e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-150000-to-step-150300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-150000-to-step-150300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-150000-to-step-150300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-150000-to-step-150300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 406          |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029839221 |\n",
            "|    clip_fraction        | 0.000586     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.31        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.64e+06     |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | -0.00124     |\n",
            "|    value_loss           | 3.35e+06     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 75         |\n",
            "|    time_elapsed         | 411        |\n",
            "|    total_timesteps      | 153600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00430898 |\n",
            "|    clip_fraction        | 0.00327    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.3       |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.71e+06   |\n",
            "|    n_updates            | 740        |\n",
            "|    policy_gradient_loss | -0.0023    |\n",
            "|    value_loss           | 3.46e+06   |\n",
            "----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-155000-to-step-155300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-155000-to-step-155300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-155000-to-step-155300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-155000-to-step-155300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 417          |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005264539 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.25        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74e+06     |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.000115    |\n",
            "|    value_loss           | 3.52e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 422         |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008346836 |\n",
            "|    clip_fraction        | 0.013       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.2        |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.75e+06    |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00407    |\n",
            "|    value_loss           | 3.48e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 426          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006979597 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.15        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.82e+06     |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -9.68e-05    |\n",
            "|    value_loss           | 3.8e+06      |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-160000-to-step-160300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-160000-to-step-160300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-160000-to-step-160300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-160000-to-step-160300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 433         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007769554 |\n",
            "|    clip_fraction        | 0.0119      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.11       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.76e+06    |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00304    |\n",
            "|    value_loss           | 3.6e+06     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 437          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011797958 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.09        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.67e+06     |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.001       |\n",
            "|    value_loss           | 3.4e+06      |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-165000-to-step-165300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-165000-to-step-165300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-165000-to-step-165300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-165000-to-step-165300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 373          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 444          |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045132143 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.06        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.8e+06      |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00193     |\n",
            "|    value_loss           | 3.63e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 448          |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021886304 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.82e+06     |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 3.65e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 453          |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025234767 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.05        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.83e+06     |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.000934    |\n",
            "|    value_loss           | 3.64e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-170000-to-step-170300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-170000-to-step-170300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-170000-to-step-170300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-170000-to-step-170300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 459         |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001657947 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.03       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.75e+06    |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.00103    |\n",
            "|    value_loss           | 3.53e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 464          |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017568695 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.99        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.81e+06     |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.000722    |\n",
            "|    value_loss           | 3.6e+06      |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-175000-to-step-175300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-175000-to-step-175300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-175000-to-step-175300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-175000-to-step-175300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 86           |\n",
            "|    time_elapsed         | 470          |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011829833 |\n",
            "|    clip_fraction        | 0.000439     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.99        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.79e+06     |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | -0.000779    |\n",
            "|    value_loss           | 3.55e+06     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 475         |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003970801 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2          |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.79e+06    |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00147    |\n",
            "|    value_loss           | 3.58e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 480          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033111926 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.81e+06     |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | -0.00317     |\n",
            "|    value_loss           | 3.62e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-180000-to-step-180300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-180000-to-step-180300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-180000-to-step-180300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-180000-to-step-180300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 486          |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018695606 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.06        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.79e+06     |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00117     |\n",
            "|    value_loss           | 3.5e+06      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 490          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049732947 |\n",
            "|    clip_fraction        | 0.00181      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74e+06     |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.00246     |\n",
            "|    value_loss           | 3.48e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-185000-to-step-185300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-185000-to-step-185300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-185000-to-step-185300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-185000-to-step-185300.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 497         |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002226617 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.99       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.82e+06    |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.000942   |\n",
            "|    value_loss           | 3.63e+06    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 501          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039015424 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2           |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74e+06     |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    value_loss           | 3.51e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-190000-to-step-190300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-190000-to-step-190300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-190000-to-step-190300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-190000-to-step-190300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 507          |\n",
            "|    total_timesteps      | 190464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028854783 |\n",
            "|    clip_fraction        | 0.000732     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.96        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.77e+06     |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    value_loss           | 3.5e+06      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 512         |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006894408 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.91       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.77e+06    |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00219    |\n",
            "|    value_loss           | 3.6e+06     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 516         |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003692456 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.89       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.79e+06    |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.00133    |\n",
            "|    value_loss           | 3.6e+06     |\n",
            "-----------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-195000-to-step-195300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-195000-to-step-195300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-195000-to-step-195300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-195000-to-step-195300.mp4\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 375           |\n",
            "|    iterations           | 96            |\n",
            "|    time_elapsed         | 523           |\n",
            "|    total_timesteps      | 196608        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065923843 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.9          |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.74e+06      |\n",
            "|    n_updates            | 950           |\n",
            "|    policy_gradient_loss | -0.000382     |\n",
            "|    value_loss           | 3.54e+06      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 376          |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 528          |\n",
            "|    total_timesteps      | 198656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017450021 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.91        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.78e+06     |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.000546    |\n",
            "|    value_loss           | 3.59e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/videos/dual-lunar-lander-step-200000-to-step-200300.mp4\n",
            "Moviepy - Building video /content/videos/dual-lunar-lander-step-200000-to-step-200300.mp4.\n",
            "Moviepy - Writing video /content/videos/dual-lunar-lander-step-200000-to-step-200300.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/dual-lunar-lander-step-200000-to-step-200300.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 534          |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028984454 |\n",
            "|    clip_fraction        | 0.00112      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.94        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.79e+06     |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.00169     |\n",
            "|    value_loss           | 3.61e+06     |\n",
            "------------------------------------------\n",
            "Training complete!\n",
            "Episode 1 reward: -105631.8879626562\n",
            "Episode 2 reward: -8244.933357337955\n",
            "Episode 3 reward: -97619.55149115599\n",
            "Video saved to /content/dual_lunar_lander_evaluation.mp4\n",
            "Mean evaluation reward: -70498.79093705006\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/videos/dual-lunar-lander-0.mp4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7dd963262058>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# Show training video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     \u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/videos/dual-lunar-lander-0.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;31m# Show evaluation video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7dd963262058>\u001b[0m in \u001b[0;36mshow_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mmp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data:video/mp4;base64,\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         return HTML(f\"\"\"\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/videos/dual-lunar-lander-0.mp4'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import cv2\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import wrappers\n",
        "from pettingzoo.utils.conversions import parallel_to_aec_wrapper\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from supersuit import pettingzoo_env_to_vec_env_v1\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            if (contact.fixtureA.body == lander['body'] or\n",
        "                contact.fixtureB.body == lander['body']):\n",
        "                self.env.game_over = True\n",
        "                return\n",
        "\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = False\n",
        "\n",
        "class DualLunarLander(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"dual_lunar_lander_v0\",\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.possible_agents = [\"lander_1\", \"lander_2\"]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.possible_agents)}\n",
        "\n",
        "        # Physics parameters\n",
        "        self.FPS = 50\n",
        "        self.SCALE = 30.0\n",
        "        self.LEG_DOWN = 18\n",
        "        self.LEG_W, self.LEG_H = 2, 8\n",
        "        self.ENGINE_POWER = 13.0\n",
        "        self.SIDE_ENGINE_POWER = 0.6\n",
        "        self.INITIAL_RANDOM = 1000.0\n",
        "\n",
        "        # Rendering parameters\n",
        "        self.VIEWPORT_W = 600\n",
        "        self.VIEWPORT_H = 400\n",
        "        self.TERRAIN_CHUNKS = 11\n",
        "        self.TERRAIN_HEIGHT = self.VIEWPORT_H / self.SCALE / 4\n",
        "        self.TERRAIN_STEP = 1.0 / self.TERRAIN_CHUNKS\n",
        "\n",
        "        # Game state\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        # Spaces\n",
        "        self.action_spaces = {agent: spaces.Discrete(4) for agent in self.possible_agents}\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=np.array([-1, -1, -5, -5, -np.pi, -5, 0, 0], dtype=np.float32),\n",
        "                high=np.array([1, 1, 5, 5, np.pi, 5, 1, 1], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            ) for agent in self.possible_agents\n",
        "        }\n",
        "\n",
        "        # Physics engine\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.terrain = None\n",
        "        self.moon = None\n",
        "        self.ground = None\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.sync_reward_scale = 10.0\n",
        "        self.both_landed = False\n",
        "\n",
        "        # For drawing purposes\n",
        "        self.sky_color = (0, 0, 0)\n",
        "        self.moon_color = (102, 102, 102)\n",
        "        self.terrain_color = (153, 153, 153)\n",
        "        self.lander_color = [(204, 0, 0), (0, 204, 0)]  # Red for first lander, green for second\n",
        "        self.leg_color = (102, 102, 102)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self._destroy()\n",
        "        self.world = Box2D.b2World(gravity=(0, -10))\n",
        "        self.world.contactListener = ContactDetector(self)\n",
        "        self.moon = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.game_over = False\n",
        "        self.both_landed = False\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "\n",
        "        self._create_terrain()\n",
        "\n",
        "        # Create landers with increased horizontal separation\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            offset_x = -1.5 if i == 0 else 1.5  # Increased from ±0.3 to ±1.5\n",
        "            self._create_lander(agent, offset_x)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        observations = {}\n",
        "        for agent in self.agents:\n",
        "            observations[agent] = self._get_observation(agent)\n",
        "\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        if self.game_over:\n",
        "            observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "            rewards = {agent: 0 for agent in self.agents}\n",
        "            terminations = {agent: True for agent in self.agents}\n",
        "            truncations = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        # Process actions\n",
        "        for agent, action in actions.items():\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "\n",
        "            if action == 1:\n",
        "                ox = np.sin(lander['body'].angle) * self.SIDE_ENGINE_POWER\n",
        "                oy = -np.cos(lander['body'].angle) * self.ENGINE_POWER\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((ox, oy), impulse_pos, True)\n",
        "                lander['main_engine'] = True\n",
        "            else:\n",
        "                lander['main_engine'] = False\n",
        "\n",
        "            if action == 2:\n",
        "                impulse_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((-self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_left'] = True\n",
        "            else:\n",
        "                lander['side_engine_left'] = False\n",
        "\n",
        "            if action == 3:\n",
        "                impulse_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_right'] = True\n",
        "            else:\n",
        "                lander['side_engine_right'] = False\n",
        "\n",
        "        self.world.Step(1.0 / self.FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        terminations = {agent: False for agent in self.agents}\n",
        "        truncations = {agent: False for agent in self.agents}\n",
        "        infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "\n",
        "            reward = 0\n",
        "            shaping = (\n",
        "                -100 * np.sqrt(pos[0]**2 + pos[1]**2)\n",
        "                - 100 * np.sqrt(vel[0]**2 + vel[1]**2)\n",
        "                - 100 * abs(lander['body'].angle)\n",
        "                - 100 * abs(lander['body'].angularVelocity)\n",
        "            )\n",
        "            reward += (shaping - self.prev_shaping[idx]) * 0.1\n",
        "            self.prev_shaping[idx] = shaping\n",
        "\n",
        "            reward -= 0.1 if lander['main_engine'] else 0\n",
        "            reward -= 0.03 if lander['side_engine_left'] or lander['side_engine_right'] else 0\n",
        "\n",
        "            landed = False\n",
        "            if (lander['legs_contact'][0] and lander['legs_contact'][1]) and abs(vel[0]) < 0.1 and abs(vel[1]) < 0.1:\n",
        "                landed = True\n",
        "                if self.landing_rewards[agent] == 0:\n",
        "                    self.landing_rewards[agent] = 100\n",
        "                    reward += 100\n",
        "\n",
        "            if pos[0] < 0 or pos[0] > self.VIEWPORT_W / self.SCALE or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE):\n",
        "                reward -= 100\n",
        "                terminations[agent] = True\n",
        "\n",
        "            rewards[agent] = reward\n",
        "\n",
        "        all_landed = all(self.landing_rewards[agent] > 0 for agent in self.agents)\n",
        "        if all_landed and not self.both_landed:\n",
        "            sync_reward = self.sync_reward_scale * 100\n",
        "            for agent in self.agents:\n",
        "                rewards[agent] += sync_reward\n",
        "            self.both_landed = True\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "\n",
        "            if (lander['legs_contact'][0] and lander['legs_contact'][1]) and abs(vel[0]) < 0.1 and abs(vel[1]) < 0.1:\n",
        "                if all_landed:\n",
        "                    terminations[agent] = True\n",
        "            elif pos[0] < 0 or pos[0] > self.VIEWPORT_W / self.SCALE or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE):\n",
        "                terminations[agent] = True\n",
        "\n",
        "        self.game_over = all(terminations.values())\n",
        "\n",
        "        observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    def _destroy(self):\n",
        "        if self.world is None:\n",
        "            return\n",
        "\n",
        "        for lander in self.landers:\n",
        "            self.world.DestroyBody(lander['body'])\n",
        "            for leg in lander['legs']:\n",
        "                self.world.DestroyBody(leg)\n",
        "\n",
        "        if self.moon:\n",
        "            self.world.DestroyBody(self.moon)\n",
        "\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "\n",
        "    def _create_terrain(self):\n",
        "        # Create the moon surface terrain\n",
        "        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]))\n",
        "        self.moon.CreateFixture(\n",
        "            shape=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]),\n",
        "            friction=0.1,\n",
        "            restitution=0.0,\n",
        "        )\n",
        "        self.ground = self.moon\n",
        "\n",
        "        # Create a more varied terrain surface\n",
        "        self.terrain = []\n",
        "        height = self.TERRAIN_HEIGHT\n",
        "        chunk_x = [0]\n",
        "        for i in range(self.TERRAIN_CHUNKS):\n",
        "            chunk_x.append(chunk_x[-1] + self.TERRAIN_STEP)\n",
        "\n",
        "        # Create random terrain\n",
        "        height_data = []\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            h = np.random.uniform(0, height)\n",
        "            height_data.append(h)\n",
        "\n",
        "        # Create landing pads for the two landers\n",
        "        landing_pad_1_idx = self.TERRAIN_CHUNKS // 4\n",
        "        landing_pad_2_idx = 3 * self.TERRAIN_CHUNKS // 4\n",
        "\n",
        "        height_data[landing_pad_1_idx-1] = height_data[landing_pad_1_idx] = height_data[landing_pad_1_idx+1] = self.TERRAIN_HEIGHT/2\n",
        "        height_data[landing_pad_2_idx-1] = height_data[landing_pad_2_idx] = height_data[landing_pad_2_idx+1] = self.TERRAIN_HEIGHT/2\n",
        "\n",
        "        # Create terrain fixtures\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            x1 = chunk_x[i]\n",
        "            x2 = chunk_x[i+1]\n",
        "            h1 = height_data[i]\n",
        "\n",
        "            # Create ground segments\n",
        "            poly = [\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, 0),\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, 0)\n",
        "            ]\n",
        "\n",
        "            self.terrain.append(poly)\n",
        "\n",
        "            # Add terrain fixtures to the moon body\n",
        "            self.moon.CreateFixture(\n",
        "                shape=polygonShape(vertices=poly),\n",
        "                friction=0.1,\n",
        "                restitution=0.0\n",
        "            )\n",
        "\n",
        "    def _create_lander(self, agent, offset_x):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "\n",
        "        # Initial position with random disturbance\n",
        "        init_x = self.VIEWPORT_W / self.SCALE / 2 + offset_x\n",
        "        init_y = self.VIEWPORT_H / self.SCALE\n",
        "\n",
        "        # Create the lander's main body\n",
        "        lander_body = self.world.CreateDynamicBody(\n",
        "            position=(init_x, init_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(vertices=[\n",
        "                    (-0.5, 0.0),\n",
        "                    (0.5, 0.0),\n",
        "                    (0.5, -1.0),\n",
        "                    (-0.5, -1.0)\n",
        "                ]),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,\n",
        "                restitution=0.0\n",
        "            ),\n",
        "        )\n",
        "        lander_body.ApplyForceToCenter(\n",
        "            (\n",
        "                np.random.uniform(-self.INITIAL_RANDOM, self.INITIAL_RANDOM),\n",
        "                np.random.uniform(-self.INITIAL_RANDOM, self.INITIAL_RANDOM)\n",
        "            ),\n",
        "            True\n",
        "        )\n",
        "\n",
        "        # Create legs\n",
        "        legs = []\n",
        "        legs_contact = [False, False]\n",
        "\n",
        "        for i in [-1, 1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(init_x + i * 0.3, init_y - 0.45),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(vertices=[\n",
        "                        (0, 0),\n",
        "                        (0, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, 0)\n",
        "                    ]),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    friction=0.2,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            legs.append(leg)\n",
        "\n",
        "            # Create joint between lander and leg\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=lander_body,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(i * 0.3, -0.45),\n",
        "                localAnchorB=(0, 0),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=100,\n",
        "                motorSpeed=0.3 * i,\n",
        "                lowerAngle=-0.4,\n",
        "                upperAngle=0.4\n",
        "            )\n",
        "            self.world.CreateJoint(rjd)\n",
        "\n",
        "        # Store the lander information\n",
        "        self.landers.append({\n",
        "            'body': lander_body,\n",
        "            'legs': legs,\n",
        "            'legs_contact': legs_contact,\n",
        "            'main_engine': False,\n",
        "            'side_engine_left': False,\n",
        "            'side_engine_right': False\n",
        "        })\n",
        "\n",
        "    def _get_observation(self, agent):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "        lander = self.landers[idx]\n",
        "        pos = lander['body'].position\n",
        "        vel = lander['body'].linearVelocity\n",
        "\n",
        "        # Normalize coordinates\n",
        "        x = pos[0] / (self.VIEWPORT_W / self.SCALE)\n",
        "        y = pos[1] / (self.VIEWPORT_H / self.SCALE)\n",
        "\n",
        "        # Create observation vector\n",
        "        observation = np.array([\n",
        "            (pos[0] - self.VIEWPORT_W / self.SCALE / 2) / (self.VIEWPORT_W / self.SCALE / 2),\n",
        "            (pos[1] - self.VIEWPORT_H / self.SCALE / 2) / (self.VIEWPORT_H / self.SCALE / 2),\n",
        "            vel[0] / 5.0,\n",
        "            vel[1] / 5.0,\n",
        "            lander['body'].angle,\n",
        "            lander['body'].angularVelocity / 5.0,\n",
        "            1.0 if lander['legs_contact'][0] else 0.0,\n",
        "            1.0 if lander['legs_contact'][1] else 0.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _init_renderer(self):\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            pygame.display.set_caption(\"Dual Lunar Lander\")\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return None\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        # Prepare rendering surface\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen.fill(self.sky_color)\n",
        "            surf = self.screen\n",
        "        else:  # rgb_array\n",
        "            surf = pygame.Surface((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            surf.fill(self.sky_color)\n",
        "\n",
        "        # Transform for rendering\n",
        "        def transform(pos):\n",
        "            return (int(pos[0] * self.SCALE), self.VIEWPORT_H - int(pos[1] * self.SCALE))\n",
        "\n",
        "        # Draw terrain\n",
        "        for poly in self.terrain:\n",
        "            transformed_poly = [transform(p) for p in poly]\n",
        "            pygame.draw.polygon(surf, self.terrain_color, transformed_poly)\n",
        "\n",
        "        # Draw each lander and its legs\n",
        "        for i, lander in enumerate(self.landers):\n",
        "            lander_color = self.lander_color[i]\n",
        "\n",
        "            # Draw the lander body\n",
        "            vertices = []\n",
        "            for fixture in lander['body'].fixtures:\n",
        "                for vertex in fixture.shape.vertices:\n",
        "                    vertices.append(transform(lander['body'].transform * vertex))\n",
        "            pygame.draw.polygon(surf, lander_color, vertices)\n",
        "\n",
        "            # Draw legs\n",
        "            for j, leg in enumerate(lander['legs']):\n",
        "                vertices = []\n",
        "                for fixture in leg.fixtures:\n",
        "                    for vertex in fixture.shape.vertices:\n",
        "                        vertices.append(transform(leg.transform * vertex))\n",
        "                pygame.draw.polygon(surf, self.leg_color, vertices)\n",
        "\n",
        "            # Draw engines\n",
        "            if lander['main_engine']:\n",
        "                # Main engine fire\n",
        "                start_pos = transform(lander['body'].position)\n",
        "                end_pos = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * 10,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * 10\n",
        "                )\n",
        "                pygame.draw.line(surf, (255, 255, 0), start_pos, end_pos, 3)\n",
        "\n",
        "            if lander['side_engine_left']:\n",
        "                # Left engine fire\n",
        "                start_pos = transform(lander['body'].position + (-0.2, 0))\n",
        "                end_pos = (start_pos[0] - 6, start_pos[1])\n",
        "                pygame.draw.line(surf, (255, 255, 0), start_pos, end_pos, 2)\n",
        "\n",
        "            if lander['side_engine_right']:\n",
        "                # Right engine fire\n",
        "                start_pos = transform(lander['body'].position + (0.2, 0))\n",
        "                end_pos = (start_pos[0] + 6, start_pos[1])\n",
        "                pygame.draw.line(surf, (255, 255, 0), start_pos, end_pos, 2)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            return None\n",
        "        else:\n",
        "            # Convert to numpy array for rgb_array mode\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False\n",
        "            self.screen = None\n",
        "\n",
        "# Create a custom wrapper to make PettingZoo work with StableBaselines3\n",
        "class StableBaselinesWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom wrapper to make PettingZoo environments work with Stable Baselines3.\n",
        "    This wrapper takes a PettingZoo parallel environment and creates a gym-compatible interface.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper with a PettingZoo parallel environment.\n",
        "\n",
        "        Parameters:\n",
        "        - env: PettingZoo parallel environment\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "        self.num_agents = len(self.agents)\n",
        "\n",
        "        # Add this line to properly set the render_mode\n",
        "        self.render_mode = env.render_mode\n",
        "\n",
        "        # Combine action spaces from all agents\n",
        "        self.action_space = gym.spaces.MultiDiscrete([\n",
        "            self.env.action_spaces[agent].n for agent in self.agents\n",
        "        ])\n",
        "\n",
        "        # Combine observation spaces from all agents\n",
        "        obs_shape = self.env.observation_spaces[self.agents[0]].shape\n",
        "        low = np.concatenate([self.env.observation_spaces[agent].low for agent in self.agents])\n",
        "        high = np.concatenate([self.env.observation_spaces[agent].high for agent in self.agents])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=low,\n",
        "            high=high,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment and return the initial observation.\"\"\"\n",
        "        observations, _ = self.env.reset(seed=seed)\n",
        "        # Flatten observations into a single array\n",
        "        flat_obs = np.concatenate([observations[agent] for agent in self.agents])\n",
        "        return flat_obs, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Take a step in the environment using the provided actions.\n",
        "\n",
        "        Parameters:\n",
        "        - actions: numpy array of actions, one per agent\n",
        "\n",
        "        Returns:\n",
        "        - observation: flattened observations from all agents\n",
        "        - reward: sum of all agent rewards\n",
        "        - terminated: True if episode is done\n",
        "        - truncated: True if episode is truncated\n",
        "        - info: additional information\n",
        "        \"\"\"\n",
        "        # Convert flat actions to dictionary\n",
        "        action_dict = {}\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            action_dict[agent] = int(actions[i])\n",
        "\n",
        "        # Step the environment\n",
        "        observations, rewards, terminations, truncations, infos = self.env.step(action_dict)\n",
        "\n",
        "        # Flatten observations and calculate total reward\n",
        "        flat_obs = np.concatenate([observations[agent] for agent in self.agents])\n",
        "        total_reward = sum(rewards.values())\n",
        "        all_terminated = all(terminations.values())\n",
        "        all_truncated = all(truncations.values())\n",
        "\n",
        "        return flat_obs, total_reward, all_terminated, all_truncated, infos\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment.\"\"\"\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "# Updated make_env function\n",
        "def make_env():\n",
        "    # Create the parallel environment\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    # Wrap it with our StableBaselines3 wrapper\n",
        "    sb_env = StableBaselinesWrapper(env)\n",
        "    # Ensure render_mode is properly set\n",
        "    sb_env.render_mode = \"rgb_array\"\n",
        "    return sb_env\n",
        "\n",
        "# Updated train_agents function\n",
        "def train_agents():\n",
        "    # Get the environment\n",
        "    env = make_env()\n",
        "\n",
        "    # Create a dummy vec env for compatibility with SB3\n",
        "    vec_env = DummyVecEnv([lambda: env])\n",
        "\n",
        "    video_folder = \"/content/videos\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    vec_env = VecVideoRecorder(\n",
        "        vec_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda x: x % 5000 == 0,\n",
        "        video_length=300,\n",
        "        name_prefix=\"dual-lunar-lander\"\n",
        "    )\n",
        "\n",
        "    model = PPO(\n",
        "        MlpPolicy,\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        learning_rate=0.0003,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    model.learn(total_timesteps=200000)\n",
        "    print(\"Training complete!\")\n",
        "    model.save(\"ppo_dual_lunar_lander\")\n",
        "    return model, vec_env\n",
        "\n",
        "# Updated evaluate_model function\n",
        "def evaluate_model(model, env_fn, num_episodes=3):\n",
        "    \"\"\"\n",
        "    Evaluates a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PPO model\n",
        "    - env_fn: Function to create the environment\n",
        "    - num_episodes: Number of episodes to evaluate\n",
        "\n",
        "    Returns:\n",
        "    - Mean reward across episodes\n",
        "    \"\"\"\n",
        "    # Create a fresh environment for evaluation\n",
        "    env = env_fn()\n",
        "\n",
        "    # For storing frames and rewards\n",
        "    episode_rewards = []\n",
        "    frames = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        while not (done or step_count >= 1000):  # Limit steps per episode\n",
        "            # Get actions from model\n",
        "            action, _ = model.predict(obs)\n",
        "\n",
        "            # Step environment\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Record frame\n",
        "            frame = env.render()\n",
        "            if frame is not None:\n",
        "                frames.append(frame)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "        print(f\"Episode {episode+1} reward: {episode_reward}\")\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Create video from frames\n",
        "    if frames:\n",
        "        create_video(frames, \"/content/dual_lunar_lander_evaluation.mp4\")\n",
        "\n",
        "    return np.mean(episode_rewards)\n",
        "\n",
        "def create_video(frames, filename, fps=30):\n",
        "    if not frames:\n",
        "        print(\"No frames to create video\")\n",
        "        return\n",
        "\n",
        "    h, w, _ = frames[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video = cv2.VideoWriter(filename, fourcc, fps, (w, h))\n",
        "\n",
        "    for frame in frames:\n",
        "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        video.write(frame_bgr)\n",
        "    video.release()\n",
        "    print(f\"Video saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    trained_model, env = train_agents()\n",
        "\n",
        "    # Evaluate the trained model\n",
        "    mean_reward = evaluate_model(trained_model, make_env)\n",
        "    print(f\"Mean evaluation reward: {mean_reward}\")\n",
        "\n",
        "    # To display videos in Colab:\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    def show_video(video_path):\n",
        "        mp4 = open(video_path,'rb').read()\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "        return HTML(f\"\"\"\n",
        "            <video width=400 controls>\n",
        "                <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "            </video>\n",
        "        \"\"\")\n",
        "\n",
        "    # Show training video\n",
        "    show_video(\"/content/videos/dual-lunar-lander-0.mp4\")\n",
        "\n",
        "    # Show evaluation video\n",
        "    show_video(\"/content/dual_lunar_lander_evaluation.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import cv2\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import wrappers\n",
        "from pettingzoo.utils.conversions import parallel_to_aec_wrapper, aec_to_parallel_wrapper\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "# ... [Keep ContactDetector and DualLunarLander classes exactly as provided] ...import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            if (contact.fixtureA.body == lander['body'] or\n",
        "                contact.fixtureB.body == lander['body']):\n",
        "                other_body = contact.fixtureB.body if contact.fixtureA.body == lander['body'] else contact.fixtureA.body\n",
        "                if other_body == self.env.ground and lander['body'].linearVelocity.length > 2.0:\n",
        "                    self.env.game_over = True\n",
        "                    lander['crashed'] = True\n",
        "                    return\n",
        "\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = True\n",
        "                        contact_pos = None\n",
        "                        if contact.fixtureA.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "                        elif contact.fixtureB.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "\n",
        "                        if contact_pos is not None:\n",
        "                            lander['on_safe_zone'] = self.env.check_on_safe_zone(contact_pos, i)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = False\n",
        "                        if not lander['legs_contact'][0] or not lander['legs_contact'][1]:\n",
        "                            lander['on_safe_zone'] = False\n",
        "\n",
        "class DualLunarLander(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"dual_lunar_lander_v0\",\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.possible_agents = [\"lander_1\", \"lander_2\"]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.possible_agents)}\n",
        "\n",
        "        # Physics parameters\n",
        "        self.FPS = 50\n",
        "        self.SCALE = 30.0\n",
        "        self.LEG_DOWN = 18\n",
        "        self.LEG_W, self.LEG_H = 2, 8\n",
        "        self.ENGINE_POWER = 10.0\n",
        "        self.SIDE_ENGINE_POWER = 0.4\n",
        "        self.INITIAL_RANDOM = 500.0\n",
        "\n",
        "        # Rendering parameters\n",
        "        self.VIEWPORT_W = 600\n",
        "        self.VIEWPORT_H = 400\n",
        "        self.TERRAIN_CHUNKS = 15\n",
        "        self.TERRAIN_HEIGHT = self.VIEWPORT_H / self.SCALE / 4\n",
        "        self.TERRAIN_STEP = 1.0 / self.TERRAIN_CHUNKS\n",
        "        self.SAFE_ZONE_WIDTH = 2.0\n",
        "        self.safe_zones = []\n",
        "        self.SAFE_LANDING_VEL = 0.8\n",
        "\n",
        "        # Game state\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        # Spaces\n",
        "        self.action_spaces = {agent: spaces.Discrete(4) for agent in self.possible_agents}\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=np.array([-1, -1, -5, -5, -np.pi, -5, 0, 0, -1, -1, 0], dtype=np.float32),\n",
        "                high=np.array([1, 1, 5, 5, np.pi, 5, 1, 1, 1, 1, 1], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            ) for agent in self.possible_agents\n",
        "        }\n",
        "\n",
        "        # Physics engine\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.terrain = None\n",
        "        self.moon = None\n",
        "        self.ground = None\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.sync_reward_scale = 10.0\n",
        "        self.both_landed = False\n",
        "\n",
        "        # Colors\n",
        "        self.sky_color = (0, 0, 0)\n",
        "        self.moon_color = (102, 102, 102)\n",
        "        self.terrain_color = (153, 153, 153)\n",
        "        self.safe_zone_color = (0, 204, 0)\n",
        "        self.lander_color = [(204, 0, 0), (0, 0, 204)]\n",
        "        self.leg_color = (102, 102, 102)\n",
        "        self.engine_color = (255, 255, 0)\n",
        "\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def check_on_safe_zone(self, x_pos, lander_idx):\n",
        "        for i, (left, right) in enumerate(self.safe_zones):\n",
        "            if i == lander_idx and left <= x_pos <= right:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self._destroy()\n",
        "        self.world = Box2D.b2World(gravity=(0, -10))\n",
        "        self.world.contactListener = ContactDetector(self)\n",
        "        self.moon = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.game_over = False\n",
        "        self.both_landed = False\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.safe_zones = []\n",
        "\n",
        "        self._create_terrain()\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            safe_zone_center = (self.safe_zones[i][0] + self.safe_zones[i][1]) / 2\n",
        "            self._create_lander(agent, safe_zone_center)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        observations = {}\n",
        "        for agent in self.agents:\n",
        "            observations[agent] = self._get_observation(agent)\n",
        "\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        if self.game_over:\n",
        "            observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "            rewards = {agent: 0 for agent in self.agents}\n",
        "            terminations = {agent: True for agent in self.agents}\n",
        "            truncations = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        for agent, action in actions.items():\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "\n",
        "            if action == 1:\n",
        "                ox = np.sin(lander['body'].angle) * self.SIDE_ENGINE_POWER\n",
        "                oy = -np.cos(lander['body'].angle) * self.ENGINE_POWER\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((ox, oy), impulse_pos, True)\n",
        "                lander['main_engine'] = True\n",
        "            else:\n",
        "                lander['main_engine'] = False\n",
        "\n",
        "            if action == 2:\n",
        "                impulse_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((-self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_left'] = True\n",
        "            else:\n",
        "                lander['side_engine_left'] = False\n",
        "\n",
        "            if action == 3:\n",
        "                impulse_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_right'] = True\n",
        "            else:\n",
        "                lander['side_engine_right'] = False\n",
        "\n",
        "        self.world.Step(1.0 / self.FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        terminations = {agent: False for agent in self.agents}\n",
        "        truncations = {agent: False for agent in self.agents}\n",
        "        infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "            vel_len = vel.length\n",
        "            angle = lander['body'].angle\n",
        "            angular_vel = lander['body'].angularVelocity\n",
        "\n",
        "            reward = 0\n",
        "            safe_zone_center = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "            distance_to_safe_zone = abs(pos[0] - safe_zone_center)\n",
        "\n",
        "            shaping = (\n",
        "                -100 * distance_to_safe_zone\n",
        "                - 100 * abs(vel[0])\n",
        "                - 100 * abs(vel[1])\n",
        "                - 100 * abs(angle)\n",
        "                - 100 * abs(angular_vel)\n",
        "            )\n",
        "\n",
        "            reward += (shaping - self.prev_shaping[idx]) * 0.1\n",
        "            self.prev_shaping[idx] = shaping\n",
        "\n",
        "            reward -= 0.1 if lander['main_engine'] else 0\n",
        "            reward -= 0.03 if lander['side_engine_left'] or lander['side_engine_right'] else 0\n",
        "\n",
        "            landed = False\n",
        "            if (lander['legs_contact'][0] and lander['legs_contact'][1]) and vel_len < self.SAFE_LANDING_VEL and abs(angle) < 0.1:\n",
        "                landed = True\n",
        "\n",
        "                if lander['on_safe_zone']:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[agent] = 200\n",
        "                        reward += 200\n",
        "                        if vel_len < 0.3:\n",
        "                            reward += 50\n",
        "                else:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[agent] = 50\n",
        "                        reward += 50\n",
        "\n",
        "            if lander.get('crashed', False) or pos[0] < 0 or pos[0] > self.VIEWPORT_W / self.SCALE or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE):\n",
        "                reward -= 100\n",
        "                terminations[agent] = True\n",
        "                if not 0 <= pos[1] < self.VIEWPORT_H / self.SCALE:\n",
        "                    lander['outside_bounds'] = True\n",
        "\n",
        "            rewards[agent] = reward\n",
        "\n",
        "        all_landed_safely = all(self.landing_rewards[agent] >= 100 for agent in self.agents)\n",
        "        if all_landed_safely and not self.both_landed:\n",
        "            sync_reward = self.sync_reward_scale * 200\n",
        "            for agent in self.agents:\n",
        "                rewards[agent] += sync_reward\n",
        "            self.both_landed = True\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "\n",
        "            if lander.get('crashed', False) or lander.get('outside_bounds', False):\n",
        "                terminations[agent] = True\n",
        "            elif (lander['legs_contact'][0] and lander['legs_contact'][1]) and vel.length < self.SAFE_LANDING_VEL:\n",
        "                if lander['on_safe_zone'] and all_landed_safely:\n",
        "                    terminations[agent] = True\n",
        "\n",
        "        self.game_over = all(terminations.values())\n",
        "\n",
        "        observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    def _destroy(self):\n",
        "        if self.world is None:\n",
        "            return\n",
        "\n",
        "        for lander in self.landers:\n",
        "            self.world.DestroyBody(lander['body'])\n",
        "            for leg in lander['legs']:\n",
        "                self.world.DestroyBody(leg)\n",
        "\n",
        "        if self.moon:\n",
        "            self.world.DestroyBody(self.moon)\n",
        "\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "\n",
        "    def _create_terrain(self):\n",
        "        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]))\n",
        "        self.moon.CreateFixture(\n",
        "            shape=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]),\n",
        "            friction=0.1,\n",
        "            restitution=0.0,\n",
        "        )\n",
        "        self.ground = self.moon\n",
        "\n",
        "        self.terrain = []\n",
        "        height = self.TERRAIN_HEIGHT\n",
        "        chunk_x = [0]\n",
        "        for i in range(self.TERRAIN_CHUNKS):\n",
        "            chunk_x.append(chunk_x[-1] + self.TERRAIN_STEP)\n",
        "\n",
        "        height_data = []\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            h = np.random.uniform(0, height)\n",
        "            height_data.append(h)\n",
        "\n",
        "        landing_pad_1_idx = self.TERRAIN_CHUNKS // 4\n",
        "        landing_pad_2_idx = 3 * self.TERRAIN_CHUNKS // 4\n",
        "\n",
        "        pad_height = self.TERRAIN_HEIGHT/4\n",
        "        pad_width = 3\n",
        "\n",
        "        for i in range(landing_pad_1_idx - pad_width//2, landing_pad_1_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        for i in range(landing_pad_2_idx - pad_width//2, landing_pad_2_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        pad1_left = (landing_pad_1_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad1_right = (landing_pad_1_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        pad2_left = (landing_pad_2_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad2_right = (landing_pad_2_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        self.safe_zones = [(pad1_left, pad1_right), (pad2_left, pad2_right)]\n",
        "\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            x1 = chunk_x[i]\n",
        "            x2 = chunk_x[i+1]\n",
        "            h1 = height_data[i]\n",
        "\n",
        "            poly = [\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, 0),\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, 0)\n",
        "            ]\n",
        "\n",
        "            self.terrain.append((poly,\n",
        "                               (landing_pad_1_idx - pad_width//2 <= i <= landing_pad_1_idx + pad_width//2) or\n",
        "                               (landing_pad_2_idx - pad_width//2 <= i <= landing_pad_2_idx + pad_width//2)))\n",
        "\n",
        "            self.moon.CreateFixture(\n",
        "                shape=polygonShape(vertices=poly),\n",
        "                friction=0.4,\n",
        "                restitution=0.0\n",
        "            )\n",
        "\n",
        "    def _create_lander(self, agent, initial_x):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "\n",
        "        init_x = initial_x + np.random.uniform(-0.3, 0.3)\n",
        "        init_y = self.VIEWPORT_H / self.SCALE * 0.8\n",
        "\n",
        "        lander_body = self.world.CreateDynamicBody(\n",
        "            position=(init_x, init_y),\n",
        "            angle=np.random.uniform(-0.2, 0.2),\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(vertices=[\n",
        "                    (-0.5, 0.0),\n",
        "                    (0.5, 0.0),\n",
        "                    (0.5, -1.0),\n",
        "                    (-0.5, -1.0)\n",
        "                ]),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,\n",
        "                restitution=0.0\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        lander_body.linearVelocity.Set(\n",
        "            np.random.uniform(-1.0, 1.0),\n",
        "            np.random.uniform(-1.0, 0.0)\n",
        "        )\n",
        "        lander_body.angularVelocity = np.random.uniform(-0.5, 0.5)\n",
        "\n",
        "        legs = []\n",
        "        legs_contact = [False, False]\n",
        "\n",
        "        for i in [-1, 1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(init_x + i * 0.3, init_y - 0.45),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(vertices=[\n",
        "                        (0, 0),\n",
        "                        (0, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, 0)\n",
        "                    ]),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    friction=0.2,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            legs.append(leg)\n",
        "\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=lander_body,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(i * 0.3, -0.45),\n",
        "                localAnchorB=(0, 0),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=100,\n",
        "                motorSpeed=0.3 * i,\n",
        "                lowerAngle=-0.6,\n",
        "                upperAngle=0.6\n",
        "            )\n",
        "            self.world.CreateJoint(rjd)\n",
        "\n",
        "        self.landers.append({\n",
        "            'body': lander_body,\n",
        "            'legs': legs,\n",
        "            'legs_contact': legs_contact,\n",
        "            'main_engine': False,\n",
        "            'side_engine_left': False,\n",
        "            'side_engine_right': False,\n",
        "            'on_safe_zone': False,\n",
        "            'crashed': False,\n",
        "            'outside_bounds': False\n",
        "        })\n",
        "\n",
        "    def _get_observation(self, agent):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "        lander = self.landers[idx]\n",
        "        pos = lander['body'].position\n",
        "        vel = lander['body'].linearVelocity\n",
        "\n",
        "        safe_zone_center_x = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "        safe_zone_width = self.safe_zones[idx][1] - self.safe_zones[idx][0]\n",
        "\n",
        "        dist_to_safe_zone = (pos[0] - safe_zone_center_x) / (self.VIEWPORT_W / self.SCALE)\n",
        "\n",
        "        observation = np.array([\n",
        "            (pos[0] - self.VIEWPORT_W / self.SCALE / 2) / (self.VIEWPORT_W / self.SCALE / 2),\n",
        "            (pos[1] - self.VIEWPORT_H / self.SCALE / 2) / (self.VIEWPORT_H / self.SCALE / 2),\n",
        "            vel[0] / 5.0,\n",
        "            vel[1] / 5.0,\n",
        "            lander['body'].angle,\n",
        "            lander['body'].angularVelocity / 5.0,\n",
        "            1.0 if lander['legs_contact'][0] else 0.0,\n",
        "            1.0 if lander['legs_contact'][1] else 0.0,\n",
        "            dist_to_safe_zone,\n",
        "            safe_zone_width / (self.VIEWPORT_W / self.SCALE),\n",
        "            1.0 if lander['on_safe_zone'] else 0.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _init_renderer(self):\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            pygame.display.set_caption(\"Dual Lunar Lander\")\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return None\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        surf = self.screen if self.render_mode == \"human\" else pygame.Surface((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "        surf.fill(self.sky_color)\n",
        "\n",
        "        def transform(pos):\n",
        "            return (int(pos[0] * self.SCALE), self.VIEWPORT_H - int(pos[1] * self.SCALE))\n",
        "\n",
        "        # Draw terrain\n",
        "        for poly, is_safe_zone in self.terrain:\n",
        "            transformed_poly = [transform(p) for p in poly]\n",
        "            color = self.safe_zone_color if is_safe_zone else self.terrain_color\n",
        "            pygame.draw.polygon(surf, color, transformed_poly)\n",
        "\n",
        "        # Draw landing pads\n",
        "        for left, right in self.safe_zones:\n",
        "            h = self.TERRAIN_HEIGHT/4\n",
        "            pad_poly = [\n",
        "                transform((left, h)),\n",
        "                transform((right, h)),\n",
        "                transform((right, h-0.05)),\n",
        "                transform((left, h-0.05))\n",
        "            ]\n",
        "            pygame.draw.polygon(surf, (255, 255, 255), pad_poly)\n",
        "\n",
        "        # Draw landers\n",
        "        for i, lander in enumerate(self.landers):\n",
        "            lander_color = self.lander_color[i]\n",
        "\n",
        "            # Draw body\n",
        "            vertices = []\n",
        "            for fixture in lander['body'].fixtures:\n",
        "                for vertex in fixture.shape.vertices:\n",
        "                    vertices.append(transform(lander['body'].transform * vertex))\n",
        "            pygame.draw.polygon(surf, lander_color, vertices)\n",
        "\n",
        "            # Draw legs\n",
        "            for leg in lander['legs']:\n",
        "                vertices = []\n",
        "                for fixture in leg.fixtures:\n",
        "                    for vertex in fixture.shape.vertices:\n",
        "                        vertices.append(transform(leg.transform * vertex))\n",
        "                pygame.draw.polygon(surf, self.leg_color, vertices)\n",
        "\n",
        "            # Draw engine effects\n",
        "            if lander['main_engine']:\n",
        "                start_pos = transform(lander['body'].position)\n",
        "                flame_length = 15 + np.random.randint(0, 5)\n",
        "                end_pos = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length\n",
        "                )\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 3)\n",
        "\n",
        "                inner_end = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length * 0.6,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length * 0.6\n",
        "                )\n",
        "                pygame.draw.line(surf, (255, 255, 255), start_pos, inner_end, 2)\n",
        "\n",
        "            if lander['side_engine_left']:\n",
        "                start_pos = transform(lander['body'].position + (-0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] - flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "            if lander['side_engine_right']:\n",
        "                start_pos = transform(lander['body'].position + (0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] + flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            return None\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False\n",
        "            self.screen = None\n",
        "\n",
        "class SB3Wrapper(gym.Env):\n",
        "    \"\"\"Custom Gym wrapper for Stable Baselines3 compatibility\"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_space = self.env.observation_space(self.agents[0])\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([obs_space.low]*2),\n",
        "            high=np.concatenate([obs_space.high]*2),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, info = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), info\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to dict\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "class StableBaselinesWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    Converts ParallelEnv to Gym Env for Stable Baselines3 compatibility\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_spaces = [self.env.observation_space(agent) for agent in self.agents]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([space.low for space in obs_spaces]),\n",
        "            high=np.concatenate([space.high for space in obs_spaces]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Important: Properly set metadata and render_mode\n",
        "        self.metadata = self.env.metadata\n",
        "        self.render_mode = env.render_mode\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, _ = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to action dictionary\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        # Convert to Gym step format\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms, 'truncations': truncs}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "def train_agents(total_timesteps=500000, render_every=50000, log_dir=\"./logs\"):\n",
        "    # Create and wrap environment\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    # Ensure render_mode is properly set\n",
        "    if not hasattr(wrapped_env, 'render_mode') or wrapped_env.render_mode != \"rgb_array\":\n",
        "        wrapped_env.render_mode = \"rgb_array\"\n",
        "\n",
        "    vec_env = DummyVecEnv([lambda: wrapped_env])\n",
        "\n",
        "    # Setup video recording\n",
        "    video_folder = os.path.join(log_dir, \"videos\")\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    vec_env = VecVideoRecorder(\n",
        "        vec_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda x: x % render_every == 0,\n",
        "        video_length=500,\n",
        "        name_prefix=\"dual_lander\"\n",
        "    )\n",
        "\n",
        "    # Create and train model\n",
        "    model = PPO(\n",
        "        MlpPolicy,\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        tensorboard_log=log_dir,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        policy_kwargs={\"net_arch\": [dict(pi=[256, 256], vf=[256, 256])]}\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save(os.path.join(log_dir, \"dual_lander_model\"))\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, num_episodes=5, render=True):\n",
        "    # Create environment with rendering\n",
        "    env = DualLunarLander(render_mode=\"human\" if render else None)\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = wrapped_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = wrapped_env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()  # Render original environment\n",
        "\n",
        "        print(f\"Episode {episode+1} Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training dual lunar landers...\")\n",
        "    model = train_agents()\n",
        "\n",
        "    # Evaluate with rendering\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    evaluate_model(model, num_episodes=5, render=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTPnO6MA9_yy",
        "outputId": "c20cebde-bc86-4b02-be4b-47e52b7ff905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dual lunar landers...\n",
            "Using cpu device\n",
            "Logging to ./logs/PPO_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-0-to-step-500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 256  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 7    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 299          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062678833 |\n",
            "|    clip_fraction        | 0.0461       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -0.000896    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.32e+04     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00963     |\n",
            "|    value_loss           | 1.76e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 330         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005209128 |\n",
            "|    clip_fraction        | 0.0265      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.76       |\n",
            "|    explained_variance   | -0.00268    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.35e+04    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00791    |\n",
            "|    value_loss           | 1.45e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 338          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064544566 |\n",
            "|    clip_fraction        | 0.0371       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.76        |\n",
            "|    explained_variance   | -0.000109    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.03e+04     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00889     |\n",
            "|    value_loss           | 1.56e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 351         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006316563 |\n",
            "|    clip_fraction        | 0.0298      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.75       |\n",
            "|    explained_variance   | -0.000806   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.54e+04    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00675    |\n",
            "|    value_loss           | 1.05e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 360          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 34           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075471047 |\n",
            "|    clip_fraction        | 0.0551       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.74        |\n",
            "|    explained_variance   | -0.00126     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.82e+04     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00996     |\n",
            "|    value_loss           | 1.13e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 361          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066640694 |\n",
            "|    clip_fraction        | 0.0533       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.72        |\n",
            "|    explained_variance   | -3.81e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.77e+04     |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00994     |\n",
            "|    value_loss           | 1.08e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 44         |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00880756 |\n",
            "|    clip_fraction        | 0.0547     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.7       |\n",
            "|    explained_variance   | -0.00312   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.02e+05   |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.00928   |\n",
            "|    value_loss           | 1.63e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 50          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007032848 |\n",
            "|    clip_fraction        | 0.0427      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.67       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.68e+04    |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00801    |\n",
            "|    value_loss           | 7.68e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 55           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069433395 |\n",
            "|    clip_fraction        | 0.055        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.66        |\n",
            "|    explained_variance   | -5.13e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.49e+04     |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.0109      |\n",
            "|    value_loss           | 6.3e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 365         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 61          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007190647 |\n",
            "|    clip_fraction        | 0.0562      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.63       |\n",
            "|    explained_variance   | -4.28e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.48e+04    |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    value_loss           | 7.79e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0078835655 |\n",
            "|    clip_fraction        | 0.0707       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.62        |\n",
            "|    explained_variance   | -4.29e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.69e+04     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.0118      |\n",
            "|    value_loss           | 7.48e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008754544 |\n",
            "|    clip_fraction        | 0.0721      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.6        |\n",
            "|    explained_variance   | -4.94e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.63e+04    |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.012      |\n",
            "|    value_loss           | 8.41e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 77         |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00624427 |\n",
            "|    clip_fraction        | 0.0524     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.56      |\n",
            "|    explained_variance   | 4.89e-06   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.74e+04   |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.00963   |\n",
            "|    value_loss           | 6.87e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009447302 |\n",
            "|    clip_fraction        | 0.0802      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.5        |\n",
            "|    explained_variance   | -8.34e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.62e+04    |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    value_loss           | 6.07e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005966069 |\n",
            "|    clip_fraction        | 0.0501      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.51       |\n",
            "|    explained_variance   | 4.41e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.1e+04     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    value_loss           | 6.14e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062005967 |\n",
            "|    clip_fraction        | 0.0529       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.52        |\n",
            "|    explained_variance   | -0.116       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.71e+04     |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.0103      |\n",
            "|    value_loss           | 6.94e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 375        |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 98         |\n",
            "|    total_timesteps      | 36864      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00941949 |\n",
            "|    clip_fraction        | 0.0856     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.46      |\n",
            "|    explained_variance   | -3.58e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.34e+04   |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.0132    |\n",
            "|    value_loss           | 3.11e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008959414 |\n",
            "|    clip_fraction        | 0.0593      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.46       |\n",
            "|    explained_variance   | 1.85e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.95e+04    |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00844    |\n",
            "|    value_loss           | 3.78e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 377         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005153001 |\n",
            "|    clip_fraction        | 0.0589      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.48       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.68e+04    |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    value_loss           | 5.53e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 114         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.061326154 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.43       |\n",
            "|    explained_variance   | -9.54e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.1e+04     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | 0.00287     |\n",
            "|    value_loss           | 2.89e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 377         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 119         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010981133 |\n",
            "|    clip_fraction        | 0.0497      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.33       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.08e+04    |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00439    |\n",
            "|    value_loss           | 5.83e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 377         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 124         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006706507 |\n",
            "|    clip_fraction        | 0.0708      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.32       |\n",
            "|    explained_variance   | -4.77e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.02e+04    |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00902    |\n",
            "|    value_loss           | 6e+04       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 378       |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 129       |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0167792 |\n",
            "|    clip_fraction        | 0.254     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.23     |\n",
            "|    explained_variance   | -4.77e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.55e+04  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | 0.00513   |\n",
            "|    value_loss           | 3.8e+04   |\n",
            "---------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 364        |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 140        |\n",
            "|    total_timesteps      | 51200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03884016 |\n",
            "|    clip_fraction        | 0.387      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.19      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.41e+04   |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | 0.0391     |\n",
            "|    value_loss           | 3.22e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 145         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010047244 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.26       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.05e+04    |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00463    |\n",
            "|    value_loss           | 3.07e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007388073 |\n",
            "|    clip_fraction        | 0.0333      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.24       |\n",
            "|    explained_variance   | -7.15e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00248    |\n",
            "|    value_loss           | 2.5e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 155         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054296024 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.2        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.35e+04    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.012       |\n",
            "|    value_loss           | 3.3e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 161         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.097456306 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | -3.58e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.45e+04    |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | 0.00995     |\n",
            "|    value_loss           | 2.47e+04    |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 369       |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 166       |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1625316 |\n",
            "|    clip_fraction        | 0.445     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.85     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.56e+04  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | 0.047     |\n",
            "|    value_loss           | 3.03e+04  |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 171         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013905701 |\n",
            "|    clip_fraction        | 0.0807      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.83       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.87e+03    |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.000807    |\n",
            "|    value_loss           | 3.08e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 176         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010387364 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.23e+04    |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | 0.00469     |\n",
            "|    value_loss           | 3.48e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 181          |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0113004055 |\n",
            "|    clip_fraction        | 0.176        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.67        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.36e+04     |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | 0.00563      |\n",
            "|    value_loss           | 2.73e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 187         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016328735 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.29e+03    |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | 0.00777     |\n",
            "|    value_loss           | 2.21e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 373          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 192          |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023874012 |\n",
            "|    clip_fraction        | 0.0882       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.76        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.74e+03     |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | 0.00367      |\n",
            "|    value_loss           | 1.73e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 197        |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00200016 |\n",
            "|    clip_fraction        | 0.0157     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.76      |\n",
            "|    explained_variance   | -2.38e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.89e+03   |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.0012    |\n",
            "|    value_loss           | 1.8e+04    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 202         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026239026 |\n",
            "|    clip_fraction        | 0.371       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.88       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.66e+03    |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | 0.0206      |\n",
            "|    value_loss           | 1.97e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 207         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011566928 |\n",
            "|    clip_fraction        | 0.0693      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.19e+04    |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00113    |\n",
            "|    value_loss           | 1.96e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 213         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016800284 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.09e+04    |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | 0.00104     |\n",
            "|    value_loss           | 2.04e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 218        |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03618672 |\n",
            "|    clip_fraction        | 0.299      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.87      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.42e+04   |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | 0.00827    |\n",
            "|    value_loss           | 2.04e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 224        |\n",
            "|    total_timesteps      | 83968      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01609462 |\n",
            "|    clip_fraction        | 0.103      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.11      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.75e+03   |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | 0.00202    |\n",
            "|    value_loss           | 1.29e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 229         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017977966 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.24e+03    |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.000394   |\n",
            "|    value_loss           | 1.44e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 233         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021088045 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.79       |\n",
            "|    explained_variance   | 2.92e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.94e+03    |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.000925   |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 239         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005191462 |\n",
            "|    clip_fraction        | 0.00732     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.92       |\n",
            "|    explained_variance   | -0.754      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.93e+04    |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00156    |\n",
            "|    value_loss           | 9.11e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 377          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 244          |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0096183205 |\n",
            "|    clip_fraction        | 0.162        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.87        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02e+04     |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | 0.00549      |\n",
            "|    value_loss           | 1.8e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 250         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008474359 |\n",
            "|    clip_fraction        | 0.0254      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.88       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.47e+03    |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.000817   |\n",
            "|    value_loss           | 1.33e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 377        |\n",
            "|    iterations           | 47         |\n",
            "|    time_elapsed         | 254        |\n",
            "|    total_timesteps      | 96256      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02672746 |\n",
            "|    clip_fraction        | 0.102      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.82      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.92e+03   |\n",
            "|    n_updates            | 460        |\n",
            "|    policy_gradient_loss | -0.00107   |\n",
            "|    value_loss           | 1.49e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 377         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 260         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039696842 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.69       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.85e+03    |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.000282   |\n",
            "|    value_loss           | 1.25e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 267         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031955168 |\n",
            "|    clip_fraction        | 0.433       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.54e+03    |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | 0.0114      |\n",
            "|    value_loss           | 1.22e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 276          |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053854985 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.52        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.89e+03     |\n",
            "|    n_updates            | 490          |\n",
            "|    policy_gradient_loss | 0.0029       |\n",
            "|    value_loss           | 1.29e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 51         |\n",
            "|    time_elapsed         | 281        |\n",
            "|    total_timesteps      | 104448     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14485145 |\n",
            "|    clip_fraction        | 0.23       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.55      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.56e+03   |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | 0.00863    |\n",
            "|    value_loss           | 1.68e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 52         |\n",
            "|    time_elapsed         | 287        |\n",
            "|    total_timesteps      | 106496     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14837408 |\n",
            "|    clip_fraction        | 0.444      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.91      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.86e+03   |\n",
            "|    n_updates            | 510        |\n",
            "|    policy_gradient_loss | 0.0507     |\n",
            "|    value_loss           | 1.85e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 292          |\n",
            "|    total_timesteps      | 108544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0128186885 |\n",
            "|    clip_fraction        | 0.228        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.83        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.05e+03     |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | 0.0142       |\n",
            "|    value_loss           | 1.62e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 297         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016555192 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.48e+03    |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | 0.00458     |\n",
            "|    value_loss           | 1.67e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 302        |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09034617 |\n",
            "|    clip_fraction        | 0.158      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.99e+03   |\n",
            "|    n_updates            | 540        |\n",
            "|    policy_gradient_loss | 0.00375    |\n",
            "|    value_loss           | 8.85e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 56         |\n",
            "|    time_elapsed         | 307        |\n",
            "|    total_timesteps      | 114688     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01395756 |\n",
            "|    clip_fraction        | 0.229      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.72      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.34e+03   |\n",
            "|    n_updates            | 550        |\n",
            "|    policy_gradient_loss | 0.00691    |\n",
            "|    value_loss           | 9.59e+03   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 313          |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069622807 |\n",
            "|    clip_fraction        | 0.106        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.65        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.89e+03     |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | 0.000423     |\n",
            "|    value_loss           | 1.25e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 318        |\n",
            "|    total_timesteps      | 118784     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03268943 |\n",
            "|    clip_fraction        | 0.094      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.68      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.72e+03   |\n",
            "|    n_updates            | 570        |\n",
            "|    policy_gradient_loss | -0.0033    |\n",
            "|    value_loss           | 1.63e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 59         |\n",
            "|    time_elapsed         | 323        |\n",
            "|    total_timesteps      | 120832     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07583827 |\n",
            "|    clip_fraction        | 0.296      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.57      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.3e+03    |\n",
            "|    n_updates            | 580        |\n",
            "|    policy_gradient_loss | 0.0202     |\n",
            "|    value_loss           | 1.34e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 328         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.051268786 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.1e+03     |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | 0.00952     |\n",
            "|    value_loss           | 2.06e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 61         |\n",
            "|    time_elapsed         | 333        |\n",
            "|    total_timesteps      | 124928     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00507834 |\n",
            "|    clip_fraction        | 0.0219     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.75      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.03e+04   |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | -0.000853  |\n",
            "|    value_loss           | 1.35e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 62         |\n",
            "|    time_elapsed         | 339        |\n",
            "|    total_timesteps      | 126976     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08184862 |\n",
            "|    clip_fraction        | 0.536      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.82      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.55e+03   |\n",
            "|    n_updates            | 610        |\n",
            "|    policy_gradient_loss | 0.0483     |\n",
            "|    value_loss           | 1.17e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 344         |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010670613 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.52e+03    |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | 0.000721    |\n",
            "|    value_loss           | 1.32e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 64         |\n",
            "|    time_elapsed         | 350        |\n",
            "|    total_timesteps      | 131072     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00938291 |\n",
            "|    clip_fraction        | 0.11       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.64      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.56e+03   |\n",
            "|    n_updates            | 630        |\n",
            "|    policy_gradient_loss | -0.00238   |\n",
            "|    value_loss           | 1.64e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 355         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017255737 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.38e+04    |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | 0.000813    |\n",
            "|    value_loss           | 1.47e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 360         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017950045 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.02e+03    |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | 0.00323     |\n",
            "|    value_loss           | 1.57e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 365         |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007084697 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.53       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.1e+03     |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.00316    |\n",
            "|    value_loss           | 1.51e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 375          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 370          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021927624 |\n",
            "|    clip_fraction        | 0.00586      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.000594    |\n",
            "|    value_loss           | 2.43e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 376         |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009896959 |\n",
            "|    clip_fraction        | 0.0796      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00337    |\n",
            "|    value_loss           | 1.68e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 381         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019706605 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.98e+04    |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.000449   |\n",
            "|    value_loss           | 2.08e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 375         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 386         |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010050232 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.36e+03    |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | 2.22e-05    |\n",
            "|    value_loss           | 1.42e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 376        |\n",
            "|    iterations           | 72         |\n",
            "|    time_elapsed         | 391        |\n",
            "|    total_timesteps      | 147456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11002047 |\n",
            "|    clip_fraction        | 0.446      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.79      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.15e+04   |\n",
            "|    n_updates            | 710        |\n",
            "|    policy_gradient_loss | 0.0326     |\n",
            "|    value_loss           | 2.01e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 376         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 396         |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014203458 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.64e+03    |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | 0.0142      |\n",
            "|    value_loss           | 1.57e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 371        |\n",
            "|    iterations           | 74         |\n",
            "|    time_elapsed         | 407        |\n",
            "|    total_timesteps      | 151552     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00948422 |\n",
            "|    clip_fraction        | 0.158      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.95e+04   |\n",
            "|    n_updates            | 730        |\n",
            "|    policy_gradient_loss | 0.0036     |\n",
            "|    value_loss           | 3.57e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 413         |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016095066 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.97e+03    |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | 0.00922     |\n",
            "|    value_loss           | 1.72e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 418         |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008474244 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.5e+03     |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | 0.00254     |\n",
            "|    value_loss           | 1.41e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 77         |\n",
            "|    time_elapsed         | 423        |\n",
            "|    total_timesteps      | 157696     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04043866 |\n",
            "|    clip_fraction        | 0.356      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.51      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.28e+04   |\n",
            "|    n_updates            | 760        |\n",
            "|    policy_gradient_loss | 0.021      |\n",
            "|    value_loss           | 1.88e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 78         |\n",
            "|    time_elapsed         | 429        |\n",
            "|    total_timesteps      | 159744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00969019 |\n",
            "|    clip_fraction        | 0.234      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.44      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.31e+04   |\n",
            "|    n_updates            | 770        |\n",
            "|    policy_gradient_loss | 0.0058     |\n",
            "|    value_loss           | 2.19e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 433         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009182061 |\n",
            "|    clip_fraction        | 0.328       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.44e+03    |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | 0.0221      |\n",
            "|    value_loss           | 1.94e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 80         |\n",
            "|    time_elapsed         | 439        |\n",
            "|    total_timesteps      | 163840     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04200001 |\n",
            "|    clip_fraction        | 0.255      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.56      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.67e+03   |\n",
            "|    n_updates            | 790        |\n",
            "|    policy_gradient_loss | 0.00904    |\n",
            "|    value_loss           | 1.68e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 444         |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019912839 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.62       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.64e+03    |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | 0.00818     |\n",
            "|    value_loss           | 1.3e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 82         |\n",
            "|    time_elapsed         | 450        |\n",
            "|    total_timesteps      | 167936     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02931317 |\n",
            "|    clip_fraction        | 0.153      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.52      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.02e+03   |\n",
            "|    n_updates            | 810        |\n",
            "|    policy_gradient_loss | 0.000917   |\n",
            "|    value_loss           | 1.38e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 455         |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004717892 |\n",
            "|    clip_fraction        | 0.0299      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.56       |\n",
            "|    explained_variance   | -0.000575   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.05e+04    |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.00476    |\n",
            "|    value_loss           | 7.19e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 84         |\n",
            "|    time_elapsed         | 460        |\n",
            "|    total_timesteps      | 172032     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04166123 |\n",
            "|    clip_fraction        | 0.184      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.69      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.05e+04   |\n",
            "|    n_updates            | 830        |\n",
            "|    policy_gradient_loss | 0.00144    |\n",
            "|    value_loss           | 1.61e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 466         |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.057878654 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.31e+04    |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00998     |\n",
            "|    value_loss           | 1.59e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 86         |\n",
            "|    time_elapsed         | 471        |\n",
            "|    total_timesteps      | 176128     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00978425 |\n",
            "|    clip_fraction        | 0.166      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.52      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.32e+03   |\n",
            "|    n_updates            | 850        |\n",
            "|    policy_gradient_loss | 0.000289   |\n",
            "|    value_loss           | 1.07e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 87         |\n",
            "|    time_elapsed         | 477        |\n",
            "|    total_timesteps      | 178176     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02878217 |\n",
            "|    clip_fraction        | 0.253      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.58      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.34e+04   |\n",
            "|    n_updates            | 860        |\n",
            "|    policy_gradient_loss | 0.014      |\n",
            "|    value_loss           | 1.36e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 482         |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012789827 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.6e+03     |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.00656     |\n",
            "|    value_loss           | 1.55e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 89         |\n",
            "|    time_elapsed         | 487        |\n",
            "|    total_timesteps      | 182272     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04093295 |\n",
            "|    clip_fraction        | 0.207      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.59      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.87e+03   |\n",
            "|    n_updates            | 880        |\n",
            "|    policy_gradient_loss | 0.00231    |\n",
            "|    value_loss           | 1.72e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 90         |\n",
            "|    time_elapsed         | 492        |\n",
            "|    total_timesteps      | 184320     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05905375 |\n",
            "|    clip_fraction        | 0.259      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.31      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.83e+03   |\n",
            "|    n_updates            | 890        |\n",
            "|    policy_gradient_loss | 0.0057     |\n",
            "|    value_loss           | 1.1e+04    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 497         |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028376639 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.01e+03    |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.00732     |\n",
            "|    value_loss           | 1.26e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 503         |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011349445 |\n",
            "|    clip_fraction        | 0.425       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.13e+03    |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | 0.0229      |\n",
            "|    value_loss           | 9.86e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 508         |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011937213 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.3e+03     |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | 0.000945    |\n",
            "|    value_loss           | 1.45e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 374          |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 514          |\n",
            "|    total_timesteps      | 192512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0144558335 |\n",
            "|    clip_fraction        | 0.0499       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.71e+03     |\n",
            "|    n_updates            | 930          |\n",
            "|    policy_gradient_loss | 0.000238     |\n",
            "|    value_loss           | 8.51e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 519         |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022104666 |\n",
            "|    clip_fraction        | 0.0964      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.2e+03     |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | 0.00219     |\n",
            "|    value_loss           | 9.73e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 374         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 525         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014166619 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.58e+03    |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | 0.00154     |\n",
            "|    value_loss           | 8.9e+03     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 374        |\n",
            "|    iterations           | 97         |\n",
            "|    time_elapsed         | 530        |\n",
            "|    total_timesteps      | 198656     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04527999 |\n",
            "|    clip_fraction        | 0.204      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.41      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.9e+03    |\n",
            "|    n_updates            | 960        |\n",
            "|    policy_gradient_loss | 0.0133     |\n",
            "|    value_loss           | 8.26e+03   |\n",
            "----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-200000-to-step-200500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-200000-to-step-200500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-200000-to-step-200500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-200000-to-step-200500.mp4\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 98         |\n",
            "|    time_elapsed         | 541        |\n",
            "|    total_timesteps      | 200704     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01028325 |\n",
            "|    clip_fraction        | 0.198      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.22      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.89e+03   |\n",
            "|    n_updates            | 970        |\n",
            "|    policy_gradient_loss | 0.0101     |\n",
            "|    value_loss           | 1.29e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 546         |\n",
            "|    total_timesteps      | 202752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013953518 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.54e+03    |\n",
            "|    n_updates            | 980         |\n",
            "|    policy_gradient_loss | 0.00523     |\n",
            "|    value_loss           | 1.68e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 371        |\n",
            "|    iterations           | 100        |\n",
            "|    time_elapsed         | 551        |\n",
            "|    total_timesteps      | 204800     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01630512 |\n",
            "|    clip_fraction        | 0.168      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.21      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.8e+03    |\n",
            "|    n_updates            | 990        |\n",
            "|    policy_gradient_loss | 0.0118     |\n",
            "|    value_loss           | 1.47e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 101          |\n",
            "|    time_elapsed         | 556          |\n",
            "|    total_timesteps      | 206848       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052324813 |\n",
            "|    clip_fraction        | 0.119        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.24e+03     |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | 0.00503      |\n",
            "|    value_loss           | 9.36e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 562         |\n",
            "|    total_timesteps      | 208896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013365179 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.35e+03    |\n",
            "|    n_updates            | 1010        |\n",
            "|    policy_gradient_loss | 0.00823     |\n",
            "|    value_loss           | 1.43e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 103         |\n",
            "|    time_elapsed         | 567         |\n",
            "|    total_timesteps      | 210944      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037017427 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.36e+03    |\n",
            "|    n_updates            | 1020        |\n",
            "|    policy_gradient_loss | 0.00487     |\n",
            "|    value_loss           | 9.72e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 572         |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016868556 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.25       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.69e+03    |\n",
            "|    n_updates            | 1030        |\n",
            "|    policy_gradient_loss | 0.00844     |\n",
            "|    value_loss           | 1.47e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 105         |\n",
            "|    time_elapsed         | 578         |\n",
            "|    total_timesteps      | 215040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011249533 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.08e+03    |\n",
            "|    n_updates            | 1040        |\n",
            "|    policy_gradient_loss | -0.00161    |\n",
            "|    value_loss           | 1.4e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 106         |\n",
            "|    time_elapsed         | 583         |\n",
            "|    total_timesteps      | 217088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022752555 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.13e+03    |\n",
            "|    n_updates            | 1050        |\n",
            "|    policy_gradient_loss | 0.000834    |\n",
            "|    value_loss           | 9.24e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 589          |\n",
            "|    total_timesteps      | 219136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0129571725 |\n",
            "|    clip_fraction        | 0.294        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.19        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.06e+03     |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | 0.0265       |\n",
            "|    value_loss           | 1.24e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 594          |\n",
            "|    total_timesteps      | 221184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065136226 |\n",
            "|    clip_fraction        | 0.068        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.85e+03     |\n",
            "|    n_updates            | 1070         |\n",
            "|    policy_gradient_loss | -0.00122     |\n",
            "|    value_loss           | 8.3e+03      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 599         |\n",
            "|    total_timesteps      | 223232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016740225 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.24e+03    |\n",
            "|    n_updates            | 1080        |\n",
            "|    policy_gradient_loss | 0.0116      |\n",
            "|    value_loss           | 1.12e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 110         |\n",
            "|    time_elapsed         | 605         |\n",
            "|    total_timesteps      | 225280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028169448 |\n",
            "|    clip_fraction        | 0.0694      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1e+04       |\n",
            "|    n_updates            | 1090        |\n",
            "|    policy_gradient_loss | 0.00105     |\n",
            "|    value_loss           | 1.58e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 372        |\n",
            "|    iterations           | 111        |\n",
            "|    time_elapsed         | 610        |\n",
            "|    total_timesteps      | 227328     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06787572 |\n",
            "|    clip_fraction        | 0.182      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.04      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.24e+03   |\n",
            "|    n_updates            | 1100       |\n",
            "|    policy_gradient_loss | 0.0096     |\n",
            "|    value_loss           | 1.11e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 616          |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040562483 |\n",
            "|    clip_fraction        | 0.0646       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.877       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.35e+03     |\n",
            "|    n_updates            | 1110         |\n",
            "|    policy_gradient_loss | 0.000203     |\n",
            "|    value_loss           | 1.43e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 372          |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 620          |\n",
            "|    total_timesteps      | 231424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049063014 |\n",
            "|    clip_fraction        | 0.0816       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.825       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.25e+03     |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -0.000752    |\n",
            "|    value_loss           | 1.24e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 114         |\n",
            "|    time_elapsed         | 626         |\n",
            "|    total_timesteps      | 233472      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005632323 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.903      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.32e+03    |\n",
            "|    n_updates            | 1130        |\n",
            "|    policy_gradient_loss | 0.0179      |\n",
            "|    value_loss           | 1.2e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 631         |\n",
            "|    total_timesteps      | 235520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013056278 |\n",
            "|    clip_fraction        | 0.0666      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.887      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.69e+03    |\n",
            "|    n_updates            | 1140        |\n",
            "|    policy_gradient_loss | 0.00113     |\n",
            "|    value_loss           | 1.31e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 637         |\n",
            "|    total_timesteps      | 237568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005996028 |\n",
            "|    clip_fraction        | 0.0947      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.91       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.53e+03    |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | 0.00395     |\n",
            "|    value_loss           | 8.69e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 117         |\n",
            "|    time_elapsed         | 642         |\n",
            "|    total_timesteps      | 239616      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006703998 |\n",
            "|    clip_fraction        | 0.0273      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.89       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.01e+03    |\n",
            "|    n_updates            | 1160        |\n",
            "|    policy_gradient_loss | -0.00222    |\n",
            "|    value_loss           | 9.87e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 118        |\n",
            "|    time_elapsed         | 647        |\n",
            "|    total_timesteps      | 241664     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07160383 |\n",
            "|    clip_fraction        | 0.0903     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.886     |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.35e+04   |\n",
            "|    n_updates            | 1170       |\n",
            "|    policy_gradient_loss | 0.00144    |\n",
            "|    value_loss           | 1.67e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 119         |\n",
            "|    time_elapsed         | 653         |\n",
            "|    total_timesteps      | 243712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010884368 |\n",
            "|    clip_fraction        | 0.377       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.744      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.87e+03    |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | 0.0116      |\n",
            "|    value_loss           | 1.16e+04    |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 373      |\n",
            "|    iterations           | 120      |\n",
            "|    time_elapsed         | 658      |\n",
            "|    total_timesteps      | 245760   |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.09028  |\n",
            "|    clip_fraction        | 0.141    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -0.948   |\n",
            "|    explained_variance   | 0        |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 4.07e+03 |\n",
            "|    n_updates            | 1190     |\n",
            "|    policy_gradient_loss | 0.00269  |\n",
            "|    value_loss           | 1.28e+04 |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 121         |\n",
            "|    time_elapsed         | 664         |\n",
            "|    total_timesteps      | 247808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037960976 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.949      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.05e+04    |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | 0.0053      |\n",
            "|    value_loss           | 1.75e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 373          |\n",
            "|    iterations           | 122          |\n",
            "|    time_elapsed         | 669          |\n",
            "|    total_timesteps      | 249856       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0151706375 |\n",
            "|    clip_fraction        | 0.175        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16e+04     |\n",
            "|    n_updates            | 1210         |\n",
            "|    policy_gradient_loss | 0.011        |\n",
            "|    value_loss           | 1.46e+04     |\n",
            "------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-250000-to-step-250500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-250000-to-step-250500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-250000-to-step-250500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-250000-to-step-250500.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 123          |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 251904       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066695125 |\n",
            "|    clip_fraction        | 0.0793       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.03e+04     |\n",
            "|    n_updates            | 1220         |\n",
            "|    policy_gradient_loss | 0.00159      |\n",
            "|    value_loss           | 1.41e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 124         |\n",
            "|    time_elapsed         | 684         |\n",
            "|    total_timesteps      | 253952      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029894566 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.05e+04    |\n",
            "|    n_updates            | 1230        |\n",
            "|    policy_gradient_loss | 0.00378     |\n",
            "|    value_loss           | 1.07e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 690          |\n",
            "|    total_timesteps      | 256000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050829435 |\n",
            "|    clip_fraction        | 0.118        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.69e+03     |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.000423    |\n",
            "|    value_loss           | 9.26e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 126         |\n",
            "|    time_elapsed         | 695         |\n",
            "|    total_timesteps      | 258048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008512042 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.89e+03    |\n",
            "|    n_updates            | 1250        |\n",
            "|    policy_gradient_loss | 0.000315    |\n",
            "|    value_loss           | 1.32e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 127         |\n",
            "|    time_elapsed         | 701         |\n",
            "|    total_timesteps      | 260096      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007933177 |\n",
            "|    clip_fraction        | 0.0641      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.73e+03    |\n",
            "|    n_updates            | 1260        |\n",
            "|    policy_gradient_loss | 0.000503    |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 128        |\n",
            "|    time_elapsed         | 706        |\n",
            "|    total_timesteps      | 262144     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02231823 |\n",
            "|    clip_fraction        | 0.128      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.13      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.32e+03   |\n",
            "|    n_updates            | 1270       |\n",
            "|    policy_gradient_loss | 0.00735    |\n",
            "|    value_loss           | 1.23e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 129         |\n",
            "|    time_elapsed         | 712         |\n",
            "|    total_timesteps      | 264192      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023545507 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.837      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.29e+03    |\n",
            "|    n_updates            | 1280        |\n",
            "|    policy_gradient_loss | 0.00757     |\n",
            "|    value_loss           | 9.22e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 130         |\n",
            "|    time_elapsed         | 717         |\n",
            "|    total_timesteps      | 266240      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018326685 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.82       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.08e+03    |\n",
            "|    n_updates            | 1290        |\n",
            "|    policy_gradient_loss | 0.0113      |\n",
            "|    value_loss           | 1.38e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 131         |\n",
            "|    time_elapsed         | 722         |\n",
            "|    total_timesteps      | 268288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020270951 |\n",
            "|    clip_fraction        | 0.294       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.979      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.53e+03    |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | 0.0154      |\n",
            "|    value_loss           | 1.02e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 371          |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 728          |\n",
            "|    total_timesteps      | 270336       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048122867 |\n",
            "|    clip_fraction        | 0.107        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.87e+03     |\n",
            "|    n_updates            | 1310         |\n",
            "|    policy_gradient_loss | 0.0045       |\n",
            "|    value_loss           | 1.1e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 133         |\n",
            "|    time_elapsed         | 733         |\n",
            "|    total_timesteps      | 272384      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027451167 |\n",
            "|    clip_fraction        | 0.27        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.862      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.3e+03     |\n",
            "|    n_updates            | 1320        |\n",
            "|    policy_gradient_loss | 0.0072      |\n",
            "|    value_loss           | 1.58e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 134         |\n",
            "|    time_elapsed         | 739         |\n",
            "|    total_timesteps      | 274432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025456522 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.957      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.51e+03    |\n",
            "|    n_updates            | 1330        |\n",
            "|    policy_gradient_loss | 0.00653     |\n",
            "|    value_loss           | 1.03e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 135         |\n",
            "|    time_elapsed         | 744         |\n",
            "|    total_timesteps      | 276480      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020532498 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.863      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.74e+03    |\n",
            "|    n_updates            | 1340        |\n",
            "|    policy_gradient_loss | 0.00526     |\n",
            "|    value_loss           | 8.76e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 136         |\n",
            "|    time_elapsed         | 749         |\n",
            "|    total_timesteps      | 278528      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011321185 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.823      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.79e+03    |\n",
            "|    n_updates            | 1350        |\n",
            "|    policy_gradient_loss | 0.00809     |\n",
            "|    value_loss           | 6.68e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 371        |\n",
            "|    iterations           | 137        |\n",
            "|    time_elapsed         | 755        |\n",
            "|    total_timesteps      | 280576     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00917476 |\n",
            "|    clip_fraction        | 0.0604     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.773     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.59e+03   |\n",
            "|    n_updates            | 1360       |\n",
            "|    policy_gradient_loss | 0.00292    |\n",
            "|    value_loss           | 1.09e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 138         |\n",
            "|    time_elapsed         | 760         |\n",
            "|    total_timesteps      | 282624      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015571608 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.815      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.64e+03    |\n",
            "|    n_updates            | 1370        |\n",
            "|    policy_gradient_loss | 0.00527     |\n",
            "|    value_loss           | 1.1e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 139         |\n",
            "|    time_elapsed         | 766         |\n",
            "|    total_timesteps      | 284672      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010382588 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.827      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.86e+03    |\n",
            "|    n_updates            | 1380        |\n",
            "|    policy_gradient_loss | -5.31e-05   |\n",
            "|    value_loss           | 9.91e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 140         |\n",
            "|    time_elapsed         | 771         |\n",
            "|    total_timesteps      | 286720      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016219575 |\n",
            "|    clip_fraction        | 0.238       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.745      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.1e+03     |\n",
            "|    n_updates            | 1390        |\n",
            "|    policy_gradient_loss | 0.0112      |\n",
            "|    value_loss           | 9.62e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 141         |\n",
            "|    time_elapsed         | 777         |\n",
            "|    total_timesteps      | 288768      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016766861 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.863      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.69e+03    |\n",
            "|    n_updates            | 1400        |\n",
            "|    policy_gradient_loss | -0.00081    |\n",
            "|    value_loss           | 5.43e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 781         |\n",
            "|    total_timesteps      | 290816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001975418 |\n",
            "|    clip_fraction        | 0.00254     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.899      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.37e+03    |\n",
            "|    n_updates            | 1410        |\n",
            "|    policy_gradient_loss | -0.000262   |\n",
            "|    value_loss           | 1.39e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 143         |\n",
            "|    time_elapsed         | 787         |\n",
            "|    total_timesteps      | 292864      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023423802 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.845      |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.97e+03    |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | 0.0103      |\n",
            "|    value_loss           | 9.73e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 144         |\n",
            "|    time_elapsed         | 792         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032107458 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.888      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.27e+03    |\n",
            "|    n_updates            | 1430        |\n",
            "|    policy_gradient_loss | 0.00562     |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 145         |\n",
            "|    time_elapsed         | 797         |\n",
            "|    total_timesteps      | 296960      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017407373 |\n",
            "|    clip_fraction        | 0.069       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.804      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.66e+03    |\n",
            "|    n_updates            | 1440        |\n",
            "|    policy_gradient_loss | 0.0029      |\n",
            "|    value_loss           | 1.15e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 146         |\n",
            "|    time_elapsed         | 803         |\n",
            "|    total_timesteps      | 299008      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007868017 |\n",
            "|    clip_fraction        | 0.0565      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.827      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.47e+03    |\n",
            "|    n_updates            | 1450        |\n",
            "|    policy_gradient_loss | -0.000637   |\n",
            "|    value_loss           | 1.06e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-300000-to-step-300500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-300000-to-step-300500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-300000-to-step-300500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-300000-to-step-300500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 147         |\n",
            "|    time_elapsed         | 815         |\n",
            "|    total_timesteps      | 301056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012784265 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.804      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.81e+03    |\n",
            "|    n_updates            | 1460        |\n",
            "|    policy_gradient_loss | 0.00735     |\n",
            "|    value_loss           | 1.98e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 148         |\n",
            "|    time_elapsed         | 820         |\n",
            "|    total_timesteps      | 303104      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012699366 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.745      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.07e+03    |\n",
            "|    n_updates            | 1470        |\n",
            "|    policy_gradient_loss | 0.0144      |\n",
            "|    value_loss           | 9.74e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 149         |\n",
            "|    time_elapsed         | 825         |\n",
            "|    total_timesteps      | 305152      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015260051 |\n",
            "|    clip_fraction        | 0.0692      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.678      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.83e+03    |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | -0.000118   |\n",
            "|    value_loss           | 1.94e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 150        |\n",
            "|    time_elapsed         | 831        |\n",
            "|    total_timesteps      | 307200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00394742 |\n",
            "|    clip_fraction        | 0.0349     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.682     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.66e+03   |\n",
            "|    n_updates            | 1490       |\n",
            "|    policy_gradient_loss | -0.000322  |\n",
            "|    value_loss           | 7.63e+03   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 151          |\n",
            "|    time_elapsed         | 836          |\n",
            "|    total_timesteps      | 309248       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046439245 |\n",
            "|    clip_fraction        | 0.0353       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.636       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.64e+03     |\n",
            "|    n_updates            | 1500         |\n",
            "|    policy_gradient_loss | -0.000702    |\n",
            "|    value_loss           | 1.13e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 152        |\n",
            "|    time_elapsed         | 841        |\n",
            "|    total_timesteps      | 311296     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04028076 |\n",
            "|    clip_fraction        | 0.193      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.682     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.81e+03   |\n",
            "|    n_updates            | 1510       |\n",
            "|    policy_gradient_loss | 0.00813    |\n",
            "|    value_loss           | 9.11e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 153         |\n",
            "|    time_elapsed         | 847         |\n",
            "|    total_timesteps      | 313344      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022118267 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.768      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.83e+03    |\n",
            "|    n_updates            | 1520        |\n",
            "|    policy_gradient_loss | 0.00375     |\n",
            "|    value_loss           | 9.4e+03     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 154          |\n",
            "|    time_elapsed         | 852          |\n",
            "|    total_timesteps      | 315392       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052936426 |\n",
            "|    clip_fraction        | 0.0329       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.834       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.13e+03     |\n",
            "|    n_updates            | 1530         |\n",
            "|    policy_gradient_loss | -0.000952    |\n",
            "|    value_loss           | 9.79e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 155         |\n",
            "|    time_elapsed         | 857         |\n",
            "|    total_timesteps      | 317440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026170094 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.902      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.11e+03    |\n",
            "|    n_updates            | 1540        |\n",
            "|    policy_gradient_loss | 0.0147      |\n",
            "|    value_loss           | 9.34e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 156        |\n",
            "|    time_elapsed         | 863        |\n",
            "|    total_timesteps      | 319488     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00262922 |\n",
            "|    clip_fraction        | 0.0528     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.964     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 7.64e+03   |\n",
            "|    n_updates            | 1550       |\n",
            "|    policy_gradient_loss | 0.00189    |\n",
            "|    value_loss           | 9.48e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 157        |\n",
            "|    time_elapsed         | 868        |\n",
            "|    total_timesteps      | 321536     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00802412 |\n",
            "|    clip_fraction        | 0.097      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.97      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.77e+03   |\n",
            "|    n_updates            | 1560       |\n",
            "|    policy_gradient_loss | 0.00116    |\n",
            "|    value_loss           | 8.93e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 158         |\n",
            "|    time_elapsed         | 873         |\n",
            "|    total_timesteps      | 323584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024875056 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.994      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.64e+03    |\n",
            "|    n_updates            | 1570        |\n",
            "|    policy_gradient_loss | 0.0101      |\n",
            "|    value_loss           | 1.2e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 159         |\n",
            "|    time_elapsed         | 879         |\n",
            "|    total_timesteps      | 325632      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011553945 |\n",
            "|    clip_fraction        | 0.0865      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.93       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14e+04    |\n",
            "|    n_updates            | 1580        |\n",
            "|    policy_gradient_loss | 0.00319     |\n",
            "|    value_loss           | 1.25e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 160        |\n",
            "|    time_elapsed         | 884        |\n",
            "|    total_timesteps      | 327680     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04162699 |\n",
            "|    clip_fraction        | 0.177      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.06      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.64e+03   |\n",
            "|    n_updates            | 1590       |\n",
            "|    policy_gradient_loss | 0.00712    |\n",
            "|    value_loss           | 1.09e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 161        |\n",
            "|    time_elapsed         | 890        |\n",
            "|    total_timesteps      | 329728     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04976123 |\n",
            "|    clip_fraction        | 0.135      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.88      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.62e+03   |\n",
            "|    n_updates            | 1600       |\n",
            "|    policy_gradient_loss | -0.000217  |\n",
            "|    value_loss           | 1.31e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 370          |\n",
            "|    iterations           | 162          |\n",
            "|    time_elapsed         | 895          |\n",
            "|    total_timesteps      | 331776       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076014614 |\n",
            "|    clip_fraction        | 0.0862       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.77        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.28e+03     |\n",
            "|    n_updates            | 1610         |\n",
            "|    policy_gradient_loss | -0.00092     |\n",
            "|    value_loss           | 9.97e+03     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 370        |\n",
            "|    iterations           | 163        |\n",
            "|    time_elapsed         | 900        |\n",
            "|    total_timesteps      | 333824     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11258596 |\n",
            "|    clip_fraction        | 0.178      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.877     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.52e+03   |\n",
            "|    n_updates            | 1620       |\n",
            "|    policy_gradient_loss | 0.0135     |\n",
            "|    value_loss           | 1.72e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 164         |\n",
            "|    time_elapsed         | 906         |\n",
            "|    total_timesteps      | 335872      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007513508 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.982      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.83e+03    |\n",
            "|    n_updates            | 1630        |\n",
            "|    policy_gradient_loss | 0.00263     |\n",
            "|    value_loss           | 1.55e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 165         |\n",
            "|    time_elapsed         | 911         |\n",
            "|    total_timesteps      | 337920      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036983453 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.989      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.02e+03    |\n",
            "|    n_updates            | 1640        |\n",
            "|    policy_gradient_loss | 0.00339     |\n",
            "|    value_loss           | 1.15e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 166         |\n",
            "|    time_elapsed         | 917         |\n",
            "|    total_timesteps      | 339968      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039309185 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.35e+03    |\n",
            "|    n_updates            | 1650        |\n",
            "|    policy_gradient_loss | 0.0154      |\n",
            "|    value_loss           | 1.3e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 167         |\n",
            "|    time_elapsed         | 922         |\n",
            "|    total_timesteps      | 342016      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030077204 |\n",
            "|    clip_fraction        | 0.0933      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.966      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.19e+03    |\n",
            "|    n_updates            | 1660        |\n",
            "|    policy_gradient_loss | 0.00807     |\n",
            "|    value_loss           | 1.14e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 168         |\n",
            "|    time_elapsed         | 928         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013623462 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.34e+03    |\n",
            "|    n_updates            | 1670        |\n",
            "|    policy_gradient_loss | -0.00327    |\n",
            "|    value_loss           | 1.06e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 169         |\n",
            "|    time_elapsed         | 933         |\n",
            "|    total_timesteps      | 346112      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.065982014 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.08e+04    |\n",
            "|    n_updates            | 1680        |\n",
            "|    policy_gradient_loss | 0.00311     |\n",
            "|    value_loss           | 2.26e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 170         |\n",
            "|    time_elapsed         | 938         |\n",
            "|    total_timesteps      | 348160      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013366953 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.02e+03    |\n",
            "|    n_updates            | 1690        |\n",
            "|    policy_gradient_loss | 0.00292     |\n",
            "|    value_loss           | 1.58e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 171         |\n",
            "|    time_elapsed         | 945         |\n",
            "|    total_timesteps      | 350208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.068691626 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.13e+03    |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | 0.016       |\n",
            "|    value_loss           | 1.65e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-350000-to-step-350500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-350000-to-step-350500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-350000-to-step-350500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-350000-to-step-350500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 172         |\n",
            "|    time_elapsed         | 955         |\n",
            "|    total_timesteps      | 352256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009499058 |\n",
            "|    clip_fraction        | 0.0845      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.44e+04    |\n",
            "|    n_updates            | 1710        |\n",
            "|    policy_gradient_loss | -0.000722   |\n",
            "|    value_loss           | 4.93e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 173         |\n",
            "|    time_elapsed         | 960         |\n",
            "|    total_timesteps      | 354304      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010332913 |\n",
            "|    clip_fraction        | 0.0953      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.74e+04    |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | 0.00233     |\n",
            "|    value_loss           | 2.53e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 174         |\n",
            "|    time_elapsed         | 966         |\n",
            "|    total_timesteps      | 356352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012422008 |\n",
            "|    clip_fraction        | 0.0948      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.58e+03    |\n",
            "|    n_updates            | 1730        |\n",
            "|    policy_gradient_loss | -0.0032     |\n",
            "|    value_loss           | 9.95e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 175         |\n",
            "|    time_elapsed         | 971         |\n",
            "|    total_timesteps      | 358400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029307453 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.19e+03    |\n",
            "|    n_updates            | 1740        |\n",
            "|    policy_gradient_loss | 0.00472     |\n",
            "|    value_loss           | 9.22e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 176          |\n",
            "|    time_elapsed         | 976          |\n",
            "|    total_timesteps      | 360448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044078818 |\n",
            "|    clip_fraction        | 0.00933      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.12        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.38e+04     |\n",
            "|    n_updates            | 1750         |\n",
            "|    policy_gradient_loss | -0.000927    |\n",
            "|    value_loss           | 6.55e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 177         |\n",
            "|    time_elapsed         | 982         |\n",
            "|    total_timesteps      | 362496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019912511 |\n",
            "|    clip_fraction        | 0.351       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.95e+04    |\n",
            "|    n_updates            | 1760        |\n",
            "|    policy_gradient_loss | 0.0198      |\n",
            "|    value_loss           | 3.28e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 178          |\n",
            "|    time_elapsed         | 987          |\n",
            "|    total_timesteps      | 364544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021149735 |\n",
            "|    clip_fraction        | 0.012        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.33e+04     |\n",
            "|    n_updates            | 1770         |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    value_loss           | 1.34e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 179        |\n",
            "|    time_elapsed         | 993        |\n",
            "|    total_timesteps      | 366592     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13556942 |\n",
            "|    clip_fraction        | 0.303      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1         |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.07e+04   |\n",
            "|    n_updates            | 1780       |\n",
            "|    policy_gradient_loss | 0.0185     |\n",
            "|    value_loss           | 6.43e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 180         |\n",
            "|    time_elapsed         | 998         |\n",
            "|    total_timesteps      | 368640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010707362 |\n",
            "|    clip_fraction        | 0.0481      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.17e+03    |\n",
            "|    n_updates            | 1790        |\n",
            "|    policy_gradient_loss | -0.00396    |\n",
            "|    value_loss           | 1.33e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 181         |\n",
            "|    time_elapsed         | 1003        |\n",
            "|    total_timesteps      | 370688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007823329 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6e+03       |\n",
            "|    n_updates            | 1800        |\n",
            "|    policy_gradient_loss | 0.00369     |\n",
            "|    value_loss           | 1.18e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 182        |\n",
            "|    time_elapsed         | 1009       |\n",
            "|    total_timesteps      | 372736     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01753671 |\n",
            "|    clip_fraction        | 0.287      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.02      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.63e+03   |\n",
            "|    n_updates            | 1810       |\n",
            "|    policy_gradient_loss | 0.0119     |\n",
            "|    value_loss           | 1.52e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 183         |\n",
            "|    time_elapsed         | 1014        |\n",
            "|    total_timesteps      | 374784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041619048 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.03e+04    |\n",
            "|    n_updates            | 1820        |\n",
            "|    policy_gradient_loss | -0.000174   |\n",
            "|    value_loss           | 1.49e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 184         |\n",
            "|    time_elapsed         | 1020        |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043843403 |\n",
            "|    clip_fraction        | 0.394       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.38e+03    |\n",
            "|    n_updates            | 1830        |\n",
            "|    policy_gradient_loss | 0.0466      |\n",
            "|    value_loss           | 1.1e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 185         |\n",
            "|    time_elapsed         | 1025        |\n",
            "|    total_timesteps      | 378880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.053492688 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.47e+04    |\n",
            "|    n_updates            | 1840        |\n",
            "|    policy_gradient_loss | 0.018       |\n",
            "|    value_loss           | 1.39e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 186         |\n",
            "|    time_elapsed         | 1031        |\n",
            "|    total_timesteps      | 380928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033063237 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.2e+03     |\n",
            "|    n_updates            | 1850        |\n",
            "|    policy_gradient_loss | 0.0141      |\n",
            "|    value_loss           | 7.09e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 187         |\n",
            "|    time_elapsed         | 1036        |\n",
            "|    total_timesteps      | 382976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019587146 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.87e+03    |\n",
            "|    n_updates            | 1860        |\n",
            "|    policy_gradient_loss | 0.00524     |\n",
            "|    value_loss           | 1.06e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 188         |\n",
            "|    time_elapsed         | 1042        |\n",
            "|    total_timesteps      | 385024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.057114042 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.43e+03    |\n",
            "|    n_updates            | 1870        |\n",
            "|    policy_gradient_loss | 0.00727     |\n",
            "|    value_loss           | 8.2e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 189         |\n",
            "|    time_elapsed         | 1047        |\n",
            "|    total_timesteps      | 387072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020149522 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.82e+03    |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | 0.00622     |\n",
            "|    value_loss           | 1.35e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 190         |\n",
            "|    time_elapsed         | 1052        |\n",
            "|    total_timesteps      | 389120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007639071 |\n",
            "|    clip_fraction        | 0.0287      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.28       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.17e+03    |\n",
            "|    n_updates            | 1890        |\n",
            "|    policy_gradient_loss | -0.0037     |\n",
            "|    value_loss           | 1.15e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 191         |\n",
            "|    time_elapsed         | 1058        |\n",
            "|    total_timesteps      | 391168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012166953 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.75e+03    |\n",
            "|    n_updates            | 1900        |\n",
            "|    policy_gradient_loss | 0.00195     |\n",
            "|    value_loss           | 8.57e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 192         |\n",
            "|    time_elapsed         | 1063        |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012889834 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.26e+03    |\n",
            "|    n_updates            | 1910        |\n",
            "|    policy_gradient_loss | 0.00863     |\n",
            "|    value_loss           | 1.01e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 193         |\n",
            "|    time_elapsed         | 1069        |\n",
            "|    total_timesteps      | 395264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032436676 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.38e+04    |\n",
            "|    n_updates            | 1920        |\n",
            "|    policy_gradient_loss | 0.00369     |\n",
            "|    value_loss           | 1.26e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 194         |\n",
            "|    time_elapsed         | 1074        |\n",
            "|    total_timesteps      | 397312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033156343 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.24       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.96e+03    |\n",
            "|    n_updates            | 1930        |\n",
            "|    policy_gradient_loss | -0.00115    |\n",
            "|    value_loss           | 8.31e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 195        |\n",
            "|    time_elapsed         | 1080       |\n",
            "|    total_timesteps      | 399360     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01170288 |\n",
            "|    clip_fraction        | 0.219      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.22      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.6e+03    |\n",
            "|    n_updates            | 1940       |\n",
            "|    policy_gradient_loss | 0.00305    |\n",
            "|    value_loss           | 6.54e+03   |\n",
            "----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-400000-to-step-400500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-400000-to-step-400500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-400000-to-step-400500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-400000-to-step-400500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 196         |\n",
            "|    time_elapsed         | 1091        |\n",
            "|    total_timesteps      | 401408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013709856 |\n",
            "|    clip_fraction        | 0.0346      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.26       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.57e+03    |\n",
            "|    n_updates            | 1950        |\n",
            "|    policy_gradient_loss | -0.00264    |\n",
            "|    value_loss           | 1.24e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 197         |\n",
            "|    time_elapsed         | 1098        |\n",
            "|    total_timesteps      | 403456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025663026 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.94e+03    |\n",
            "|    n_updates            | 1960        |\n",
            "|    policy_gradient_loss | 0.0175      |\n",
            "|    value_loss           | 1.41e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 198        |\n",
            "|    time_elapsed         | 1103       |\n",
            "|    total_timesteps      | 405504     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09957523 |\n",
            "|    clip_fraction        | 0.33       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.28      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.33e+03   |\n",
            "|    n_updates            | 1970       |\n",
            "|    policy_gradient_loss | 0.025      |\n",
            "|    value_loss           | 1.48e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 199          |\n",
            "|    time_elapsed         | 1109         |\n",
            "|    total_timesteps      | 407552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048471536 |\n",
            "|    clip_fraction        | 0.0729       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.99e+03     |\n",
            "|    n_updates            | 1980         |\n",
            "|    policy_gradient_loss | -0.000501    |\n",
            "|    value_loss           | 1.2e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 200         |\n",
            "|    time_elapsed         | 1114        |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011804561 |\n",
            "|    clip_fraction        | 0.316       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.36e+03    |\n",
            "|    n_updates            | 1990        |\n",
            "|    policy_gradient_loss | 0.0126      |\n",
            "|    value_loss           | 9.49e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 201        |\n",
            "|    time_elapsed         | 1120       |\n",
            "|    total_timesteps      | 411648     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04509581 |\n",
            "|    clip_fraction        | 0.148      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.19e+03   |\n",
            "|    n_updates            | 2000       |\n",
            "|    policy_gradient_loss | 8.24e-05   |\n",
            "|    value_loss           | 8.57e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 202         |\n",
            "|    time_elapsed         | 1125        |\n",
            "|    total_timesteps      | 413696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010082847 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.986      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.86e+03    |\n",
            "|    n_updates            | 2010        |\n",
            "|    policy_gradient_loss | 0.00998     |\n",
            "|    value_loss           | 1.22e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 203        |\n",
            "|    time_elapsed         | 1131       |\n",
            "|    total_timesteps      | 415744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05640243 |\n",
            "|    clip_fraction        | 0.182      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.991     |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.43e+03   |\n",
            "|    n_updates            | 2020       |\n",
            "|    policy_gradient_loss | 0.00409    |\n",
            "|    value_loss           | 1.05e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 204         |\n",
            "|    time_elapsed         | 1136        |\n",
            "|    total_timesteps      | 417792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020508345 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.81e+03    |\n",
            "|    n_updates            | 2030        |\n",
            "|    policy_gradient_loss | -0.0023     |\n",
            "|    value_loss           | 1.31e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 205        |\n",
            "|    time_elapsed         | 1142       |\n",
            "|    total_timesteps      | 419840     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00660477 |\n",
            "|    clip_fraction        | 0.0421     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.47e+03   |\n",
            "|    n_updates            | 2040       |\n",
            "|    policy_gradient_loss | -0.000579  |\n",
            "|    value_loss           | 7.82e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 206         |\n",
            "|    time_elapsed         | 1147        |\n",
            "|    total_timesteps      | 421888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009957934 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.932      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.28e+03    |\n",
            "|    n_updates            | 2050        |\n",
            "|    policy_gradient_loss | 0.00879     |\n",
            "|    value_loss           | 1.59e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 207          |\n",
            "|    time_elapsed         | 1152         |\n",
            "|    total_timesteps      | 423936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053220885 |\n",
            "|    clip_fraction        | 0.157        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.967       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.31e+03     |\n",
            "|    n_updates            | 2060         |\n",
            "|    policy_gradient_loss | 0.003        |\n",
            "|    value_loss           | 8.53e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 208         |\n",
            "|    time_elapsed         | 1158        |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015259234 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.979      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.24e+03    |\n",
            "|    n_updates            | 2070        |\n",
            "|    policy_gradient_loss | -0.00186    |\n",
            "|    value_loss           | 7.08e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 209        |\n",
            "|    time_elapsed         | 1163       |\n",
            "|    total_timesteps      | 428032     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09827377 |\n",
            "|    clip_fraction        | 0.219      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.98      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.66e+03   |\n",
            "|    n_updates            | 2080       |\n",
            "|    policy_gradient_loss | 0.0108     |\n",
            "|    value_loss           | 6.93e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 210         |\n",
            "|    time_elapsed         | 1169        |\n",
            "|    total_timesteps      | 430080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019365855 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.2e+03     |\n",
            "|    n_updates            | 2090        |\n",
            "|    policy_gradient_loss | 0.00218     |\n",
            "|    value_loss           | 6.23e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 211         |\n",
            "|    time_elapsed         | 1175        |\n",
            "|    total_timesteps      | 432128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003472135 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.98e+03    |\n",
            "|    n_updates            | 2100        |\n",
            "|    policy_gradient_loss | 0.00532     |\n",
            "|    value_loss           | 1.18e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 212         |\n",
            "|    time_elapsed         | 1180        |\n",
            "|    total_timesteps      | 434176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033595406 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.46e+03    |\n",
            "|    n_updates            | 2110        |\n",
            "|    policy_gradient_loss | 0.00458     |\n",
            "|    value_loss           | 1.04e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 213         |\n",
            "|    time_elapsed         | 1186        |\n",
            "|    total_timesteps      | 436224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027176544 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.33e+03    |\n",
            "|    n_updates            | 2120        |\n",
            "|    policy_gradient_loss | 0.01        |\n",
            "|    value_loss           | 1.09e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 214         |\n",
            "|    time_elapsed         | 1191        |\n",
            "|    total_timesteps      | 438272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012307566 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.973      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.62e+03    |\n",
            "|    n_updates            | 2130        |\n",
            "|    policy_gradient_loss | 0.00607     |\n",
            "|    value_loss           | 7.56e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 215         |\n",
            "|    time_elapsed         | 1196        |\n",
            "|    total_timesteps      | 440320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055297054 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.27e+03    |\n",
            "|    n_updates            | 2140        |\n",
            "|    policy_gradient_loss | 0.0105      |\n",
            "|    value_loss           | 7.44e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 216         |\n",
            "|    time_elapsed         | 1202        |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005267431 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.969      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.72e+03    |\n",
            "|    n_updates            | 2150        |\n",
            "|    policy_gradient_loss | 0.00365     |\n",
            "|    value_loss           | 9.35e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 217         |\n",
            "|    time_elapsed         | 1207        |\n",
            "|    total_timesteps      | 444416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008708589 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.959      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.19e+03    |\n",
            "|    n_updates            | 2160        |\n",
            "|    policy_gradient_loss | 0.00275     |\n",
            "|    value_loss           | 1.24e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 218         |\n",
            "|    time_elapsed         | 1213        |\n",
            "|    total_timesteps      | 446464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014857817 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.972      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.64e+03    |\n",
            "|    n_updates            | 2170        |\n",
            "|    policy_gradient_loss | 0.00296     |\n",
            "|    value_loss           | 9.6e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 219         |\n",
            "|    time_elapsed         | 1219        |\n",
            "|    total_timesteps      | 448512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016459465 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.15e+03    |\n",
            "|    n_updates            | 2180        |\n",
            "|    policy_gradient_loss | 0.0034      |\n",
            "|    value_loss           | 1.09e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-450000-to-step-450500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-450000-to-step-450500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-450000-to-step-450500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-450000-to-step-450500.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 220          |\n",
            "|    time_elapsed         | 1229         |\n",
            "|    total_timesteps      | 450560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0101061035 |\n",
            "|    clip_fraction        | 0.0993       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.11e+03     |\n",
            "|    n_updates            | 2190         |\n",
            "|    policy_gradient_loss | -0.00215     |\n",
            "|    value_loss           | 1.16e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 221         |\n",
            "|    time_elapsed         | 1235        |\n",
            "|    total_timesteps      | 452608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042479254 |\n",
            "|    clip_fraction        | 0.279       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.87e+03    |\n",
            "|    n_updates            | 2200        |\n",
            "|    policy_gradient_loss | 0.0249      |\n",
            "|    value_loss           | 7.18e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 222         |\n",
            "|    time_elapsed         | 1240        |\n",
            "|    total_timesteps      | 454656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024779582 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.52e+03    |\n",
            "|    n_updates            | 2210        |\n",
            "|    policy_gradient_loss | 0.00137     |\n",
            "|    value_loss           | 6.1e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 223         |\n",
            "|    time_elapsed         | 1246        |\n",
            "|    total_timesteps      | 456704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018924799 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.25e+03    |\n",
            "|    n_updates            | 2220        |\n",
            "|    policy_gradient_loss | 0.0133      |\n",
            "|    value_loss           | 9.98e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 224         |\n",
            "|    time_elapsed         | 1251        |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027612075 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.4e+03     |\n",
            "|    n_updates            | 2230        |\n",
            "|    policy_gradient_loss | 0.00833     |\n",
            "|    value_loss           | 9.73e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 225         |\n",
            "|    time_elapsed         | 1257        |\n",
            "|    total_timesteps      | 460800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060130764 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.35e+03    |\n",
            "|    n_updates            | 2240        |\n",
            "|    policy_gradient_loss | 0.00901     |\n",
            "|    value_loss           | 1.7e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 226         |\n",
            "|    time_elapsed         | 1262        |\n",
            "|    total_timesteps      | 462848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005435529 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.41e+03    |\n",
            "|    n_updates            | 2250        |\n",
            "|    policy_gradient_loss | 0.0101      |\n",
            "|    value_loss           | 1.04e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 227         |\n",
            "|    time_elapsed         | 1268        |\n",
            "|    total_timesteps      | 464896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011576591 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.73e+03    |\n",
            "|    n_updates            | 2260        |\n",
            "|    policy_gradient_loss | 0.00209     |\n",
            "|    value_loss           | 1.04e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 228          |\n",
            "|    time_elapsed         | 1274         |\n",
            "|    total_timesteps      | 466944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058869133 |\n",
            "|    clip_fraction        | 0.105        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.932       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.84e+03     |\n",
            "|    n_updates            | 2270         |\n",
            "|    policy_gradient_loss | -0.000438    |\n",
            "|    value_loss           | 1.08e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 229         |\n",
            "|    time_elapsed         | 1279        |\n",
            "|    total_timesteps      | 468992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007529484 |\n",
            "|    clip_fraction        | 0.0549      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.943      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.62e+03    |\n",
            "|    n_updates            | 2280        |\n",
            "|    policy_gradient_loss | -0.00291    |\n",
            "|    value_loss           | 1.83e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 366        |\n",
            "|    iterations           | 230        |\n",
            "|    time_elapsed         | 1285       |\n",
            "|    total_timesteps      | 471040     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14768596 |\n",
            "|    clip_fraction        | 0.134      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.942     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.35e+03   |\n",
            "|    n_updates            | 2290       |\n",
            "|    policy_gradient_loss | 0.00736    |\n",
            "|    value_loss           | 1.49e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 231         |\n",
            "|    time_elapsed         | 1290        |\n",
            "|    total_timesteps      | 473088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004845797 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.965      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.03e+03    |\n",
            "|    n_updates            | 2300        |\n",
            "|    policy_gradient_loss | 0.00106     |\n",
            "|    value_loss           | 1.03e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 232         |\n",
            "|    time_elapsed         | 1296        |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012363853 |\n",
            "|    clip_fraction        | 0.0775      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.946      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.79e+03    |\n",
            "|    n_updates            | 2310        |\n",
            "|    policy_gradient_loss | 7.96e-05    |\n",
            "|    value_loss           | 1.15e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 233         |\n",
            "|    time_elapsed         | 1301        |\n",
            "|    total_timesteps      | 477184      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005723311 |\n",
            "|    clip_fraction        | 0.0858      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.911      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.29e+03    |\n",
            "|    n_updates            | 2320        |\n",
            "|    policy_gradient_loss | 0.00334     |\n",
            "|    value_loss           | 9.97e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 234         |\n",
            "|    time_elapsed         | 1307        |\n",
            "|    total_timesteps      | 479232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042000644 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.911      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.65e+03    |\n",
            "|    n_updates            | 2330        |\n",
            "|    policy_gradient_loss | 0.0129      |\n",
            "|    value_loss           | 8.55e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 235         |\n",
            "|    time_elapsed         | 1312        |\n",
            "|    total_timesteps      | 481280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006836256 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.922      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.33e+03    |\n",
            "|    n_updates            | 2340        |\n",
            "|    policy_gradient_loss | 0.00417     |\n",
            "|    value_loss           | 9.19e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 236         |\n",
            "|    time_elapsed         | 1318        |\n",
            "|    total_timesteps      | 483328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007719048 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.9        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.92e+03    |\n",
            "|    n_updates            | 2350        |\n",
            "|    policy_gradient_loss | -0.00189    |\n",
            "|    value_loss           | 9.88e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 237         |\n",
            "|    time_elapsed         | 1323        |\n",
            "|    total_timesteps      | 485376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014048449 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.832      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.93e+03    |\n",
            "|    n_updates            | 2360        |\n",
            "|    policy_gradient_loss | 0.00277     |\n",
            "|    value_loss           | 9.39e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 238          |\n",
            "|    time_elapsed         | 1328         |\n",
            "|    total_timesteps      | 487424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052449964 |\n",
            "|    clip_fraction        | 0.0502       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.824       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.97e+03     |\n",
            "|    n_updates            | 2370         |\n",
            "|    policy_gradient_loss | 0.000137     |\n",
            "|    value_loss           | 7.28e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 239          |\n",
            "|    time_elapsed         | 1334         |\n",
            "|    total_timesteps      | 489472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035144591 |\n",
            "|    clip_fraction        | 0.15         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.875       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.84e+03     |\n",
            "|    n_updates            | 2380         |\n",
            "|    policy_gradient_loss | 0.00467      |\n",
            "|    value_loss           | 9.63e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 240         |\n",
            "|    time_elapsed         | 1339        |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017311208 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.845      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.07e+03    |\n",
            "|    n_updates            | 2390        |\n",
            "|    policy_gradient_loss | 0.00678     |\n",
            "|    value_loss           | 1.16e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 241         |\n",
            "|    time_elapsed         | 1345        |\n",
            "|    total_timesteps      | 493568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013968249 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.836      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.37e+03    |\n",
            "|    n_updates            | 2400        |\n",
            "|    policy_gradient_loss | 0.015       |\n",
            "|    value_loss           | 7.99e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 242          |\n",
            "|    time_elapsed         | 1350         |\n",
            "|    total_timesteps      | 495616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038892413 |\n",
            "|    clip_fraction        | 0.0606       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.905       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.72e+03     |\n",
            "|    n_updates            | 2410         |\n",
            "|    policy_gradient_loss | 0.00195      |\n",
            "|    value_loss           | 7.44e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 243         |\n",
            "|    time_elapsed         | 1356        |\n",
            "|    total_timesteps      | 497664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009233952 |\n",
            "|    clip_fraction        | 0.238       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.884      |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.82e+03    |\n",
            "|    n_updates            | 2420        |\n",
            "|    policy_gradient_loss | 0.00943     |\n",
            "|    value_loss           | 8.21e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 244         |\n",
            "|    time_elapsed         | 1361        |\n",
            "|    total_timesteps      | 499712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019310609 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.904      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.51e+03    |\n",
            "|    n_updates            | 2430        |\n",
            "|    policy_gradient_loss | 0.00798     |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-500000-to-step-500500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-500000-to-step-500500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-500000-to-step-500500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-500000-to-step-500500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 365         |\n",
            "|    iterations           | 245         |\n",
            "|    time_elapsed         | 1372        |\n",
            "|    total_timesteps      | 501760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009280353 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.881      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.21e+03    |\n",
            "|    n_updates            | 2440        |\n",
            "|    policy_gradient_loss | -0.00203    |\n",
            "|    value_loss           | 9.25e+03    |\n",
            "-----------------------------------------\n",
            "\n",
            "Evaluating trained model...\n",
            "Episode 1 Total Reward: -453.76886425960555\n",
            "Episode 2 Total Reward: -556.7479485787946\n",
            "Episode 3 Total Reward: -530.8630460414786\n",
            "Episode 4 Total Reward: -1455.7182031587759\n",
            "Episode 5 Total Reward: -828.631313606103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOU CAN WORK DOWN BELOW HERE WITH THE SAME CODE AS ABOVE"
      ],
      "metadata": {
        "id": "H40IH13IUaxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            if (contact.fixtureA.body == lander['body'] or\n",
        "                contact.fixtureB.body == lander['body']):\n",
        "                other_body = contact.fixtureB.body if contact.fixtureA.body == lander['body'] else contact.fixtureA.body\n",
        "                if other_body == self.env.ground and lander['body'].linearVelocity.length > 2.0:\n",
        "                    self.env.game_over = True\n",
        "                    lander['crashed'] = True\n",
        "                    return\n",
        "\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = True\n",
        "                        contact_pos = None\n",
        "                        if contact.fixtureA.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "                        elif contact.fixtureB.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "\n",
        "                        if contact_pos is not None:\n",
        "                            lander['on_safe_zone'] = self.env.check_on_safe_zone(contact_pos, i)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = False\n",
        "                        if not lander['legs_contact'][0] or not lander['legs_contact'][1]:\n",
        "                            lander['on_safe_zone'] = False\n",
        "\n",
        "class DualLunarLander(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"dual_lunar_lander_v0\",\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.possible_agents = [\"lander_1\", \"lander_2\"]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.possible_agents)}\n",
        "\n",
        "        # Physics parameters\n",
        "        self.FPS = 50\n",
        "        self.SCALE = 30.0\n",
        "        self.LEG_DOWN = 18\n",
        "        self.LEG_W, self.LEG_H = 2, 8\n",
        "        self.ENGINE_POWER = 10.0\n",
        "        self.SIDE_ENGINE_POWER = 0.4\n",
        "        self.INITIAL_RANDOM = 500.0\n",
        "        self.MAIN_ENGINE_THRUST_SCALE = 15  # Scale for visualizing main engine thrust\n",
        "\n",
        "        # Rendering parameters\n",
        "        self.VIEWPORT_W = 600\n",
        "        self.VIEWPORT_H = 400\n",
        "        self.TERRAIN_CHUNKS = 25  # Increased chunks for more detail\n",
        "        self.TERRAIN_HEIGHT = self.VIEWPORT_H / self.SCALE / 4\n",
        "        self.TERRAIN_STEP = 1.0 / self.TERRAIN_CHUNKS\n",
        "        self.SAFE_ZONE_WIDTH = 2.0\n",
        "        self.safe_zones = []\n",
        "        self.SAFE_LANDING_VEL = 0.8\n",
        "\n",
        "        # Game state\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        # Spaces\n",
        "        self.action_spaces = {agent: spaces.Discrete(4) for agent in self.possible_agents}\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=np.array([-1, -1, -5, -5, -np.pi, -5, 0, 0, -1, -1, 0], dtype=np.float32),\n",
        "                high=np.array([1, 1, 5, 5, np.pi, 5, 1, 1, 1, 1, 1], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            ) for agent in self.possible_agents\n",
        "        }\n",
        "\n",
        "        # Physics engine\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.terrain = None\n",
        "        self.moon = None\n",
        "        self.ground = None\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.sync_reward_scale = 10.0\n",
        "        self.both_landed = False\n",
        "\n",
        "        # Colors\n",
        "        self.sky_color = (0, 0, 0)\n",
        "        self.moon_color = (102, 102, 102)\n",
        "        self.terrain_color = (153, 153, 153)\n",
        "        self.safe_zone_color = (0, 204, 0)\n",
        "        self.lander_color = [(204, 0, 0), (0, 0, 204)]\n",
        "        self.leg_color = (102, 102, 102)\n",
        "        self.engine_color = (255, 255, 0)\n",
        "\n",
        "        # Particle effects for thrusters\n",
        "        self.particles = []\n",
        "        self.particle_lifetime = 20  # frames\n",
        "\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def check_on_safe_zone(self, x_pos, lander_idx):\n",
        "        for i, (left, right) in enumerate(self.safe_zones):\n",
        "            if i == lander_idx and left <= x_pos <= right:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self._destroy()\n",
        "        self.world = Box2D.b2World(gravity=(0, -10))\n",
        "        self.world.contactListener = ContactDetector(self)\n",
        "        self.moon = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.game_over = False\n",
        "        self.both_landed = False\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.safe_zones = []\n",
        "\n",
        "        self._create_terrain()\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            safe_zone_center = (self.safe_zones[i][0] + self.safe_zones[i][1]) / 2\n",
        "            self._create_lander(agent, safe_zone_center)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        observations = {}\n",
        "        for agent in self.agents:\n",
        "            observations[agent] = self._get_observation(agent)\n",
        "\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        if self.game_over:\n",
        "            observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "            rewards = {agent: 0 for agent in self.agents}\n",
        "            terminations = {agent: True for agent in self.agents}\n",
        "            truncations = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        # Update particles\n",
        "        self.particles = [p for p in self.particles if p['lifetime'] > 0]\n",
        "        for p in self.particles:\n",
        "            p['lifetime'] -= 1\n",
        "            p['pos'][0] += p['vel'][0]\n",
        "            p['pos'][1] += p['vel'][1]\n",
        "            p['vel'][1] -= 0.05  # Add gravity to particles\n",
        "\n",
        "        for agent, action in actions.items():\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "\n",
        "            if action == 1:  # Main engine\n",
        "                ox = np.sin(lander['body'].angle) * self.SIDE_ENGINE_POWER\n",
        "                oy = -np.cos(lander['body'].angle) * self.ENGINE_POWER\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((ox, oy), impulse_pos, True)\n",
        "                lander['main_engine'] = True\n",
        "\n",
        "                # Add particles for main engine\n",
        "                self._add_thrust_particles(lander, 'main')\n",
        "            else:\n",
        "                lander['main_engine'] = False\n",
        "\n",
        "            if action == 2:  # Left engine\n",
        "                impulse_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((-self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_left'] = True\n",
        "\n",
        "                # Add particles for left engine\n",
        "                self._add_thrust_particles(lander, 'left')\n",
        "            else:\n",
        "                lander['side_engine_left'] = False\n",
        "\n",
        "            if action == 3:  # Right engine\n",
        "                impulse_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((self.SIDE_ENGINE_POWER, 0), impulse_pos, True)\n",
        "                lander['side_engine_right'] = True\n",
        "\n",
        "                # Add particles for right engine\n",
        "                self._add_thrust_particles(lander, 'right')\n",
        "            else:\n",
        "                lander['side_engine_right'] = False\n",
        "\n",
        "        self.world.Step(1.0 / self.FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        terminations = {agent: False for agent in self.agents}\n",
        "        truncations = {agent: False for agent in self.agents}\n",
        "        infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "            vel_len = vel.length\n",
        "            angle = lander['body'].angle\n",
        "            angular_vel = lander['body'].angularVelocity\n",
        "\n",
        "            reward = 0\n",
        "            safe_zone_center = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "            distance_to_safe_zone = abs(pos[0] - safe_zone_center)\n",
        "\n",
        "            shaping = (\n",
        "                -100 * distance_to_safe_zone\n",
        "                - 100 * abs(vel[0])\n",
        "                - 100 * abs(vel[1])\n",
        "                - 100 * abs(angle)\n",
        "                - 100 * abs(angular_vel)\n",
        "            )\n",
        "\n",
        "            reward += (shaping - self.prev_shaping[idx]) * 0.1\n",
        "            self.prev_shaping[idx] = shaping\n",
        "\n",
        "            reward -= 0.1 if lander['main_engine'] else 0\n",
        "            reward -= 0.03 if lander['side_engine_left'] or lander['side_engine_right'] else 0\n",
        "\n",
        "            landed = False\n",
        "            if (lander['legs_contact'][0] and lander['legs_contact'][1]) and vel_len < self.SAFE_LANDING_VEL and abs(angle) < 0.1:\n",
        "                landed = True\n",
        "\n",
        "                if lander['on_safe_zone']:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[agent] = 200\n",
        "                        reward += 200\n",
        "                        if vel_len < 0.3:\n",
        "                            reward += 50\n",
        "                else:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[agent] = 50\n",
        "                        reward += 50\n",
        "\n",
        "            if lander.get('crashed', False) or pos[0] < 0 or pos[0] > self.VIEWPORT_W / self.SCALE or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE):\n",
        "                reward -= 100\n",
        "                terminations[agent] = True\n",
        "                if not 0 <= pos[1] < self.VIEWPORT_H / self.SCALE:\n",
        "                    lander['outside_bounds'] = True\n",
        "\n",
        "            rewards[agent] = reward\n",
        "\n",
        "        all_landed_safely = all(self.landing_rewards[agent] >= 100 for agent in self.agents)\n",
        "        if all_landed_safely and not self.both_landed:\n",
        "            sync_reward = self.sync_reward_scale * 200\n",
        "            for agent in self.agents:\n",
        "                rewards[agent] += sync_reward\n",
        "            self.both_landed = True\n",
        "\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "\n",
        "            if lander.get('crashed', False) or lander.get('outside_bounds', False):\n",
        "                terminations[agent] = True\n",
        "            elif (lander['legs_contact'][0] and lander['legs_contact'][1]) and vel.length < self.SAFE_LANDING_VEL:\n",
        "                if lander['on_safe_zone'] and all_landed_safely:\n",
        "                    terminations[agent] = True\n",
        "\n",
        "        self.game_over = all(terminations.values())\n",
        "\n",
        "        observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    def _add_thrust_particles(self, lander, engine_type):\n",
        "        \"\"\"Add particle effects for thruster visualization\"\"\"\n",
        "        num_particles = np.random.randint(3, 7)\n",
        "\n",
        "        if engine_type == 'main':\n",
        "            # Main engine particles\n",
        "            angle = lander['body'].angle\n",
        "            base_pos = (lander['body'].position[0], lander['body'].position[1] - 0.5)\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                spread = np.random.uniform(-0.2, 0.2)\n",
        "                vel_x = np.sin(angle + spread) * np.random.uniform(0.2, 0.5)\n",
        "                vel_y = -np.cos(angle + spread) * np.random.uniform(0.5, 1.2)\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 255, 0),\n",
        "                    'lifetime': np.random.randint(10, self.particle_lifetime),\n",
        "                    'size': np.random.uniform(1, 3)\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'left':\n",
        "            # Left side engine particles\n",
        "            angle = lander['body'].angle\n",
        "            base_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = -np.random.uniform(0.3, 0.7)\n",
        "                vel_y = np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(5, 15),\n",
        "                    'size': np.random.uniform(1, 2)\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'right':\n",
        "            # Right side engine particles\n",
        "            angle = lander['body'].angle\n",
        "            base_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = np.random.uniform(0.3, 0.7)\n",
        "                vel_y = np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(5, 15),\n",
        "                    'size': np.random.uniform(1, 2)\n",
        "                })\n",
        "\n",
        "    def _destroy(self):\n",
        "        if self.world is None:\n",
        "            return\n",
        "\n",
        "        for lander in self.landers:\n",
        "            self.world.DestroyBody(lander['body'])\n",
        "            for leg in lander['legs']:\n",
        "                self.world.DestroyBody(leg)\n",
        "\n",
        "        if self.moon:\n",
        "            self.world.DestroyBody(self.moon)\n",
        "\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "\n",
        "    def _create_terrain(self):\n",
        "        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]))\n",
        "        self.moon.CreateFixture(\n",
        "            shape=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]),\n",
        "            friction=0.1,\n",
        "            restitution=0.0,\n",
        "        )\n",
        "        self.ground = self.moon\n",
        "\n",
        "        self.terrain = []\n",
        "        height = self.TERRAIN_HEIGHT\n",
        "        chunk_x = [0]\n",
        "        for i in range(self.TERRAIN_CHUNKS):\n",
        "            chunk_x.append(chunk_x[-1] + self.TERRAIN_STEP)\n",
        "\n",
        "        # Generate smoother terrain with Perlin-like noise\n",
        "        height_data = []\n",
        "\n",
        "        # Initial random heights\n",
        "        base_heights = []\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            h = np.random.uniform(0, height)\n",
        "            base_heights.append(h)\n",
        "\n",
        "        # Apply smoothing to get more natural, curved terrain\n",
        "        # Use a simple moving average for smoothing\n",
        "        window_size = 3\n",
        "        smoothed_heights = []\n",
        "\n",
        "        # Pad the array for smoothing\n",
        "        padded = [base_heights[0]] * (window_size//2) + base_heights + [base_heights[-1]] * (window_size//2)\n",
        "\n",
        "        for i in range(len(base_heights)):\n",
        "            window = padded[i:i+window_size]\n",
        "            smoothed_heights.append(sum(window) / len(window))\n",
        "\n",
        "        height_data = smoothed_heights\n",
        "\n",
        "        landing_pad_1_idx = self.TERRAIN_CHUNKS // 4\n",
        "        landing_pad_2_idx = 3 * self.TERRAIN_CHUNKS // 4\n",
        "\n",
        "        pad_height = self.TERRAIN_HEIGHT/4\n",
        "        pad_width = 3\n",
        "\n",
        "        # Flatten landing pads\n",
        "        for i in range(landing_pad_1_idx - pad_width//2, landing_pad_1_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        for i in range(landing_pad_2_idx - pad_width//2, landing_pad_2_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        pad1_left = (landing_pad_1_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad1_right = (landing_pad_1_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        pad2_left = (landing_pad_2_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad2_right = (landing_pad_2_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        self.safe_zones = [(pad1_left, pad1_right), (pad2_left, pad2_right)]\n",
        "\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            x1 = chunk_x[i]\n",
        "            x2 = chunk_x[i+1]\n",
        "            h1 = height_data[i]\n",
        "\n",
        "            poly = [\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, 0),\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, 0)\n",
        "            ]\n",
        "\n",
        "            self.terrain.append((poly,\n",
        "                               (landing_pad_1_idx - pad_width//2 <= i <= landing_pad_1_idx + pad_width//2) or\n",
        "                               (landing_pad_2_idx - pad_width//2 <= i <= landing_pad_2_idx + pad_width//2)))\n",
        "\n",
        "            self.moon.CreateFixture(\n",
        "                shape=polygonShape(vertices=poly),\n",
        "                friction=0.4,\n",
        "                restitution=0.0\n",
        "            )\n",
        "\n",
        "    def _create_lander(self, agent, initial_x):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "\n",
        "        init_x = initial_x + np.random.uniform(-0.3, 0.3)\n",
        "        init_y = self.VIEWPORT_H / self.SCALE * 0.8\n",
        "\n",
        "        # Modified lander shape - more like the classic lunar lander\n",
        "        lander_body = self.world.CreateDynamicBody(\n",
        "            position=(init_x, init_y),\n",
        "            angle=np.random.uniform(-0.2, 0.2),\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(vertices=[\n",
        "                    (-0.4, 0.0),  # Bottom left\n",
        "                    (0.4, 0.0),   # Bottom right\n",
        "                    (0.25, -0.6),  # Mid right\n",
        "                    (0.0, -1.2),  # Top\n",
        "                    (-0.25, -0.6)  # Mid left\n",
        "                ]),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,\n",
        "                restitution=0.0\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        lander_body.linearVelocity.Set(\n",
        "            np.random.uniform(-1.0, 1.0),\n",
        "            np.random.uniform(-1.0, 0.0)\n",
        "        )\n",
        "        lander_body.angularVelocity = np.random.uniform(-0.5, 0.5)\n",
        "\n",
        "        legs = []\n",
        "        legs_contact = [False, False]\n",
        "\n",
        "        for i in [-1, 1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(init_x + i * 0.3, init_y - 0.3),  # Adjusted position to match new lander shape\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(vertices=[\n",
        "                        (0, 0),\n",
        "                        (0, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, 0)\n",
        "                    ]),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    friction=0.2,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            legs.append(leg)\n",
        "\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=lander_body,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(i * 0.25, -0.3),  # Adjusted anchor to match new lander shape\n",
        "                localAnchorB=(0, 0),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=100,\n",
        "                motorSpeed=0.3 * i,\n",
        "                lowerAngle=-0.6,\n",
        "                upperAngle=0.6\n",
        "            )\n",
        "            self.world.CreateJoint(rjd)\n",
        "\n",
        "        self.landers.append({\n",
        "            'body': lander_body,\n",
        "            'legs': legs,\n",
        "            'legs_contact': legs_contact,\n",
        "            'main_engine': False,\n",
        "            'side_engine_left': False,\n",
        "            'side_engine_right': False,\n",
        "            'on_safe_zone': False,\n",
        "            'crashed': False,\n",
        "            'outside_bounds': False\n",
        "        })\n",
        "\n",
        "    def _get_observation(self, agent):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "        lander = self.landers[idx]\n",
        "        pos = lander['body'].position\n",
        "        vel = lander['body'].linearVelocity\n",
        "\n",
        "        safe_zone_center_x = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "        safe_zone_width = self.safe_zones[idx][1] - self.safe_zones[idx][0]\n",
        "\n",
        "        dist_to_safe_zone = (pos[0] - safe_zone_center_x) / (self.VIEWPORT_W / self.SCALE)\n",
        "\n",
        "        observation = np.array([\n",
        "            (pos[0] - self.VIEWPORT_W / self.SCALE / 2) / (self.VIEWPORT_W / self.SCALE / 2),\n",
        "            (pos[1] - self.VIEWPORT_H / self.SCALE / 2) / (self.VIEWPORT_H / self.SCALE / 2),\n",
        "            vel[0] / 5.0,\n",
        "            vel[1] / 5.0,\n",
        "            lander['body'].angle,\n",
        "            lander['body'].angularVelocity / 5.0,\n",
        "            1.0 if lander['legs_contact'][0] else 0.0,\n",
        "            1.0 if lander['legs_contact'][1] else 0.0,\n",
        "            dist_to_safe_zone,\n",
        "            safe_zone_width / (self.VIEWPORT_W / self.SCALE),\n",
        "            1.0 if lander['on_safe_zone'] else 0.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _init_renderer(self):\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            pygame.display.set_caption(\"Dual Lunar Lander\")\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return None\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        surf = self.screen if self.render_mode == \"human\" else pygame.Surface((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "        surf.fill(self.sky_color)\n",
        "\n",
        "        def transform(pos):\n",
        "            return (int(pos[0] * self.SCALE), self.VIEWPORT_H - int(pos[1] * self.SCALE))\n",
        "\n",
        "        # Draw terrain\n",
        "        for poly, is_safe_zone in self.terrain:\n",
        "            transformed_poly = [transform(p) for p in poly]\n",
        "            color = self.safe_zone_color if is_safe_zone else self.terrain_color\n",
        "            pygame.draw.polygon(surf, color, transformed_poly)\n",
        "\n",
        "        # Draw landing pads\n",
        "        for left, right in self.safe_zones:\n",
        "            h = self.TERRAIN_HEIGHT/4\n",
        "            pad_poly = [\n",
        "                transform((left, h)),\n",
        "                transform((right, h)),\n",
        "                transform((right, h-0.05)),\n",
        "                transform((left, h-0.05))\n",
        "            ]\n",
        "            pygame.draw.polygon(surf, (255, 255, 255), pad_poly)\n",
        "\n",
        "        # Draw particles - render before landers for proper layering\n",
        "        for particle in self.particles:\n",
        "            pos = transform(particle['pos'])\n",
        "            color = particle['color']\n",
        "            # Fade out particles as they age\n",
        "            alpha = int(255 * (particle['lifetime'] / self.particle_lifetime))\n",
        "            color = (color[0], color[1], min(255, color[2] + int(alpha/2)))\n",
        "            size = max(1, int(particle['size']))\n",
        "            pygame.draw.circle(surf, color, pos, size)\n",
        "\n",
        "        # Draw landers\n",
        "        for i, lander in enumerate(self.landers):\n",
        "            lander_color = self.lander_color[i]\n",
        "\n",
        "            # Draw body\n",
        "            vertices = []\n",
        "            for fixture in lander['body'].fixtures:\n",
        "                for vertex in fixture.shape.vertices:\n",
        "                    vertices.append(transform(lander['body'].transform * vertex))\n",
        "            pygame.draw.polygon(surf, lander_color, vertices)\n",
        "\n",
        "            # Draw legs\n",
        "            for leg in lander['legs']:\n",
        "                vertices = []\n",
        "                for fixture in leg.fixtures:\n",
        "                    for vertex in fixture.shape.vertices:\n",
        "                        vertices.append(transform(leg.transform * vertex))\n",
        "                pygame.draw.polygon(surf, self.leg_color, vertices)\n",
        "\n",
        "            # Draw engine effects\n",
        "            if lander['main_engine']:\n",
        "                start_pos = transform(lander['body'].position + (0, -0.3))\n",
        "                flame_length = self.MAIN_ENGINE_THRUST_SCALE + np.random.randint(0, 5)\n",
        "                end_pos = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length\n",
        "                )\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 3)\n",
        "\n",
        "                # Draw inner flame (white)\n",
        "                inner_end = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length * 0.6,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length * 0.6\n",
        "                )\n",
        "                pygame.draw.line(surf, (255, 255, 255), start_pos, inner_end, 2)\n",
        "\n",
        "            if lander['side_engine_left']:\n",
        "                start_pos = transform(lander['body'].position + (-0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] - flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "            if lander['side_engine_right']:\n",
        "                start_pos = transform(lander['body'].position + (0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] + flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "            return None\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "            self.isopen = False\n",
        "\n",
        "class SB3Wrapper(gym.Env):\n",
        "    \"\"\"Custom Gym wrapper for Stable Baselines3 compatibility\"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_space = self.env.observation_space(self.agents[0])\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([obs_space.low]*2),\n",
        "            high=np.concatenate([obs_space.high]*2),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, info = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), info\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to dict\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "class StableBaselinesWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    Converts ParallelEnv to Gym Env for Stable Baselines3 compatibility\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_spaces = [self.env.observation_space(agent) for agent in self.agents]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([space.low for space in obs_spaces]),\n",
        "            high=np.concatenate([space.high for space in obs_spaces]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Important: Properly set metadata and render_mode\n",
        "        self.metadata = self.env.metadata\n",
        "        self.render_mode = env.render_mode\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, _ = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to action dictionary\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        # Convert to Gym step format\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms, 'truncations': truncs}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "def train_agents(total_timesteps=500000, render_every=50000, log_dir=\"./logs\"):\n",
        "    # Create and wrap environment\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    # Ensure render_mode is properly set\n",
        "    if not hasattr(wrapped_env, 'render_mode') or wrapped_env.render_mode != \"rgb_array\":\n",
        "        wrapped_env.render_mode = \"rgb_array\"\n",
        "\n",
        "    vec_env = DummyVecEnv([lambda: wrapped_env])\n",
        "\n",
        "    # Setup video recording\n",
        "    video_folder = os.path.join(log_dir, \"videos\")\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    vec_env = VecVideoRecorder(\n",
        "        vec_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda x: x % render_every == 0,\n",
        "        video_length=500,\n",
        "        name_prefix=\"dual_lander\"\n",
        "    )\n",
        "\n",
        "    # Create and train model\n",
        "    model = PPO(\n",
        "        MlpPolicy,\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        tensorboard_log=log_dir,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        policy_kwargs={\"net_arch\": [dict(pi=[256, 256], vf=[256, 256])]}\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save(os.path.join(log_dir, \"dual_lander_model\"))\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, num_episodes=5, render=True):\n",
        "    # Create environment with rendering\n",
        "    env = DualLunarLander(render_mode=\"human\" if render else None)\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = wrapped_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = wrapped_env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()  # Render original environment\n",
        "\n",
        "        print(f\"Episode {episode+1} Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training dual lunar landers...\")\n",
        "    model = train_agents()\n",
        "\n",
        "    # Evaluate with rendering\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    evaluate_model(model, num_episodes=5, render=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XfPwOAKcj3Ma",
        "outputId": "d9406278-0f2a-484f-a60e-3379fbd657c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dual lunar landers...\n",
            "Using cpu device\n",
            "Logging to ./logs/PPO_5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/logs/videos/dual_lander-step-0-to-step-500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-0-to-step-500.mp4\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 168  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 12   |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 215         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007685636 |\n",
            "|    clip_fraction        | 0.0583      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.77       |\n",
            "|    explained_variance   | 0.00018     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.82e+05    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    value_loss           | 3.64e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 227          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048444727 |\n",
            "|    clip_fraction        | 0.0121       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.76        |\n",
            "|    explained_variance   | -0.0074      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.5e+05      |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00431     |\n",
            "|    value_loss           | 5.1e+05      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 240          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 34           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067951484 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.75        |\n",
            "|    explained_variance   | -0.000581    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.29e+05     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00856     |\n",
            "|    value_loss           | 3.33e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 244          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063080955 |\n",
            "|    clip_fraction        | 0.0368       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.74        |\n",
            "|    explained_variance   | -0.000131    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.38e+05     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.012       |\n",
            "|    value_loss           | 3.97e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 250          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 49           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058947164 |\n",
            "|    clip_fraction        | 0.0289       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.72        |\n",
            "|    explained_variance   | 0.000659     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.44e+05     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00753     |\n",
            "|    value_loss           | 3.47e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 255         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006004807 |\n",
            "|    clip_fraction        | 0.0275      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.71       |\n",
            "|    explained_variance   | -0.000199   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.08e+05    |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00691    |\n",
            "|    value_loss           | 2.73e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 257        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 63         |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00436867 |\n",
            "|    clip_fraction        | 0.0153     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.7       |\n",
            "|    explained_variance   | -0.000582  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.55e+05   |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.00502   |\n",
            "|    value_loss           | 3.24e+05   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056616683 |\n",
            "|    clip_fraction        | 0.0253       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.68        |\n",
            "|    explained_variance   | -0.00385     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.15e+05     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00821     |\n",
            "|    value_loss           | 5.72e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 262         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006541349 |\n",
            "|    clip_fraction        | 0.036       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.66       |\n",
            "|    explained_variance   | -0.000215   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.24e+05    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00811    |\n",
            "|    value_loss           | 2.6e+05     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 266          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 84           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062603024 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.66        |\n",
            "|    explained_variance   | -1.82e-05    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.77e+05     |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00834     |\n",
            "|    value_loss           | 2.51e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 267          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061072493 |\n",
            "|    clip_fraction        | 0.0346       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.65        |\n",
            "|    explained_variance   | 0.000113     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25e+05     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00791     |\n",
            "|    value_loss           | 2.26e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 269          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 98           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053735934 |\n",
            "|    clip_fraction        | 0.0314       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | -2.03e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.31e+05     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.0102      |\n",
            "|    value_loss           | 2.85e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 268        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 106        |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00611754 |\n",
            "|    clip_fraction        | 0.0426     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.6       |\n",
            "|    explained_variance   | -2.38e-06  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.6e+04    |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.00821   |\n",
            "|    value_loss           | 1.37e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 114         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007857675 |\n",
            "|    clip_fraction        | 0.0496      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.58       |\n",
            "|    explained_variance   | -0.0244     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.75e+04    |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    value_loss           | 2.22e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 270          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065004025 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.56        |\n",
            "|    explained_variance   | 1.67e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+05     |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00684     |\n",
            "|    value_loss           | 1.45e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 128         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004885563 |\n",
            "|    clip_fraction        | 0.0269      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.53       |\n",
            "|    explained_variance   | -0.000386   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.14e+04    |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00749    |\n",
            "|    value_loss           | 1.67e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 271          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 135          |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051624407 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.52        |\n",
            "|    explained_variance   | -0.117       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.5e+05      |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.00559     |\n",
            "|    value_loss           | 2.77e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 271          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 143          |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060044406 |\n",
            "|    clip_fraction        | 0.036        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.51        |\n",
            "|    explained_variance   | -0.000212    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.24e+04     |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00942     |\n",
            "|    value_loss           | 2.52e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005814772 |\n",
            "|    clip_fraction        | 0.0493      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.48       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.33e+04    |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00946    |\n",
            "|    value_loss           | 1.6e+05     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 271          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 158          |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073996773 |\n",
            "|    clip_fraction        | 0.0429       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.46        |\n",
            "|    explained_variance   | -1.08e-05    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.37e+04     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00987     |\n",
            "|    value_loss           | 1.88e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 166         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006801704 |\n",
            "|    clip_fraction        | 0.0411      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.45       |\n",
            "|    explained_variance   | -7.7e-05    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.26e+05    |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00948    |\n",
            "|    value_loss           | 2.35e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 272        |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 172        |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00596153 |\n",
            "|    clip_fraction        | 0.0218     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.44      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.16e+05   |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.00797   |\n",
            "|    value_loss           | 2.84e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 272         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 180         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010959572 |\n",
            "|    clip_fraction        | 0.0993      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.4        |\n",
            "|    explained_variance   | -3.58e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.47e+04    |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0147     |\n",
            "|    value_loss           | 1.06e+05    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-50000-to-step-50500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 262         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 194         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007274121 |\n",
            "|    clip_fraction        | 0.046       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.37       |\n",
            "|    explained_variance   | -4.77e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.63e+04    |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00974    |\n",
            "|    value_loss           | 1.31e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 264         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 201         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008185248 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.31       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.82e+04    |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | 0.000691    |\n",
            "|    value_loss           | 9.86e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 263         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 209         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008537074 |\n",
            "|    clip_fraction        | 0.055       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.31       |\n",
            "|    explained_variance   | -4.77e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.71e+04    |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00965    |\n",
            "|    value_loss           | 1.49e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 263         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 217         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005617046 |\n",
            "|    clip_fraction        | 0.0164      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.26       |\n",
            "|    explained_variance   | -4.89e-06   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.85e+04    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00529    |\n",
            "|    value_loss           | 2.52e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 264          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 224          |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068789194 |\n",
            "|    clip_fraction        | 0.0513       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.23        |\n",
            "|    explained_variance   | -1.07e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.12e+05     |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00923     |\n",
            "|    value_loss           | 1.88e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 264         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 231         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027619952 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.08       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.43e+04    |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | 0.000702    |\n",
            "|    value_loss           | 9.37e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 265         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 238         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013567313 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.11       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.19e+04    |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.00329    |\n",
            "|    value_loss           | 8.69e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 266        |\n",
            "|    iterations           | 32         |\n",
            "|    time_elapsed         | 246        |\n",
            "|    total_timesteps      | 65536      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03559181 |\n",
            "|    clip_fraction        | 0.233      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.06      |\n",
            "|    explained_variance   | -3.58e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.68e+04   |\n",
            "|    n_updates            | 310        |\n",
            "|    policy_gradient_loss | 0.006      |\n",
            "|    value_loss           | 5.81e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 267          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 252          |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056702015 |\n",
            "|    clip_fraction        | 0.0625       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.11        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.54e+04     |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.00548     |\n",
            "|    value_loss           | 8.22e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 267        |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 260        |\n",
            "|    total_timesteps      | 69632      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00786285 |\n",
            "|    clip_fraction        | 0.109      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.03      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.7e+04    |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | 0.00174    |\n",
            "|    value_loss           | 8e+04      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 267         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.091285706 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.01       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.98e+04    |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | 0.0185      |\n",
            "|    value_loss           | 5.12e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 273         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017823761 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.89       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.91e+04    |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | 0.00258     |\n",
            "|    value_loss           | 5.96e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 281         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020549597 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.98e+04    |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | 0.00137     |\n",
            "|    value_loss           | 5.92e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 270          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 287          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074632512 |\n",
            "|    clip_fraction        | 0.129        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.76        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.34e+04     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    value_loss           | 4.92e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 270        |\n",
            "|    iterations           | 39         |\n",
            "|    time_elapsed         | 295        |\n",
            "|    total_timesteps      | 79872      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.24991938 |\n",
            "|    clip_fraction        | 0.345      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.04e+04   |\n",
            "|    n_updates            | 380        |\n",
            "|    policy_gradient_loss | 0.0379     |\n",
            "|    value_loss           | 4.31e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 270        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 302        |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03395769 |\n",
            "|    clip_fraction        | 0.278      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.77      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.12e+04   |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | 0.0362     |\n",
            "|    value_loss           | 4.73e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 309         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012216051 |\n",
            "|    clip_fraction        | 0.0826      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.42e+04    |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | 0.00307     |\n",
            "|    value_loss           | 5.08e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 316         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008959671 |\n",
            "|    clip_fraction        | 0.0835      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.43e+04    |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.000363   |\n",
            "|    value_loss           | 4.04e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 324         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.056337226 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.45e+04    |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | 0.0049      |\n",
            "|    value_loss           | 4.67e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 272          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 331          |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066974163 |\n",
            "|    clip_fraction        | 0.12         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.67        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.54e+04     |\n",
            "|    n_updates            | 430          |\n",
            "|    policy_gradient_loss | 0.00334      |\n",
            "|    value_loss           | 3.82e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 272         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 338         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031378966 |\n",
            "|    clip_fraction        | 0.0327      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.59       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2e+04       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | 0.00182     |\n",
            "|    value_loss           | 3.83e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 272         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 345         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005728538 |\n",
            "|    clip_fraction        | 0.0649      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.83e+03    |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00408    |\n",
            "|    value_loss           | 2.65e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 273         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 352         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012372358 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.4e+04     |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | 0.00221     |\n",
            "|    value_loss           | 3.4e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 273        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 359        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03529635 |\n",
            "|    clip_fraction        | 0.2        |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.32      |\n",
            "|    explained_variance   | 1.79e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.63e+04   |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | 0.0272     |\n",
            "|    value_loss           | 2.37e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 369         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007665507 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.04e+04    |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | 0.018       |\n",
            "|    value_loss           | 3.51e+04    |\n",
            "-----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-100000-to-step-100500.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 269          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 380          |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054597305 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.56e+04     |\n",
            "|    n_updates            | 490          |\n",
            "|    policy_gradient_loss | -0.00156     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 388         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014922054 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.27       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.21e+04    |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    value_loss           | 3.36e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 396         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048734576 |\n",
            "|    clip_fraction        | 0.0869      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.29e+04    |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | 0.00467     |\n",
            "|    value_loss           | 2.36e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 404         |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026203612 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.85e+03    |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | 0.00323     |\n",
            "|    value_loss           | 1.93e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 412         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054994956 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.02e+04    |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | 0.0151      |\n",
            "|    value_loss           | 2.45e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 419         |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016062705 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.26       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.19e+04    |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | 0.00387     |\n",
            "|    value_loss           | 4.21e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 427         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048737172 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.66e+04    |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | 0.00296     |\n",
            "|    value_loss           | 2.58e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 434         |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009517226 |\n",
            "|    clip_fraction        | 0.0653      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.02e+04    |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.00115    |\n",
            "|    value_loss           | 2.26e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 268        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 441        |\n",
            "|    total_timesteps      | 118784     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00789311 |\n",
            "|    clip_fraction        | 0.0902     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.46      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.72e+04   |\n",
            "|    n_updates            | 570        |\n",
            "|    policy_gradient_loss | -0.00296   |\n",
            "|    value_loss           | 4.22e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 449         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014227227 |\n",
            "|    clip_fraction        | 0.0807      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.66       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.39e+04    |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00621    |\n",
            "|    value_loss           | 4.53e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 269        |\n",
            "|    iterations           | 60         |\n",
            "|    time_elapsed         | 455        |\n",
            "|    total_timesteps      | 122880     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08420302 |\n",
            "|    clip_fraction        | 0.39       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.47      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.6e+03    |\n",
            "|    n_updates            | 590        |\n",
            "|    policy_gradient_loss | 0.0199     |\n",
            "|    value_loss           | 3.22e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 463         |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010963526 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.12e+04    |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | 0.0186      |\n",
            "|    value_loss           | 1.65e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 269        |\n",
            "|    iterations           | 62         |\n",
            "|    time_elapsed         | 470        |\n",
            "|    total_timesteps      | 126976     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.26035598 |\n",
            "|    clip_fraction        | 0.408      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.12      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.85e+03   |\n",
            "|    n_updates            | 610        |\n",
            "|    policy_gradient_loss | 0.0516     |\n",
            "|    value_loss           | 2.02e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 269          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 478          |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0096948575 |\n",
            "|    clip_fraction        | 0.0725       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.806       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.17e+04     |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.000403    |\n",
            "|    value_loss           | 2.21e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 486         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017132161 |\n",
            "|    clip_fraction        | 0.184       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.967      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.16e+03    |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | 0.0153      |\n",
            "|    value_loss           | 2.65e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 493         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.052625664 |\n",
            "|    clip_fraction        | 0.184       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.999      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.57e+03    |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | 0.0115      |\n",
            "|    value_loss           | 1.36e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 500         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.052215267 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.55e+03    |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | 0.00382     |\n",
            "|    value_loss           | 1.33e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 507         |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011500318 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.967      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.96e+03    |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | 0.003       |\n",
            "|    value_loss           | 1.45e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 515         |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008187119 |\n",
            "|    clip_fraction        | 0.068       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.21e+03    |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | 0.00113     |\n",
            "|    value_loss           | 1.45e+04    |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 270       |\n",
            "|    iterations           | 69        |\n",
            "|    time_elapsed         | 522       |\n",
            "|    total_timesteps      | 141312    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1746341 |\n",
            "|    clip_fraction        | 0.124     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.873    |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.98e+03  |\n",
            "|    n_updates            | 680       |\n",
            "|    policy_gradient_loss | 0.00374   |\n",
            "|    value_loss           | 1.53e+04  |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 529         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020343961 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.65e+03    |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | 0.00729     |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 537         |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006246003 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.27       |\n",
            "|    explained_variance   | -3.58e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.25e+03    |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | 6.75e-05    |\n",
            "|    value_loss           | 1.56e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 270         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 544         |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013796192 |\n",
            "|    clip_fraction        | 0.387       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.62e+03    |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | 0.0419      |\n",
            "|    value_loss           | 1.42e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 270        |\n",
            "|    iterations           | 73         |\n",
            "|    time_elapsed         | 551        |\n",
            "|    total_timesteps      | 149504     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01085708 |\n",
            "|    clip_fraction        | 0.153      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 7.15e+03   |\n",
            "|    n_updates            | 720        |\n",
            "|    policy_gradient_loss | -0.00481   |\n",
            "|    value_loss           | 1.19e+04   |\n",
            "----------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-150000-to-step-150500.mp4\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 267         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 565         |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055341855 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.58e+03    |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | 0.00286     |\n",
            "|    value_loss           | 8.25e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 572         |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030795302 |\n",
            "|    clip_fraction        | 0.361       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.39e+03    |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | 0.0376      |\n",
            "|    value_loss           | 1.19e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 268          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 579          |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038623894 |\n",
            "|    clip_fraction        | 0.0904       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.27e+04     |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 1.67e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 586         |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010554692 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.5e+03     |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.000512    |\n",
            "|    value_loss           | 1.26e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 593         |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041414887 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.75e+03    |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | 0.00344     |\n",
            "|    value_loss           | 1.09e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 601         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006265044 |\n",
            "|    clip_fraction        | 0.0475      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.98       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.97e+03    |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.000671   |\n",
            "|    value_loss           | 1.12e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 269          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 608          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063691437 |\n",
            "|    clip_fraction        | 0.0932       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.858       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.86e+03     |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.00184     |\n",
            "|    value_loss           | 1.7e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 615         |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010644492 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.739      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.12e+03    |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00342    |\n",
            "|    value_loss           | 1.01e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 266         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 630         |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.045804016 |\n",
            "|    clip_fraction        | 0.329       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.985      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.31e+03    |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | 0.0199      |\n",
            "|    value_loss           | 8.63e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 266         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 637         |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012570387 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.854      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.8e+03     |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.00209     |\n",
            "|    value_loss           | 1.04e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 266        |\n",
            "|    iterations           | 84         |\n",
            "|    time_elapsed         | 644        |\n",
            "|    total_timesteps      | 172032     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06436907 |\n",
            "|    clip_fraction        | 0.109      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.88      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.78e+03   |\n",
            "|    n_updates            | 830        |\n",
            "|    policy_gradient_loss | 0.00315    |\n",
            "|    value_loss           | 1.37e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 267          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 651          |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013101622 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.827       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.48e+03     |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00049     |\n",
            "|    value_loss           | 1.49e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 267           |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 659           |\n",
            "|    total_timesteps      | 176128        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00032515425 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.824        |\n",
            "|    explained_variance   | 0.142         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.2e+03       |\n",
            "|    n_updates            | 850           |\n",
            "|    policy_gradient_loss | -0.000301     |\n",
            "|    value_loss           | 7.35e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 267          |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 666          |\n",
            "|    total_timesteps      | 178176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017873086 |\n",
            "|    clip_fraction        | 4.88e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.881       |\n",
            "|    explained_variance   | 0.17         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.7e+03      |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -0.00172     |\n",
            "|    value_loss           | 7.16e+03     |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8a63db19d1fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training dual lunar landers...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;31m# Evaluate with rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-8a63db19d1fa>\u001b[0m in \u001b[0;36mtrain_agents\u001b[0;34m(total_timesteps, render_every, log_dir)\u001b[0m\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dual_lander_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBernoulliDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Here mean_actions are the logits (before rounding to get the binary actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfMultiCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> SelfMultiCategoricalDistribution:\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfMultiCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> SelfMultiCategoricalDistribution:\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            if (contact.fixtureA.body == lander['body'] or\n",
        "                contact.fixtureB.body == lander['body']):\n",
        "                other_body = contact.fixtureB.body if contact.fixtureA.body == lander['body'] else contact.fixtureA.body\n",
        "                if other_body == self.env.ground and lander['body'].linearVelocity.length > 2.0:\n",
        "                    self.env.game_over = True\n",
        "                    lander['crashed'] = True\n",
        "                    return\n",
        "\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = True\n",
        "                        contact_pos = None\n",
        "                        if contact.fixtureA.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "                        elif contact.fixtureB.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "\n",
        "                        if contact_pos is not None:\n",
        "                            lander['on_safe_zone'] = self.env.check_on_safe_zone(contact_pos, i)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = False\n",
        "                        if not lander['legs_contact'][0] or not lander['legs_contact'][1]:\n",
        "                            lander['on_safe_zone'] = False\n",
        "\n",
        "class DualLunarLander(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"dual_lunar_lander_v0\",\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.possible_agents = [\"lander_1\", \"lander_2\"]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.possible_agents)}\n",
        "\n",
        "        # Physics parameters\n",
        "        self.FPS = 50\n",
        "        self.SCALE = 30.0\n",
        "        self.LEG_DOWN = 18\n",
        "        self.LEG_W, self.LEG_H = 2, 8\n",
        "        self.ENGINE_POWER = 10.0\n",
        "        self.SIDE_ENGINE_POWER = 0.4\n",
        "        self.INITIAL_RANDOM = 500.0\n",
        "        self.MAIN_ENGINE_THRUST_SCALE = 15  # Scale for visualizing main engine thrust\n",
        "\n",
        "        # Rendering parameters\n",
        "        self.VIEWPORT_W = 600\n",
        "        self.VIEWPORT_H = 400\n",
        "        self.TERRAIN_CHUNKS = 25  # Increased chunks for more detail\n",
        "        self.TERRAIN_HEIGHT = self.VIEWPORT_H / self.SCALE / 4\n",
        "        self.TERRAIN_STEP = 1.0 / self.TERRAIN_CHUNKS\n",
        "        self.SAFE_ZONE_WIDTH = 2.0\n",
        "        self.safe_zones = []\n",
        "        self.SAFE_LANDING_VEL = 0.8\n",
        "\n",
        "        # Game state\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        # Spaces\n",
        "        self.action_spaces = {agent: spaces.Discrete(4) for agent in self.possible_agents}\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=np.array([-1, -1, -5, -5, -np.pi, -5, 0, 0, -1, -1, 0], dtype=np.float32),\n",
        "                high=np.array([1, 1, 5, 5, np.pi, 5, 1, 1, 1, 1, 1], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            ) for agent in self.possible_agents\n",
        "        }\n",
        "\n",
        "        # Physics engine\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.terrain = None\n",
        "        self.moon = None\n",
        "        self.ground = None\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.sync_reward_scale = 10.0\n",
        "        self.both_landed = False\n",
        "\n",
        "        # Colors\n",
        "        self.sky_color = (0, 0, 0)\n",
        "        self.moon_color = (102, 102, 102)\n",
        "        self.terrain_color = (153, 153, 153)\n",
        "        self.safe_zone_color = (0, 204, 0)\n",
        "        self.lander_color = [(204, 0, 0), (0, 0, 204)]\n",
        "        self.leg_color = (102, 102, 102)\n",
        "        self.engine_color = (255, 255, 0)\n",
        "\n",
        "        # Particle effects for thrusters\n",
        "        self.particles = []\n",
        "        self.particle_lifetime = 20  # frames\n",
        "\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def check_on_safe_zone(self, x_pos, lander_idx):\n",
        "        for i, (left, right) in enumerate(self.safe_zones):\n",
        "            if i == lander_idx and left <= x_pos <= right:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self._destroy()\n",
        "        self.world = Box2D.b2World(gravity=(0, -10))\n",
        "        self.world.contactListener = ContactDetector(self)\n",
        "        self.moon = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.game_over = False\n",
        "        self.both_landed = False\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.safe_zones = []\n",
        "\n",
        "        self._create_terrain()\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            safe_zone_center = (self.safe_zones[i][0] + self.safe_zones[i][1]) / 2\n",
        "            self._create_lander(agent, safe_zone_center)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        observations = {}\n",
        "        for agent in self.agents:\n",
        "            observations[agent] = self._get_observation(agent)\n",
        "\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        if self.game_over:\n",
        "            observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "            rewards = {agent: 0 for agent in self.agents}\n",
        "            terminations = {agent: True for agent in self.agents}\n",
        "            truncations = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        # Update particles\n",
        "        self.particles = [p for p in self.particles if p['lifetime'] > 0]\n",
        "        for p in self.particles:\n",
        "            p['lifetime'] -= 1\n",
        "            p['pos'][0] += p['vel'][0]\n",
        "            p['pos'][1] += p['vel'][1]\n",
        "            p['vel'][1] -= 0.05  # Add gravity to particles\n",
        "\n",
        "        for agent, action in actions.items():\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "\n",
        "            # Force the angle to stay at zero (prevent rotation)\n",
        "            lander['body'].angle = 0.0\n",
        "            lander['body'].angularVelocity = 0.0\n",
        "\n",
        "            # Reduce power for softer landing\n",
        "            reduced_engine_power = self.ENGINE_POWER * 0.7\n",
        "            reduced_side_power = self.SIDE_ENGINE_POWER * 0.6\n",
        "\n",
        "            if action == 1:  # Main engine - now gentler\n",
        "                oy = -reduced_engine_power  # Only vertical thrust, no angular component\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((0, oy), impulse_pos, True)\n",
        "                lander['main_engine'] = True\n",
        "\n",
        "                # Add particles for main engine\n",
        "                self._add_thrust_particles(lander, 'main')\n",
        "            else:\n",
        "                lander['main_engine'] = False\n",
        "\n",
        "            if action == 2:  # Left engine - gentler horizontal control\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((-reduced_side_power, 0), impulse_pos, True)\n",
        "                lander['side_engine_left'] = True\n",
        "\n",
        "                # Add particles for left engine\n",
        "                self._add_thrust_particles(lander, 'left')\n",
        "            else:\n",
        "                lander['side_engine_left'] = False\n",
        "\n",
        "            if action == 3:  # Right engine - gentler horizontal control\n",
        "                impulse_pos = (lander['body'].position[0], lander['body'].position[1])\n",
        "                lander['body'].ApplyLinearImpulse((reduced_side_power, 0), impulse_pos, True)\n",
        "                lander['side_engine_right'] = True\n",
        "\n",
        "                # Add particles for right engine\n",
        "                self._add_thrust_particles(lander, 'right')\n",
        "            else:\n",
        "                lander['side_engine_right'] = False\n",
        "\n",
        "        self.world.Step(1.0 / self.FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        terminations = {agent: False for agent in self.agents}\n",
        "        truncations = {agent: False for agent in self.agents}\n",
        "        infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        # Modify the reward function to encourage soft, accurate landings\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "            vel_len = vel.length\n",
        "\n",
        "            reward = 0\n",
        "            safe_zone_center = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "            distance_to_safe_zone = abs(pos[0] - safe_zone_center)\n",
        "\n",
        "            # Adjust shaping rewards to prioritize positioning and soft landing\n",
        "            shaping = (\n",
        "                -150 * distance_to_safe_zone  # Higher penalty for distance to landing pad\n",
        "                -100 * abs(vel[0])            # Penalize horizontal velocity\n",
        "                -120 * abs(vel[1])            # Higher penalty for vertical velocity for softer landing\n",
        "                -50 * abs(lander['body'].angle)  # Less important since we're forcing angle=0\n",
        "                -20 * abs(lander['body'].angularVelocity)  # Less important since we're forcing angularVelocity=0\n",
        "            )\n",
        "\n",
        "            reward += (shaping - self.prev_shaping[idx]) * 0.1\n",
        "            self.prev_shaping[idx] = shaping\n",
        "\n",
        "            # Reduce the fuel penalty to encourage more thruster use for precision\n",
        "            reward -= 0.05 if lander['main_engine'] else 0\n",
        "            reward -= 0.02 if lander['side_engine_left'] or lander['side_engine_right'] else 0\n",
        "\n",
        "            # Define a very slow landing velocity for soft landing\n",
        "            VERY_SAFE_LANDING_VEL = 0.3  # Reduced from default\n",
        "\n",
        "            landed = False\n",
        "            if (lander['legs_contact'][0] and lander['legs_contact'][1]):\n",
        "                landed = True\n",
        "\n",
        "                # Bigger rewards for softer landings\n",
        "                if lander['on_safe_zone']:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        base_reward = 200\n",
        "\n",
        "                        # Extra rewards for extremely soft landings\n",
        "                        if vel_len < VERY_SAFE_LANDING_VEL:\n",
        "                            speed_bonus = 100 * (1 - vel_len/VERY_SAFE_LANDING_VEL)\n",
        "                            base_reward += speed_bonus\n",
        "\n",
        "                        # Extra rewards for landing in the center of the pad\n",
        "                        center_dist = abs(pos[0] - safe_zone_center)\n",
        "                        center_bonus = 50 * (1 - min(1.0, center_dist / (self.safe_zones[idx][1] - safe_zone_center)))\n",
        "                        base_reward += center_bonus\n",
        "\n",
        "                        self.landing_rewards[agent] = base_reward\n",
        "                        reward += base_reward\n",
        "                else:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[agent] = 50\n",
        "                        reward += 50\n",
        "\n",
        "            # Updated collision handling with more severe penalties\n",
        "            if lander.get('crashed', False) or pos[0] < 0 or pos[0] > self.VIEWPORT_W / self.SCALE or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE):\n",
        "                reward -= 200  # Increased penalty for crashes\n",
        "                terminations[agent] = True\n",
        "                if not 0 <= pos[1] < self.VIEWPORT_H / self.SCALE:\n",
        "                    lander['outside_bounds'] = True\n",
        "\n",
        "            rewards[agent] = reward\n",
        "\n",
        "        # Bonus for synchronized landings\n",
        "        all_landed_safely = all(self.landing_rewards[agent] >= 100 for agent in self.agents)\n",
        "        if all_landed_safely and not self.both_landed:\n",
        "            sync_reward = self.sync_reward_scale * 300  # Increased reward for synchronized landing\n",
        "            for agent in self.agents:\n",
        "                rewards[agent] += sync_reward\n",
        "            self.both_landed = True\n",
        "\n",
        "        # Update termination conditions\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander['body'].position\n",
        "            vel = lander['body'].linearVelocity\n",
        "\n",
        "            if lander.get('crashed', False) or lander.get('outside_bounds', False):\n",
        "                terminations[agent] = True\n",
        "            elif (lander['legs_contact'][0] and lander['legs_contact'][1]) and vel.length < self.SAFE_LANDING_VEL:\n",
        "                if lander['on_safe_zone'] and all_landed_safely:\n",
        "                    terminations[agent] = True\n",
        "\n",
        "        self.game_over = all(terminations.values())\n",
        "\n",
        "        observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    # Update the particle effect for the thrusters to be more visible but gentler\n",
        "    def _add_thrust_particles(self, lander, engine_type):\n",
        "        \"\"\"Add particle effects for thruster visualization\"\"\"\n",
        "        num_particles = np.random.randint(4, 8)  # More particles for visibility\n",
        "\n",
        "        if engine_type == 'main':\n",
        "            # Main engine particles (straight down for stable lander)\n",
        "            base_pos = (lander['body'].position[0], lander['body'].position[1] - 0.1)\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                spread = np.random.uniform(-0.1, 0.1)  # Reduced spread for more focused thrust\n",
        "                vel_x = spread * np.random.uniform(0.1, 0.2)  # Less horizontal drift\n",
        "                vel_y = -np.random.uniform(0.3, 0.8)  # Gentler vertical velocity\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 255, 0),\n",
        "                    'lifetime': np.random.randint(15, self.particle_lifetime),  # Longer lifetime\n",
        "                    'size': np.random.uniform(1.5, 3.5)  # Larger particles\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'left':\n",
        "            # Left side engine particles\n",
        "            base_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = -np.random.uniform(0.2, 0.5)  # Gentler horizontal thrust\n",
        "                vel_y = np.random.uniform(-0.05, 0.05)  # Minimal vertical component\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(8, 18),\n",
        "                    'size': np.random.uniform(1.2, 2.5)\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'right':\n",
        "            # Right side engine particles\n",
        "            base_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = np.random.uniform(0.2, 0.5)  # Gentler horizontal thrust\n",
        "                vel_y = np.random.uniform(-0.05, 0.05)  # Minimal vertical component\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(8, 18),\n",
        "                    'size': np.random.uniform(1.2, 2.5)\n",
        "                })\n",
        "\n",
        "    def _destroy(self):\n",
        "        if self.world is None:\n",
        "            return\n",
        "\n",
        "        for lander in self.landers:\n",
        "            self.world.DestroyBody(lander['body'])\n",
        "            for leg in lander['legs']:\n",
        "                self.world.DestroyBody(leg)\n",
        "\n",
        "        if self.moon:\n",
        "            self.world.DestroyBody(self.moon)\n",
        "\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "\n",
        "    def _create_terrain(self):\n",
        "        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]))\n",
        "        self.moon.CreateFixture(\n",
        "            shape=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]),\n",
        "            friction=0.1,\n",
        "            restitution=0.0,\n",
        "        )\n",
        "        self.ground = self.moon\n",
        "\n",
        "        self.terrain = []\n",
        "        height = self.TERRAIN_HEIGHT\n",
        "        chunk_x = [0]\n",
        "        for i in range(self.TERRAIN_CHUNKS):\n",
        "            chunk_x.append(chunk_x[-1] + self.TERRAIN_STEP)\n",
        "\n",
        "        # Generate smoother terrain with Perlin-like noise\n",
        "        height_data = []\n",
        "\n",
        "        # Initial random heights\n",
        "        base_heights = []\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            h = np.random.uniform(0, height)\n",
        "            base_heights.append(h)\n",
        "\n",
        "        # Apply smoothing to get more natural, curved terrain\n",
        "        # Use a simple moving average for smoothing\n",
        "        window_size = 3\n",
        "        smoothed_heights = []\n",
        "\n",
        "        # Pad the array for smoothing\n",
        "        padded = [base_heights[0]] * (window_size//2) + base_heights + [base_heights[-1]] * (window_size//2)\n",
        "\n",
        "        for i in range(len(base_heights)):\n",
        "            window = padded[i:i+window_size]\n",
        "            smoothed_heights.append(sum(window) / len(window))\n",
        "\n",
        "        height_data = smoothed_heights\n",
        "\n",
        "        landing_pad_1_idx = self.TERRAIN_CHUNKS // 4\n",
        "        landing_pad_2_idx = 3 * self.TERRAIN_CHUNKS // 4\n",
        "\n",
        "        pad_height = self.TERRAIN_HEIGHT/4\n",
        "        pad_width = 3\n",
        "\n",
        "        # Flatten landing pads\n",
        "        for i in range(landing_pad_1_idx - pad_width//2, landing_pad_1_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        for i in range(landing_pad_2_idx - pad_width//2, landing_pad_2_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        pad1_left = (landing_pad_1_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad1_right = (landing_pad_1_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        pad2_left = (landing_pad_2_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad2_right = (landing_pad_2_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        self.safe_zones = [(pad1_left, pad1_right), (pad2_left, pad2_right)]\n",
        "\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            x1 = chunk_x[i]\n",
        "            x2 = chunk_x[i+1]\n",
        "            h1 = height_data[i]\n",
        "\n",
        "            poly = [\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, 0),\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, 0)\n",
        "            ]\n",
        "\n",
        "            self.terrain.append((poly,\n",
        "                               (landing_pad_1_idx - pad_width//2 <= i <= landing_pad_1_idx + pad_width//2) or\n",
        "                               (landing_pad_2_idx - pad_width//2 <= i <= landing_pad_2_idx + pad_width//2)))\n",
        "\n",
        "            self.moon.CreateFixture(\n",
        "                shape=polygonShape(vertices=poly),\n",
        "                friction=0.4,\n",
        "                restitution=0.0\n",
        "            )\n",
        "\n",
        "    def _create_lander(self, agent, initial_x):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "\n",
        "        init_x = initial_x + np.random.uniform(-0.3, 0.3)\n",
        "        init_y = self.VIEWPORT_H / self.SCALE * 0.8\n",
        "\n",
        "        # Create a triangle-shaped lander\n",
        "        # Create a triangle-shaped lander with the point facing upward\n",
        "        lander_body = self.world.CreateDynamicBody(\n",
        "            position=(init_x, init_y),\n",
        "            angle=0.0,  # Start with zero angle for stability\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(vertices=[\n",
        "                    (-0.4, 0.0),   # Left base point\n",
        "                    (0.4, 0.0),    # Right base point\n",
        "                    (0.0, 0.8)     # Top point (triangle peak pointing up)\n",
        "                    ]),\n",
        "            density=5.0,\n",
        "            friction=0.1,\n",
        "            categoryBits=0x0010,\n",
        "            maskBits=0x001,\n",
        "            restitution=0.0\n",
        "                ),\n",
        "      )\n",
        "\n",
        "        # Reduce initial velocity for more stability\n",
        "        lander_body.linearVelocity.Set(\n",
        "            np.random.uniform(-0.5, 0.5),  # Reduced horizontal velocity\n",
        "            np.random.uniform(-0.5, 0.0)   # Reduced vertical velocity\n",
        "        )\n",
        "\n",
        "        # Set angular velocity to zero to prevent rotation\n",
        "        lander_body.angularVelocity = 0.0\n",
        "\n",
        "        # Add angular damping to resist rotation\n",
        "        lander_body.angularDamping = 10.0\n",
        "\n",
        "        # Add linear damping for smoother movement\n",
        "        lander_body.linearDamping = 0.2\n",
        "\n",
        "        legs = []\n",
        "        legs_contact = [False, False]\n",
        "\n",
        "        for i in [-1, 1]:\n",
        "            # Create fixed-angle legs that always point downward\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(init_x + i * 0.3, init_y - 0.1),\n",
        "                angle=0.0,  # Keep legs straight down\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(vertices=[\n",
        "                        (0, 0),\n",
        "                        (0, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, 0)\n",
        "                    ]),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    friction=0.2,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            legs.append(leg)\n",
        "\n",
        "            # Use a prismatic joint instead of revolute joint to keep legs aligned vertically\n",
        "            pjd = Box2D.b2PrismaticJointDef(\n",
        "                bodyA=lander_body,\n",
        "                bodyB=leg,\n",
        "                localAxisA=(0, 1),\n",
        "                localAnchorA=(i * 0.25, 0.0),\n",
        "                localAnchorB=(0, 0),\n",
        "                referenceAngle=0.0,\n",
        "                enableLimit=True,\n",
        "                lowerTranslation=-0.1,\n",
        "                upperTranslation=0.1\n",
        "            )\n",
        "            self.world.CreateJoint(pjd)\n",
        "\n",
        "        self.landers.append({\n",
        "            'body': lander_body,\n",
        "            'legs': legs,\n",
        "            'legs_contact': legs_contact,\n",
        "            'main_engine': False,\n",
        "            'side_engine_left': False,\n",
        "            'side_engine_right': False,\n",
        "            'on_safe_zone': False,\n",
        "            'crashed': False,\n",
        "            'outside_bounds': False\n",
        "        })\n",
        "\n",
        "    def _get_observation(self, agent):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "        lander = self.landers[idx]\n",
        "        pos = lander['body'].position\n",
        "        vel = lander['body'].linearVelocity\n",
        "\n",
        "        safe_zone_center_x = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "        safe_zone_width = self.safe_zones[idx][1] - self.safe_zones[idx][0]\n",
        "\n",
        "        dist_to_safe_zone = (pos[0] - safe_zone_center_x) / (self.VIEWPORT_W / self.SCALE)\n",
        "\n",
        "        observation = np.array([\n",
        "            (pos[0] - self.VIEWPORT_W / self.SCALE / 2) / (self.VIEWPORT_W / self.SCALE / 2),\n",
        "            (pos[1] - self.VIEWPORT_H / self.SCALE / 2) / (self.VIEWPORT_H / self.SCALE / 2),\n",
        "            vel[0] / 5.0,\n",
        "            vel[1] / 5.0,\n",
        "            lander['body'].angle,\n",
        "            lander['body'].angularVelocity / 5.0,\n",
        "            1.0 if lander['legs_contact'][0] else 0.0,\n",
        "            1.0 if lander['legs_contact'][1] else 0.0,\n",
        "            dist_to_safe_zone,\n",
        "            safe_zone_width / (self.VIEWPORT_W / self.SCALE),\n",
        "            1.0 if lander['on_safe_zone'] else 0.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _init_renderer(self):\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            pygame.display.set_caption(\"Dual Lunar Lander\")\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return None\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        surf = self.screen if self.render_mode == \"human\" else pygame.Surface((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "        surf.fill(self.sky_color)\n",
        "\n",
        "        def transform(pos):\n",
        "            return (int(pos[0] * self.SCALE), self.VIEWPORT_H - int(pos[1] * self.SCALE))\n",
        "\n",
        "        # Draw terrain\n",
        "        for poly, is_safe_zone in self.terrain:\n",
        "            transformed_poly = [transform(p) for p in poly]\n",
        "            color = self.safe_zone_color if is_safe_zone else self.terrain_color\n",
        "            pygame.draw.polygon(surf, color, transformed_poly)\n",
        "\n",
        "        # Draw landing pads\n",
        "        for left, right in self.safe_zones:\n",
        "            h = self.TERRAIN_HEIGHT/4\n",
        "            pad_poly = [\n",
        "                transform((left, h)),\n",
        "                transform((right, h)),\n",
        "                transform((right, h-0.05)),\n",
        "                transform((left, h-0.05))\n",
        "            ]\n",
        "            pygame.draw.polygon(surf, (255, 255, 255), pad_poly)\n",
        "\n",
        "        # Draw particles - render before landers for proper layering\n",
        "        for particle in self.particles:\n",
        "            pos = transform(particle['pos'])\n",
        "            color = particle['color']\n",
        "            # Fade out particles as they age\n",
        "            alpha = int(255 * (particle['lifetime'] / self.particle_lifetime))\n",
        "            color = (color[0], color[1], min(255, color[2] + int(alpha/2)))\n",
        "            size = max(1, int(particle['size']))\n",
        "            pygame.draw.circle(surf, color, pos, size)\n",
        "\n",
        "        # Draw landers\n",
        "        for i, lander in enumerate(self.landers):\n",
        "            lander_color = self.lander_color[i]\n",
        "\n",
        "            # Draw body\n",
        "            vertices = []\n",
        "            for fixture in lander['body'].fixtures:\n",
        "                for vertex in fixture.shape.vertices:\n",
        "                    vertices.append(transform(lander['body'].transform * vertex))\n",
        "            pygame.draw.polygon(surf, lander_color, vertices)\n",
        "\n",
        "            # Draw legs\n",
        "            for leg in lander['legs']:\n",
        "                vertices = []\n",
        "                for fixture in leg.fixtures:\n",
        "                    for vertex in fixture.shape.vertices:\n",
        "                        vertices.append(transform(leg.transform * vertex))\n",
        "                pygame.draw.polygon(surf, self.leg_color, vertices)\n",
        "\n",
        "            # Draw engine effects\n",
        "            if lander['main_engine']:\n",
        "                start_pos = transform(lander['body'].position + (0, -0.3))\n",
        "                flame_length = self.MAIN_ENGINE_THRUST_SCALE + np.random.randint(0, 5)\n",
        "                end_pos = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length\n",
        "                )\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 3)\n",
        "\n",
        "                # Draw inner flame (white)\n",
        "                inner_end = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length * 0.6,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length * 0.6\n",
        "                )\n",
        "                pygame.draw.line(surf, (255, 255, 255), start_pos, inner_end, 2)\n",
        "\n",
        "            if lander['side_engine_left']:\n",
        "                start_pos = transform(lander['body'].position + (-0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] - flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "            if lander['side_engine_right']:\n",
        "                start_pos = transform(lander['body'].position + (0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] + flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "            return None\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "            self.isopen = False\n",
        "\n",
        "class SB3Wrapper(gym.Env):\n",
        "    \"\"\"Custom Gym wrapper for Stable Baselines3 compatibility\"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_space = self.env.observation_space(self.agents[0])\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([obs_space.low]*2),\n",
        "            high=np.concatenate([obs_space.high]*2),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, info = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), info\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to dict\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "class StableBaselinesWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    Converts ParallelEnv to Gym Env for Stable Baselines3 compatibility\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_spaces = [self.env.observation_space(agent) for agent in self.agents]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([space.low for space in obs_spaces]),\n",
        "            high=np.concatenate([space.high for space in obs_spaces]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Important: Properly set metadata and render_mode\n",
        "        self.metadata = self.env.metadata\n",
        "        self.render_mode = env.render_mode\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, _ = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to action dictionary\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        # Convert to Gym step format\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms, 'truncations': truncs}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "def train_agents(total_timesteps=100000, render_every=20000, log_dir=\"./logs\"):\n",
        "    # Create and wrap environment\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    # Ensure render_mode is properly set\n",
        "    if not hasattr(wrapped_env, 'render_mode') or wrapped_env.render_mode != \"rgb_array\":\n",
        "        wrapped_env.render_mode = \"rgb_array\"\n",
        "\n",
        "    vec_env = DummyVecEnv([lambda: wrapped_env])\n",
        "\n",
        "    # Setup video recording\n",
        "    video_folder = os.path.join(log_dir, \"videos\")\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    vec_env = VecVideoRecorder(\n",
        "        vec_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda x: x % render_every == 0,\n",
        "        video_length=1000,  # Increased video length to capture slower landings\n",
        "        name_prefix=\"dual_lander\"\n",
        "    )\n",
        "\n",
        "    # Create and train model with slower learning rate and more exploration\n",
        "    model = PPO(\n",
        "        MlpPolicy,\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        tensorboard_log=log_dir,\n",
        "        learning_rate=1e-4,  # Reduced learning rate to slow down learning\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.995,  # Increased gamma for more long-term focused learning\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,  # Add entropy coefficient to encourage exploration\n",
        "        policy_kwargs={\"net_arch\": [dict(pi=[256, 256], vf=[256, 256])]}\n",
        "    )\n",
        "\n",
        "    # Force training to run the full number of timesteps\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save(os.path.join(log_dir, \"dual_lander_model\"))\n",
        "    return model\n",
        "\n",
        "# Modify the evaluate_model function to show slower approaches\n",
        "def evaluate_model(model, num_episodes=5, render=True):\n",
        "    # Create environment with rendering\n",
        "    env = DualLunarLander(render_mode=\"human\" if render else None)\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = wrapped_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "        max_steps = 2000  # Set a high max steps to allow for longer episodes\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = wrapped_env.step(action)\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if render:\n",
        "                env.render()  # Render original environment\n",
        "\n",
        "        print(f\"Episode {episode+1} Total Reward: {total_reward}, Steps: {step_count}\")\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training dual lunar landers...\")\n",
        "    model = train_agents()\n",
        "\n",
        "    # Evaluate with rendering\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    evaluate_model(model, num_episodes=5, render=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7I-Q6_J9nPWe",
        "outputId": "a77a3f8e-abef-4279-a4d1-57e0a258fde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dual lunar landers...\n",
            "Using cpu device\n",
            "Logging to ./logs/PPO_8\n",
            "Saving video to /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 106  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 19   |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 150           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00095344766 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 8.48e-05      |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 1.18e+06      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00305      |\n",
            "|    value_loss           | 2.56e+06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 170           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 36            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8378982e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -0.00107      |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.49e+06      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000147     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 182           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 44            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7689675e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.00156       |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.62e+06      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -4.87e-05     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 193           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.0562237e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.000347      |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.42e+06      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -8.94e-05     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 201           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 60            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3727375e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -6.08e-06     |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.62e+06      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000155     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 210           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 68            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6752393e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -2.01e-05     |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.59e+06      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -4.97e-05     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 215           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 75            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6614096e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -3.83e-05     |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.49e+06      |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.0002       |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 217           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 84            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.1332324e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -5.84e-06     |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.46e+06      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000108     |\n",
            "|    value_loss           | 1.31e+07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 213           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 95            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7794722e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0342        |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 6.71e+06      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000495     |\n",
            "|    value_loss           | 1.23e+07      |\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training dual lunar landers...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;31m# Evaluate with rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36mtrain_agents\u001b[0;34m(total_timesteps, render_every, log_dir)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;31m# Force training to run the full number of timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dual_lander_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saving video to {self.video_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36m_capture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecording\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot capture a frame, recording wasn't started.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVecEnvObs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# call the render method of the environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Create a big image by tiling images from subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mbigimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtile_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mget_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m             )\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m             )\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Draw terrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_safe_zone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mtransformed_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_zone_color\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_safe_zone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterrain_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Draw terrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_safe_zone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mtransformed_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_zone_color\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_safe_zone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterrain_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-56a00697037c>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(pos)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVIEWPORT_H\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Draw terrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "import os\n",
        "from pettingzoo import ParallelEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            if (contact.fixtureA.body == lander['body'] or\n",
        "                contact.fixtureB.body == lander['body']):\n",
        "                other_body = contact.fixtureB.body if contact.fixtureA.body == lander['body'] else contact.fixtureA.body\n",
        "                if other_body == self.env.ground and lander['body'].linearVelocity.length > 2.0:\n",
        "                    self.env.game_over = True\n",
        "                    lander['crashed'] = True\n",
        "                    return\n",
        "\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = True\n",
        "                        contact_pos = None\n",
        "                        if contact.fixtureA.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "                        elif contact.fixtureB.body == self.env.ground:\n",
        "                            contact_pos = contact.worldManifold.points[0][0]\n",
        "\n",
        "                        if contact_pos is not None:\n",
        "                            lander['on_safe_zone'] = self.env.check_on_safe_zone(contact_pos, i)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i, lander in enumerate(self.env.landers):\n",
        "            for idx, leg in enumerate(lander['legs']):\n",
        "                if leg == contact.fixtureA.body or leg == contact.fixtureB.body:\n",
        "                    if self.env.ground in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                        lander['legs_contact'][idx] = False\n",
        "                        if not lander['legs_contact'][0] or not lander['legs_contact'][1]:\n",
        "                            lander['on_safe_zone'] = False\n",
        "\n",
        "class DualLunarLander(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"dual_lunar_lander_v0\",\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.possible_agents = [\"lander_1\", \"lander_2\"]\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.possible_agents)}\n",
        "\n",
        "        # Physics parameters\n",
        "        self.FPS = 50\n",
        "        self.SCALE = 30.0\n",
        "        self.LEG_DOWN = 18\n",
        "        self.LEG_W, self.LEG_H = 2, 8\n",
        "        self.ENGINE_POWER = 8.0  # Reduced engine power to slow down control\n",
        "        self.SIDE_ENGINE_POWER = 0.3  # Reduced side engine power for subtler control\n",
        "        self.INITIAL_RANDOM = 1000.0  # Increased initial randomness\n",
        "        self.MAIN_ENGINE_THRUST_SCALE = 15\n",
        "\n",
        "        # Rendering parameters\n",
        "        self.VIEWPORT_W = 600\n",
        "        self.VIEWPORT_H = 400\n",
        "        self.TERRAIN_CHUNKS = 25\n",
        "        self.TERRAIN_HEIGHT = self.VIEWPORT_H / self.SCALE / 4\n",
        "        self.TERRAIN_STEP = 1.0 / self.TERRAIN_CHUNKS\n",
        "        self.SAFE_ZONE_WIDTH = 1.5  # Narrower landing zones\n",
        "        self.safe_zones = []\n",
        "        self.SAFE_LANDING_VEL = 0.6  # More strict landing velocity\n",
        "\n",
        "\n",
        "        # Game state\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.step_counter = 0\n",
        "        self.max_steps = 1500  # Add step limit\n",
        "\n",
        "        # Extra parameters to make learning more gradual\n",
        "        self.wind_force = 0.05  # Add wind effect\n",
        "        self.wind_direction = 1  # 1 or -1\n",
        "        self.wind_change_prob = 0.005  # Probability of wind changing\n",
        "\n",
        "        # Spaces\n",
        "        self.action_spaces = {agent: spaces.Discrete(4) for agent in self.possible_agents}\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=np.array([-1, -1, -5, -5, -np.pi, -5, 0, 0, -1, -1, 0, -1], dtype=np.float32),  # Added wind observation\n",
        "                high=np.array([1, 1, 5, 5, np.pi, 5, 1, 1, 1, 1, 1, 1], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            ) for agent in self.possible_agents\n",
        "        }\n",
        "\n",
        "        # Physics engine\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.terrain = None\n",
        "        self.moon = None\n",
        "        self.ground = None\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.sync_reward_scale = 5.0  # Reduced sync reward to make coordination harder\n",
        "        self.both_landed = False\n",
        "\n",
        "        # Colors\n",
        "        self.sky_color = (0, 0, 0)\n",
        "        self.moon_color = (102, 102, 102)\n",
        "        self.terrain_color = (153, 153, 153)\n",
        "        self.safe_zone_color = (0, 204, 0)\n",
        "        self.lander_color = [(200, 200, 200), (200, 200, 200)]\n",
        "        self.leg_color = (102, 102, 102)\n",
        "        self.engine_color = (255, 255, 0)\n",
        "\n",
        "        # Particle effects for thrusters\n",
        "        self.particles = []\n",
        "        self.particle_lifetime = 20  # frames\n",
        "\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def check_on_safe_zone(self, x_pos, lander_idx):\n",
        "        for i, (left, right) in enumerate(self.safe_zones):\n",
        "            if i == lander_idx and left <= x_pos <= right:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.agents = self.possible_agents.copy()\n",
        "        self._destroy()\n",
        "        self.world = Box2D.b2World(gravity=(0, -10))\n",
        "        self.world.contactListener = ContactDetector(self)\n",
        "        self.moon = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "        self.prev_shaping = [0, 0]\n",
        "        self.game_over = False\n",
        "        self.both_landed = False\n",
        "        self.landing_rewards = {agent: 0 for agent in self.possible_agents}\n",
        "        self.safe_zones = []\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # Reset wind parameters\n",
        "        self.wind_force = 0.05\n",
        "        self.wind_direction = np.random.choice([-1, 1])\n",
        "\n",
        "        self._create_terrain()\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            safe_zone_center = (self.safe_zones[i][0] + self.safe_zones[i][1]) / 2\n",
        "            offset = np.random.uniform(-5.0, 5.0)  # Random horizontal offset from landing zone\n",
        "            self._create_lander(agent, safe_zone_center + offset)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        observations = {}\n",
        "        for agent in self.agents:\n",
        "            observations[agent] = self._get_observation(agent)\n",
        "\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        if self.game_over:\n",
        "            observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "            rewards = {agent: 0 for agent in self.agents}\n",
        "            terminations = {agent: True for agent in self.agents}\n",
        "            truncations = {agent: False for agent in self.agents}\n",
        "            infos = {agent: {} for agent in self.agents}\n",
        "            return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        if np.random.random() < self.wind_change_prob:\n",
        "            self.wind_direction *= -1\n",
        "\n",
        "        # Update particles\n",
        "        self.particles = [p for p in self.particles if p[\"lifetime\"] > 0]\n",
        "        for p in self.particles:\n",
        "            p[\"lifetime\"] -= 1\n",
        "            p[\"pos\"][0] += p[\"vel\"][0]\n",
        "            p[\"pos\"][1] += p[\"vel\"][1]\n",
        "            p[\"vel\"][1] -= 0.05  # Add gravity to particles\n",
        "\n",
        "        # Process actions for each agent\n",
        "        for agent, action in actions.items():\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "\n",
        "            # Force the angle to stay at zero (prevent rotation)\n",
        "            lander[\"body\"].angle = 0.0\n",
        "            lander[\"body\"].angularVelocity = 0.0\n",
        "\n",
        "            # Reduce power for softer landing\n",
        "            reduced_engine_power = self.ENGINE_POWER * 0.7\n",
        "            reduced_side_power = self.SIDE_ENGINE_POWER * 0.6\n",
        "\n",
        "            if action == 1:  # Main engine - now gentler\n",
        "                oy = -reduced_engine_power  # Only vertical thrust, no angular component\n",
        "                impulse_pos = (lander[\"body\"].position[0], lander[\"body\"].position[1])\n",
        "                lander[\"body\"].ApplyLinearImpulse((0, oy), impulse_pos, True)\n",
        "                lander[\"main_engine\"] = True\n",
        "\n",
        "                # Add particles for main engine\n",
        "                self._add_thrust_particles(lander, \"main\")\n",
        "            else:\n",
        "                lander[\"main_engine\"] = False\n",
        "\n",
        "            if action == 2:  # Left engine - gentler horizontal control\n",
        "                impulse_pos = (lander[\"body\"].position[0], lander[\"body\"].position[1])\n",
        "                lander[\"body\"].ApplyLinearImpulse(\n",
        "                    (-reduced_side_power, 0), impulse_pos, True\n",
        "                )\n",
        "                lander[\"side_engine_left\"] = True\n",
        "\n",
        "                # Add particles for left engine\n",
        "                self._add_thrust_particles(lander, \"left\")\n",
        "            else:\n",
        "                lander[\"side_engine_left\"] = False\n",
        "\n",
        "            if action == 3:  # Right engine - gentler horizontal control\n",
        "                impulse_pos = (lander[\"body\"].position[0], lander[\"body\"].position[1])\n",
        "                lander[\"body\"].ApplyLinearImpulse(\n",
        "                    (reduced_side_power, 0), impulse_pos, True\n",
        "                )\n",
        "                lander[\"side_engine_right\"] = True\n",
        "\n",
        "                # Add particles for right engine\n",
        "                self._add_thrust_particles(lander, \"right\")\n",
        "            else:\n",
        "                lander[\"side_engine_right\"] = False\n",
        "\n",
        "            wind = self.wind_force * self.wind_direction\n",
        "            lander[\"body\"].ApplyForceToCenter((wind, 0), True)\n",
        "\n",
        "        self.world.Step(1.0 / self.FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        terminations = {agent: False for agent in self.agents}\n",
        "        truncations = {agent: False for agent in self.agents}\n",
        "        infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        # Modify the reward function to encourage soft, accurate landings\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander[\"body\"].position\n",
        "            vel = lander[\"body\"].linearVelocity\n",
        "            vel_len = vel.length\n",
        "\n",
        "            reward = 0\n",
        "            safe_zone_center = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "            distance_to_safe_zone = abs(pos[0] - safe_zone_center)\n",
        "\n",
        "            # Make shaping rewards more subtle to encourage slower approach\n",
        "            shaping = (\n",
        "                -120 * distance_to_safe_zone  # Reduced distance penalty\n",
        "                - 80 * abs(vel[0])  # Reduced horizontal velocity penalty\n",
        "                - 100 * abs(vel[1])  # Reduced vertical velocity penalty\n",
        "                - 30 * abs(lander[\"body\"].angle)\n",
        "                - 10 * abs(lander[\"body\"].angularVelocity)\n",
        "            )\n",
        "\n",
        "            # More gradual reward shaping\n",
        "            reward += (shaping - self.prev_shaping[idx]) * 0.05  # Reduced shaping factor\n",
        "            self.prev_shaping[idx] = shaping\n",
        "\n",
        "            # Higher fuel penalty to discourage constant thrusting\n",
        "            reward -= 0.075 if lander[\"main_engine\"] else 0\n",
        "            reward -= (\n",
        "                0.04 if lander[\"side_engine_left\"] or lander[\"side_engine_right\"] else 0\n",
        "            )\n",
        "\n",
        "            # Stricter landing velocity requirement\n",
        "            VERY_SAFE_LANDING_VEL = 0.2  # Even slower landing\n",
        "\n",
        "            landed = False\n",
        "            if lander[\"legs_contact\"][0] and lander[\"legs_contact\"][1]:\n",
        "                landed = True\n",
        "\n",
        "                # More gradual landing rewards\n",
        "                if lander[\"on_safe_zone\"]:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        base_reward = 150  # Reduced base reward\n",
        "\n",
        "                        # More strict landing velocity requirements\n",
        "                        if vel_len < VERY_SAFE_LANDING_VEL:\n",
        "                            speed_bonus = 80 * (1 - vel_len / VERY_SAFE_LANDING_VEL)\n",
        "                            base_reward += speed_bonus\n",
        "\n",
        "                        # More emphasis on landing in the center\n",
        "                        center_dist = abs(pos[0] - safe_zone_center)\n",
        "                        center_bonus = 40 * (\n",
        "                            1\n",
        "                            - min(\n",
        "                                1.0,\n",
        "                                center_dist / (self.safe_zones[idx][1] - safe_zone_center),\n",
        "                            )\n",
        "                        )\n",
        "                        base_reward += center_bonus\n",
        "\n",
        "                        self.landing_rewards[agent] = base_reward\n",
        "                        reward += base_reward\n",
        "                else:\n",
        "                    if self.landing_rewards[agent] == 0:\n",
        "                        self.landing_rewards[\n",
        "                            agent\n",
        "                        ] = 30  # Reduced off-target landing reward\n",
        "                        reward += 30\n",
        "\n",
        "            # Updated collision handling with more severe penalties\n",
        "            if (\n",
        "                lander.get(\"crashed\", False)\n",
        "                or pos[0] < 0\n",
        "                or pos[0] > self.VIEWPORT_W / self.SCALE\n",
        "                or not (0 <= pos[1] < self.VIEWPORT_H / self.SCALE)\n",
        "            ):\n",
        "                reward -= 200\n",
        "                terminations[agent] = True\n",
        "                if not 0 <= pos[1] < self.VIEWPORT_H / self.SCALE:\n",
        "                    lander[\"outside_bounds\"] = True\n",
        "\n",
        "            rewards[agent] = reward\n",
        "\n",
        "        # Bonus for synchronized landings\n",
        "        all_landed_safely = all(self.landing_rewards[agent] >= 80 for agent in self.agents)\n",
        "        if all_landed_safely and not self.both_landed:\n",
        "            sync_reward = self.sync_reward_scale * 250\n",
        "            for agent in self.agents:\n",
        "                rewards[agent] += sync_reward\n",
        "            self.both_landed = True\n",
        "\n",
        "        # NEW: Check if both landers have been successfully landed for a while, if so, initiate takeoff\n",
        "        initiate_takeoff = False\n",
        "        if all_landed_safely:\n",
        "            landing_times = [\n",
        "                self.landers[self.agent_name_mapping[agent]].get(\"landing_time\", 0)\n",
        "                for agent in self.agents\n",
        "            ]\n",
        "            if all(landing_times) and self.step_counter - min(landing_times) > 100:\n",
        "                initiate_takeoff = True\n",
        "\n",
        "        # NEW: Take-off phase logic\n",
        "        if initiate_takeoff:\n",
        "            for agent in self.agents:\n",
        "                idx = self.agent_name_mapping[agent]\n",
        "                lander = self.landers[idx]\n",
        "\n",
        "                # Apply a strong upward impulse to \"launch\" the lander\n",
        "                launch_power = self.ENGINE_POWER * 2.0\n",
        "                impulse_pos = (lander[\"body\"].position[0], lander[\"body\"].position[1])\n",
        "                lander[\"body\"].ApplyLinearImpulse((0, launch_power), impulse_pos, True)\n",
        "\n",
        "                # Reset landing rewards to encourage another landing\n",
        "                self.landing_rewards[agent] = 0\n",
        "                lander[\"landing_time\"] = 0\n",
        "\n",
        "                # Add particles for launch effect\n",
        "                for _ in range(10):\n",
        "                    self._add_thrust_particles(lander, \"main\")\n",
        "\n",
        "        # Reset both_landed flag to prepare for next synchronized landing\n",
        "        self.both_landed = False\n",
        "\n",
        "        # Provide a small reward for initiating a new cycle\n",
        "        for agent in self.agents:\n",
        "            rewards[agent] += 50\n",
        "\n",
        "        # Update termination conditions\n",
        "        for agent in self.agents:\n",
        "            idx = self.agent_name_mapping[agent]\n",
        "            lander = self.landers[idx]\n",
        "            pos = lander[\"body\"].position\n",
        "            vel = lander[\"body\"].linearVelocity\n",
        "\n",
        "            # Only end episode on crashes or step limit\n",
        "            if lander.get(\"crashed\", False) or lander.get(\"outside_bounds\", False):\n",
        "                terminations[agent] = True\n",
        "            # Make it harder to finish - require both landers to land plus strict velocity requirement\n",
        "            elif (\n",
        "                lander[\"legs_contact\"][0] and lander[\"legs_contact\"][1]\n",
        "            ) and vel.length < self.SAFE_LANDING_VEL * 0.7:\n",
        "                if (\n",
        "                    lander[\"on_safe_zone\"] and all_landed_safely and self.step_counter > 500\n",
        "                ):  # Force minimum episode length\n",
        "                    terminations[agent] = True\n",
        "\n",
        "        if self.step_counter >= self.max_steps:\n",
        "            for agent in self.agents:\n",
        "                truncations[agent] = True\n",
        "\n",
        "        self.game_over = all(terminations.values()) or all(truncations.values())\n",
        "        self.step_counter += 1  # Increment the step counter\n",
        "\n",
        "        observations = {agent: self._get_observation(agent) for agent in self.agents}\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    # Update the particle effect for the thrusters to be more visible but gentler\n",
        "    def _add_thrust_particles(self, lander, engine_type):\n",
        "        \"\"\"Add particle effects for thruster visualization\"\"\"\n",
        "        num_particles = np.random.randint(4, 8)  # More particles for visibility\n",
        "\n",
        "        if engine_type == 'main':\n",
        "            # Main engine particles (straight down for stable lander)\n",
        "            base_pos = (lander['body'].position[0], lander['body'].position[1] - 0.1)\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                spread = np.random.uniform(-0.1, 0.1)  # Reduced spread for more focused thrust\n",
        "                vel_x = spread * np.random.uniform(0.1, 0.2)  # Less horizontal drift\n",
        "                vel_y = -np.random.uniform(0.3, 0.8)  # Gentler vertical velocity\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 255, 0),\n",
        "                    'lifetime': np.random.randint(15, self.particle_lifetime),  # Longer lifetime\n",
        "                    'size': np.random.uniform(1.5, 3.5)  # Larger particles\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'left':\n",
        "            # Left side engine particles\n",
        "            base_pos = (lander['body'].position[0] - 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = -np.random.uniform(0.2, 0.5)  # Gentler horizontal thrust\n",
        "                vel_y = np.random.uniform(-0.05, 0.05)  # Minimal vertical component\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(8, 18),\n",
        "                    'size': np.random.uniform(1.2, 2.5)\n",
        "                })\n",
        "\n",
        "        elif engine_type == 'right':\n",
        "            # Right side engine particles\n",
        "            base_pos = (lander['body'].position[0] + 0.2, lander['body'].position[1])\n",
        "\n",
        "            for _ in range(num_particles):\n",
        "                vel_x = np.random.uniform(0.2, 0.5)  # Gentler horizontal thrust\n",
        "                vel_y = np.random.uniform(-0.05, 0.05)  # Minimal vertical component\n",
        "\n",
        "                self.particles.append({\n",
        "                    'pos': [base_pos[0], base_pos[1]],\n",
        "                    'vel': [vel_x, vel_y],\n",
        "                    'color': (255, 200, 0),\n",
        "                    'lifetime': np.random.randint(8, 18),\n",
        "                    'size': np.random.uniform(1.2, 2.5)\n",
        "                })\n",
        "\n",
        "    def _destroy(self):\n",
        "        if self.world is None:\n",
        "            return\n",
        "\n",
        "        for lander in self.landers:\n",
        "            self.world.DestroyBody(lander['body'])\n",
        "            for leg in lander['legs']:\n",
        "                self.world.DestroyBody(leg)\n",
        "\n",
        "        if self.moon:\n",
        "            self.world.DestroyBody(self.moon)\n",
        "\n",
        "        self.world = None\n",
        "        self.landers = []\n",
        "        self.particles = []\n",
        "\n",
        "    def _create_terrain(self):\n",
        "        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]))\n",
        "        self.moon.CreateFixture(\n",
        "            shape=edgeShape(vertices=[(0, 0), (self.VIEWPORT_W / self.SCALE, 0)]),\n",
        "            friction=0.1,\n",
        "            restitution=0.0,\n",
        "        )\n",
        "        self.ground = self.moon\n",
        "\n",
        "        self.terrain = []\n",
        "        height = self.TERRAIN_HEIGHT\n",
        "        chunk_x = [0]\n",
        "        for i in range(self.TERRAIN_CHUNKS):\n",
        "            chunk_x.append(chunk_x[-1] + self.TERRAIN_STEP)\n",
        "\n",
        "        # Generate smoother terrain with Perlin-like noise\n",
        "        height_data = []\n",
        "\n",
        "        # Initial random heights\n",
        "        base_heights = []\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            h = np.random.uniform(0, height)\n",
        "            base_heights.append(h)\n",
        "\n",
        "        # Apply smoothing to get more natural, curved terrain\n",
        "        # Use a simple moving average for smoothing\n",
        "        window_size = 3\n",
        "        smoothed_heights = []\n",
        "\n",
        "\n",
        "        # Pad the array for smoothing\n",
        "        padded = [base_heights[0]] * (window_size//2) + base_heights + [base_heights[-1]] * (window_size//2)\n",
        "\n",
        "        for i in range(len(base_heights)):\n",
        "            window = padded[i:i+window_size]\n",
        "            smoothed_heights.append(sum(window) / len(window))\n",
        "\n",
        "        height_data = smoothed_heights\n",
        "\n",
        "        landing_pad_1_idx = self.TERRAIN_CHUNKS // 4\n",
        "        landing_pad_2_idx = 3 * self.TERRAIN_CHUNKS // 4\n",
        "\n",
        "        pad_height = self.TERRAIN_HEIGHT/4\n",
        "        pad_width = 3\n",
        "\n",
        "        # Flatten landing pads\n",
        "        for i in range(landing_pad_1_idx - pad_width//2, landing_pad_1_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        for i in range(landing_pad_2_idx - pad_width//2, landing_pad_2_idx + pad_width//2 + 1):\n",
        "            if 0 <= i < len(height_data):\n",
        "                height_data[i] = pad_height\n",
        "\n",
        "        pad1_left = (landing_pad_1_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad1_right = (landing_pad_1_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        pad2_left = (landing_pad_2_idx - pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "        pad2_right = (landing_pad_2_idx + pad_width//2) * self.TERRAIN_STEP * self.VIEWPORT_W / self.SCALE\n",
        "\n",
        "        self.safe_zones = [(pad1_left, pad1_right), (pad2_left, pad2_right)]\n",
        "\n",
        "        for i in range(len(chunk_x)-1):\n",
        "            x1 = chunk_x[i]\n",
        "            x2 = chunk_x[i+1]\n",
        "            h1 = height_data[i]\n",
        "\n",
        "            poly = [\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, h1),\n",
        "                (x2 * self.VIEWPORT_W / self.SCALE, 0),\n",
        "                (x1 * self.VIEWPORT_W / self.SCALE, 0)\n",
        "            ]\n",
        "\n",
        "            self.terrain.append((poly,\n",
        "                               (landing_pad_1_idx - pad_width//2 <= i <= landing_pad_1_idx + pad_width//2) or\n",
        "                               (landing_pad_2_idx - pad_width//2 <= i <= landing_pad_2_idx + pad_width//2)))\n",
        "\n",
        "            self.moon.CreateFixture(\n",
        "                shape=polygonShape(vertices=poly),\n",
        "                friction=0.4,\n",
        "                restitution=0.0\n",
        "            )\n",
        "\n",
        "    def _create_lander(self, agent, initial_x):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "\n",
        "        init_x = initial_x + np.random.uniform(-0.3, 0.3)\n",
        "        init_y = self.VIEWPORT_H / self.SCALE * 0.9  # Start higher\n",
        "\n",
        "        # Create a triangle-shaped lander\n",
        "        # Create a triangle-shaped lander with the point facing upward\n",
        "        lander_body = self.world.CreateDynamicBody(\n",
        "            position=(init_x, init_y),\n",
        "            angle=0.0,  # Start with zero angle for stability\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(vertices=[\n",
        "                    (-0.4, 0.0),   # Left base point\n",
        "                    (0.4, 0.0),    # Right base point\n",
        "                    (0.0, 0.8)     # Top point (triangle peak pointing up)\n",
        "                    ]),\n",
        "            density=5.0,\n",
        "            friction=0.1,\n",
        "            categoryBits=0x0010,\n",
        "            maskBits=0x001,\n",
        "            restitution=0.0\n",
        "                ),\n",
        "        )\n",
        "\n",
        "        # Reduce initial velocity for more stability\n",
        "        lander_body.linearVelocity.Set(\n",
        "            np.random.uniform(-0.5, 0.5),  # Reduced horizontal velocity\n",
        "            np.random.uniform(-0.5, 0.0)   # Reduced vertical velocity\n",
        "        )\n",
        "\n",
        "        # Set angular velocity to zero to prevent rotation\n",
        "        lander_body.angularVelocity = 0.0\n",
        "\n",
        "        # Add angular damping to resist rotation\n",
        "        lander_body.angularDamping = 10.0\n",
        "\n",
        "        # Add linear damping for smoother movement\n",
        "        lander_body.linearDamping = 0.2\n",
        "\n",
        "        legs = []\n",
        "        legs_contact = [False, False]\n",
        "\n",
        "        for i in [-1, 1]:\n",
        "            # Create fixed-angle legs that always point downward\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(init_x + i * 0.3, init_y - 0.1),\n",
        "                angle=0.0,  # Keep legs straight down\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(vertices=[\n",
        "                        (0, 0),\n",
        "                        (0, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, -self.LEG_H / self.SCALE),\n",
        "                        (i * self.LEG_W / self.SCALE, 0)\n",
        "                    ]),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    friction=0.2,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            legs.append(leg)\n",
        "\n",
        "            # Use a prismatic joint instead of revolute joint to keep legs aligned vertically\n",
        "            pjd = Box2D.b2PrismaticJointDef(\n",
        "                bodyA=lander_body,\n",
        "                bodyB=leg,\n",
        "                localAxisA=(0, 1),\n",
        "                localAnchorA=(i * 0.25, 0.0),\n",
        "                localAnchorB=(0, 0),\n",
        "                referenceAngle=0.0,\n",
        "                enableLimit=True,\n",
        "                lowerTranslation=-0.1,\n",
        "                upperTranslation=0.1\n",
        "            )\n",
        "            self.world.CreateJoint(pjd)\n",
        "\n",
        "        self.landers.append({\n",
        "            'body': lander_body,\n",
        "            'legs': legs,\n",
        "            'legs_contact': legs_contact,\n",
        "            'main_engine': False,\n",
        "            'side_engine_left': False,\n",
        "            'side_engine_right': False,\n",
        "            'on_safe_zone': False,\n",
        "            'crashed': False,\n",
        "            'outside_bounds': False\n",
        "        })\n",
        "\n",
        "    def _get_observation(self, agent):\n",
        "        idx = self.agent_name_mapping[agent]\n",
        "        lander = self.landers[idx]\n",
        "        pos = lander['body'].position\n",
        "        vel = lander['body'].linearVelocity\n",
        "\n",
        "        safe_zone_center_x = (self.safe_zones[idx][0] + self.safe_zones[idx][1]) / 2\n",
        "        safe_zone_width = self.safe_zones[idx][1] - self.safe_zones[idx][0]\n",
        "\n",
        "        dist_to_safe_zone = (pos[0] - safe_zone_center_x) / (self.VIEWPORT_W / self.SCALE)\n",
        "\n",
        "        # Add wind force to observation\n",
        "        observation = np.array([\n",
        "            (pos[0] - self.VIEWPORT_W / self.SCALE / 2) / (self.VIEWPORT_W / self.SCALE / 2),\n",
        "            (pos[1] - self.VIEWPORT_H / self.SCALE / 2) / (self.VIEWPORT_H / self.SCALE / 2),\n",
        "            vel[0] / 5.0,\n",
        "            vel[1] / 5.0,\n",
        "            lander['body'].angle,\n",
        "            lander['body'].angularVelocity / 5.0,\n",
        "            1.0 if lander['legs_contact'][0] else 0.0,\n",
        "            1.0 if lander['legs_contact'][1] else 0.0,\n",
        "            dist_to_safe_zone,\n",
        "            safe_zone_width / (self.VIEWPORT_W / self.SCALE),\n",
        "            1.0 if lander['on_safe_zone'] else 0.0,\n",
        "            self.wind_direction * self.wind_force  # Include wind information\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def _init_renderer(self):\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "            pygame.display.set_caption(\"Dual Lunar Lander\")\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return None\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            self._init_renderer()\n",
        "\n",
        "        surf = self.screen if self.render_mode == \"human\" else pygame.Surface((self.VIEWPORT_W, self.VIEWPORT_H))\n",
        "        surf.fill(self.sky_color)\n",
        "\n",
        "        def transform(pos):\n",
        "            return (int(pos[0] * self.SCALE), self.VIEWPORT_H - int(pos[1] * self.SCALE))\n",
        "\n",
        "        # Draw terrain\n",
        "        for poly, is_safe_zone in self.terrain:\n",
        "            transformed_poly = [transform(p) for p in poly]\n",
        "            color = self.safe_zone_color if is_safe_zone else self.terrain_color\n",
        "            pygame.draw.polygon(surf, color, transformed_poly)\n",
        "\n",
        "        # Draw landing pads\n",
        "        for left, right in self.safe_zones:\n",
        "            h = self.TERRAIN_HEIGHT/4\n",
        "            pad_poly = [\n",
        "                transform((left, h)),\n",
        "                transform((right, h)),\n",
        "                transform((right, h-0.05)),\n",
        "                transform((left, h-0.05))\n",
        "            ]\n",
        "            pygame.draw.polygon(surf, (255, 255, 255), pad_poly)\n",
        "\n",
        "        # Draw particles - render before landers for proper layering\n",
        "        for particle in self.particles:\n",
        "            pos = transform(particle['pos'])\n",
        "            color = particle['color']\n",
        "            # Fade out particles as they age\n",
        "            alpha = int(255 * (particle['lifetime'] / self.particle_lifetime))\n",
        "            color = (color[0], color[1], min(255, color[2] + int(alpha/2)))\n",
        "            size = max(1, int(particle['size']))\n",
        "            pygame.draw.circle(surf, color, pos, size)\n",
        "\n",
        "        # Draw landers\n",
        "        for i, lander in enumerate(self.landers):\n",
        "            lander_color = self.lander_color[i]\n",
        "\n",
        "            # Draw body\n",
        "            vertices = []\n",
        "            for fixture in lander['body'].fixtures:\n",
        "                for vertex in fixture.shape.vertices:\n",
        "                    vertices.append(transform(lander['body'].transform * vertex))\n",
        "            pygame.draw.polygon(surf, lander_color, vertices)\n",
        "\n",
        "            # Draw legs\n",
        "            for leg in lander['legs']:\n",
        "                vertices = []\n",
        "                for fixture in leg.fixtures:\n",
        "                    for vertex in fixture.shape.vertices:\n",
        "                        vertices.append(transform(leg.transform * vertex))\n",
        "                pygame.draw.polygon(surf, self.leg_color, vertices)\n",
        "\n",
        "            # Draw engine effects\n",
        "            if lander['main_engine']:\n",
        "                start_pos = transform(lander['body'].position + (0, -0.3))\n",
        "                flame_length = self.MAIN_ENGINE_THRUST_SCALE + np.random.randint(0, 5)\n",
        "                end_pos = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length\n",
        "                )\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 3)\n",
        "\n",
        "                # Draw inner flame (white)\n",
        "                inner_end = (\n",
        "                    start_pos[0] + np.sin(lander['body'].angle) * flame_length * 0.6,\n",
        "                    start_pos[1] - np.cos(lander['body'].angle) * flame_length * 0.6\n",
        "                )\n",
        "                pygame.draw.line(surf, (255, 255, 255), start_pos, inner_end, 2)\n",
        "\n",
        "            if lander['side_engine_left']:\n",
        "                start_pos = transform(lander['body'].position + (-0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] - flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "            if lander['side_engine_right']:\n",
        "                start_pos = transform(lander['body'].position + (0.2, 0))\n",
        "                flame_length = 8 + np.random.randint(0, 3)\n",
        "                end_pos = (start_pos[0] + flame_length, start_pos[1])\n",
        "                pygame.draw.line(surf, self.engine_color, start_pos, end_pos, 2)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "            return None\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "            self.isopen = False\n",
        "\n",
        "class SB3Wrapper(gym.Env):\n",
        "    \"\"\"Custom Gym wrapper for Stable Baselines3 compatibility\"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_space = self.env.observation_space(self.agents[0])\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([obs_space.low]*2),\n",
        "            high=np.concatenate([obs_space.high]*2),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, info = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), info\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to dict\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "class StableBaselinesWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    Converts ParallelEnv to Gym Env for Stable Baselines3 compatibility\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.agents = self.env.possible_agents\n",
        "\n",
        "        # Action space: MultiDiscrete for both agents\n",
        "        self.action_space = spaces.MultiDiscrete(\n",
        "            [self.env.action_space(agent).n for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Observation space: Combined observations\n",
        "        obs_spaces = [self.env.observation_space(agent) for agent in self.agents]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.concatenate([space.low for space in obs_spaces]),\n",
        "            high=np.concatenate([space.high for space in obs_spaces]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Important: Properly set metadata and render_mode\n",
        "        self.metadata = self.env.metadata\n",
        "        self.render_mode = env.render_mode\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs_dict, _ = self.env.reset(**kwargs)\n",
        "        return self._flatten_obs(obs_dict), {}\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Convert array to action dictionary\n",
        "        action_dict = {agent: actions[i] for i, agent in enumerate(self.agents)}\n",
        "        obs_dict, rewards, terms, truncs, infos = self.env.step(action_dict)\n",
        "\n",
        "        # Convert to Gym step format\n",
        "        return (\n",
        "            self._flatten_obs(obs_dict),\n",
        "            sum(rewards.values()),\n",
        "            all(terms.values()),\n",
        "            False,\n",
        "            {'individual_rewards': rewards, 'terminations': terms, 'truncations': truncs}\n",
        "        )\n",
        "\n",
        "    def _flatten_obs(self, obs_dict):\n",
        "        return np.concatenate([obs_dict[agent] for agent in self.agents])\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "def train_agents(total_timesteps=100000, render_every=20000, log_dir=\"./logs\"):\n",
        "    # Create and wrap environment\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    # Ensure render_mode is properly set\n",
        "    if not hasattr(wrapped_env, 'render_mode') or wrapped_env.render_mode != \"rgb_array\":\n",
        "        wrapped_env.render_mode = \"rgb_array\"\n",
        "\n",
        "    vec_env = DummyVecEnv([lambda: wrapped_env])\n",
        "\n",
        "    # Setup video recording\n",
        "    video_folder = os.path.join(log_dir, \"videos\")\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    # Modified video recording setup - removed reset_on_done parameter\n",
        "    vec_env = VecVideoRecorder(\n",
        "        vec_env,\n",
        "        video_folder,\n",
        "        record_video_trigger=lambda x: x % render_every == 0,\n",
        "        video_length=1000,  # Length in frames\n",
        "        name_prefix=\"dual_lander\"\n",
        "        # The reset_on_done parameter doesn't exist in this version of VecVideoRecorder\n",
        "    )\n",
        "\n",
        "    # Create and train model with slower learning rate and more exploration\n",
        "    model = PPO(\n",
        "        MlpPolicy,\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        tensorboard_log=log_dir,\n",
        "        learning_rate=5e-5,  # Even slower learning rate for more gradual improvement\n",
        "        n_steps=4096,        # Longer n_steps to capture full landing-takeoff cycles\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.999,        # Higher gamma for even more long-term focus\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.02,      # More entropy for more exploration\n",
        "        policy_kwargs={\"net_arch\": [dict(pi=[256, 256, 128], vf=[256, 256, 128])]}  # Deeper network\n",
        "    )\n",
        "\n",
        "    # Force training to run the full number of timesteps\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save(os.path.join(log_dir, \"dual_lander_model\"))\n",
        "    return model\n",
        "\n",
        "# Fixed record_model_videos function\n",
        "def record_model_videos(model, num_videos=5, frames_per_video=1500, output_dir=\"./videos\"):\n",
        "    \"\"\"\n",
        "    Records videos of the trained model performing multiple episodes\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create environment with rendering\n",
        "    env = DualLunarLander(render_mode=\"rgb_array\")\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "    vec_env = DummyVecEnv([lambda: wrapped_env])\n",
        "\n",
        "    for i in range(num_videos):\n",
        "        video_path = os.path.join(output_dir, f\"dual_lander_demo_{i}.mp4\")\n",
        "\n",
        "        # Create a video recorder without reset_on_done parameter\n",
        "        video_env = VecVideoRecorder(\n",
        "            vec_env,\n",
        "            output_dir,\n",
        "            record_video_trigger=lambda x: x == 0,  # Start recording immediately\n",
        "            video_length=frames_per_video,\n",
        "            name_prefix=f\"dual_lander_demo_{i}\"\n",
        "        )\n",
        "\n",
        "        obs = video_env.reset()\n",
        "        done = [False]\n",
        "        for _ in range(frames_per_video):\n",
        "            if done[0]:\n",
        "                obs = video_env.reset()\n",
        "\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = video_env.step(action)\n",
        "\n",
        "        video_env.close()\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "# Modify the evaluate_model function to show slower approaches\n",
        "def evaluate_model(model, num_episodes=5, render=True):\n",
        "    # Create environment with rendering\n",
        "    env = DualLunarLander(render_mode=\"human\" if render else None)\n",
        "    wrapped_env = StableBaselinesWrapper(env)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = wrapped_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "        max_steps = 2000  # Set a high max steps to allow for longer episodes\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = wrapped_env.step(action)\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if render:\n",
        "                env.render()  # Render original environment\n",
        "\n",
        "        print(f\"Episode {episode+1} Total Reward: {total_reward}, Steps: {step_count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training dual lunar landers...\")\n",
        "    model = train_agents()\n",
        "\n",
        "    # Evaluate with rendering\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    evaluate_model(model, num_episodes=5, render=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCnAOxJvx37L",
        "outputId": "bdcb4f9b-36ee-423e-b401-3cfaa24be5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dual lunar landers...\n",
            "Using cpu device\n",
            "Logging to ./logs/PPO_12\n",
            "Saving video to /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-0-to-step-1000.mp4\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 172  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 23   |\n",
            "|    total_timesteps | 4096 |\n",
            "-----------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 199           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 41            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.8230645e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -0.000163     |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.72e+06      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000362     |\n",
            "|    value_loss           | 3.45e+06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 208           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 58            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3195226e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0169        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.63e+06      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000224     |\n",
            "|    value_loss           | 3.22e+06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 216           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 75            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.7175057e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0213        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.86e+06      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000119     |\n",
            "|    value_loss           | 3.65e+06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 214           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 95            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3770932e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0224        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.43e+06      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00026      |\n",
            "|    value_loss           | 2.96e+06      |\n",
            "-------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-20000-to-step-21000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-20000-to-step-21000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-20000-to-step-21000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-20000-to-step-21000.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 202          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.186354e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.0264       |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.83e+06     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000323    |\n",
            "|    value_loss           | 3.67e+06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 206           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 138           |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1433091e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -0.000129     |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 8.82e+07      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000148     |\n",
            "|    value_loss           | 2.47e+08      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 210           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 155           |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6044756e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0353        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.5e+06       |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000272     |\n",
            "|    value_loss           | 2.91e+06      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 212          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 173          |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.304552e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.0349       |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.67e+06     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.000194    |\n",
            "|    value_loss           | 3.38e+06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 209          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 195          |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.415833e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.0332       |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.73e+06     |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000211    |\n",
            "|    value_loss           | 3.56e+06     |\n",
            "------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-40000-to-step-41000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-40000-to-step-41000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-40000-to-step-41000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-40000-to-step-41000.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 206          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 218          |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.336232e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.00432      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.77e+06     |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000211    |\n",
            "|    value_loss           | 3.61e+06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 208           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 235           |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.8314694e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -0.000226     |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.04e+08      |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000105     |\n",
            "|    value_loss           | 2.47e+08      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 209          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 253          |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.811054e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -0.0001      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.21e+08     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -5.29e-05    |\n",
            "|    value_loss           | 2.49e+08     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 211           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 271           |\n",
            "|    total_timesteps      | 57344         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2381656e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0463        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.75e+06      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000166     |\n",
            "|    value_loss           | 3.54e+06      |\n",
            "-------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-60000-to-step-61000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-60000-to-step-61000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-60000-to-step-61000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-60000-to-step-61000.mp4\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 205          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 299          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.143026e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -0.000188    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 8.77e+07     |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | 1.66e-05     |\n",
            "|    value_loss           | 2.39e+08     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 206           |\n",
            "|    iterations           | 16            |\n",
            "|    time_elapsed         | 317           |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8521154e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0419        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.79e+06      |\n",
            "|    n_updates            | 150           |\n",
            "|    policy_gradient_loss | -0.000371     |\n",
            "|    value_loss           | 3.5e+06       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 207           |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 334           |\n",
            "|    total_timesteps      | 69632         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3382742e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.044         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.61e+06      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.000211     |\n",
            "|    value_loss           | 3.4e+06       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 209          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 352          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.695446e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.0415       |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.77e+06     |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.000198    |\n",
            "|    value_loss           | 3.59e+06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 210           |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 369           |\n",
            "|    total_timesteps      | 77824         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4368314e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0566        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.54e+06      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -0.000207     |\n",
            "|    value_loss           | 3.07e+06      |\n",
            "-------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-80000-to-step-81000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-80000-to-step-81000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-80000-to-step-81000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-80000-to-step-81000.mp4\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 205           |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 399           |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0574538e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -9.33e-05     |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.38e+08      |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -0.000249     |\n",
            "|    value_loss           | 2.34e+08      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 206          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 416          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.399816e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.0597       |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.72e+06     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000127    |\n",
            "|    value_loss           | 3.37e+06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 207           |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 434           |\n",
            "|    total_timesteps      | 90112         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8309045e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0578        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.46e+06      |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -0.000225     |\n",
            "|    value_loss           | 3.12e+06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 208           |\n",
            "|    iterations           | 23            |\n",
            "|    time_elapsed         | 451           |\n",
            "|    total_timesteps      | 94208         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5728554e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0665        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.75e+06      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -0.000198     |\n",
            "|    value_loss           | 3.48e+06      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 210          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 467          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.123038e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | -0.00017     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.78e+08     |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -4.49e-05    |\n",
            "|    value_loss           | 3.05e+08     |\n",
            "------------------------------------------\n",
            "Saving video to /content/logs/videos/dual_lander-step-100000-to-step-101000.mp4\n",
            "Moviepy - Building video /content/logs/videos/dual_lander-step-100000-to-step-101000.mp4.\n",
            "Moviepy - Writing video /content/logs/videos/dual_lander-step-100000-to-step-101000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/logs/videos/dual_lander-step-100000-to-step-101000.mp4\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 205           |\n",
            "|    iterations           | 25            |\n",
            "|    time_elapsed         | 497           |\n",
            "|    total_timesteps      | 102400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8031718e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0.0638        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.65e+06      |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | -0.000278     |\n",
            "|    value_loss           | 3.14e+06      |\n",
            "-------------------------------------------\n",
            "\n",
            "Evaluating trained model...\n",
            "Episode 1 Total Reward: -4913.210958392552, Steps: 187\n",
            "Episode 2 Total Reward: -27704.18572882086, Steps: 393\n",
            "Episode 3 Total Reward: 6077.525461252112, Steps: 76\n",
            "Episode 4 Total Reward: 2602.5464465780233, Steps: 100\n",
            "Episode 5 Total Reward: 5696.731098831193, Steps: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Velocity values from 0 to 2.0\n",
        "velocities = np.linspace(0, 2.0, 100)\n",
        "\n",
        "# Calculate rewards for different landing velocities\n",
        "very_safe_vel = 0.2\n",
        "safe_vel = 0.6\n",
        "\n",
        "# Calculate reward components\n",
        "speed_bonus = np.array([80 * (1 - min(v / very_safe_vel, 1)) if v <= very_safe_vel\n",
        "                        else 0 for v in velocities])\n",
        "\n",
        "# Base reward is 150 for safe landing, plus speed bonus\n",
        "total_reward_safe = np.array([150 + sb if v <= safe_vel else 0 for v, sb in zip(velocities, speed_bonus)])\n",
        "\n",
        "# Distance from center reward component (simulated)\n",
        "distances = np.linspace(0, 1.0, 100)  # Normalized distance from center\n",
        "center_bonus = 40 * (1 - distances)\n",
        "\n",
        "# Plot velocity rewards\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(velocities, total_reward_safe, 'b-', linewidth=3, label='Total Landing Reward')\n",
        "plt.plot(velocities, speed_bonus, 'g--', linewidth=2, label='Speed Bonus Component')\n",
        "plt.axvline(x=very_safe_vel, color='r', linestyle=':', label=f'Very Safe Velocity ({very_safe_vel})')\n",
        "plt.axvline(x=safe_vel, color='r', linestyle='--', label=f'Safe Velocity Limit ({safe_vel})')\n",
        "plt.xlabel('Landing Velocity')\n",
        "plt.ylabel('Reward Points')\n",
        "plt.title('Reward Structure for Landing Velocity')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot distance rewards\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(distances, center_bonus, 'r-', linewidth=3)\n",
        "plt.xlabel('Normalized Distance from Center')\n",
        "plt.ylabel('Center Bonus Points')\n",
        "plt.title('Reward Structure for Landing Position')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('reward_structure.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Reward structure graph created and saved as 'reward_structure.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "122JuGtpHY0d",
        "outputId": "68b56c97-90d1-4e93-ee2f-59d25d6dc347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward structure graph created and saved as 'reward_structure.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Simulate training data\n",
        "total_timesteps = 100000\n",
        "x = np.arange(0, total_timesteps, 1000)\n",
        "\n",
        "# Simulated reward curve with initial exploration and gradual improvement\n",
        "# Using a sigmoid-shaped curve with noise to simulate learning progress\n",
        "def reward_curve(x, noise_level=0.1):\n",
        "    max_reward = 400  # Maximum achievable reward\n",
        "    min_reward = -100  # Starting reward level\n",
        "    mid_point = 40000  # Where the steepest learning happens\n",
        "    steepness = 20000  # How quickly learning happens\n",
        "\n",
        "    base = min_reward + (max_reward - min_reward) / (1 + np.exp(-(x - mid_point) / steepness))\n",
        "    noise = np.random.normal(0, noise_level * (max_reward - min_reward), size=len(x))\n",
        "    return base + noise\n",
        "\n",
        "# Generate noisy data for episode rewards\n",
        "rewards = reward_curve(x)\n",
        "smoothed_rewards = np.convolve(rewards, np.ones(5)/5, mode='valid')\n",
        "smoothed_x = x[2:-2]  # Adjust x for the convolution window\n",
        "\n",
        "# Plot episode rewards\n",
        "plt.plot(x, rewards, 'b-', alpha=0.3, label='Episode Rewards')\n",
        "plt.plot(smoothed_x, smoothed_rewards, 'r-', linewidth=2, label='Smoothed Rewards')\n",
        "\n",
        "# Add annotations for key training phases\n",
        "plt.annotate('Exploration Phase', xy=(10000, reward_curve(np.array([10000]))[0]),\n",
        "             xytext=(5000, reward_curve(np.array([10000]))[0] - 100),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "plt.annotate('Rapid Learning', xy=(40000, reward_curve(np.array([40000]))[0]),\n",
        "             xytext=(30000, reward_curve(np.array([40000]))[0] - 70),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "plt.annotate('Fine-tuning', xy=(80000, reward_curve(np.array([80000]))[0]),\n",
        "             xytext=(65000, reward_curve(np.array([80000]))[0] + 50),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "plt.xlabel('Training Timesteps')\n",
        "plt.ylabel('Episode Reward')\n",
        "plt.title('Simulated Training Progress for Dual Lunar Lander')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curve.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Training curve graph created and saved as 'training_curve.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdhzwN3VHczh",
        "outputId": "a22c9d4b-3c2e-4db1-8416-538810548936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training curve graph created and saved as 'training_curve.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Simulation parameters\n",
        "time_steps = np.arange(0, 500)\n",
        "gravity = -10 / 50  # Gravity per step\n",
        "engine_power = 8.0 * 0.7 / 50  # Reduced main engine power per step\n",
        "wind_force = 0.05 / 50  # Wind force per step\n",
        "initial_height = 12\n",
        "initial_vel_y = -0.5\n",
        "\n",
        "# Simulate different control strategies\n",
        "def simulate_trajectory(control_strategy, wind_direction=1):\n",
        "    height = np.zeros(len(time_steps))\n",
        "    vel_y = np.zeros(len(time_steps))\n",
        "    pos_x = np.zeros(len(time_steps))\n",
        "    vel_x = np.zeros(len(time_steps))\n",
        "    fuel_used = np.zeros(len(time_steps))\n",
        "\n",
        "    height[0] = initial_height\n",
        "    vel_y[0] = initial_vel_y\n",
        "    pos_x[0] = 0\n",
        "    vel_x[0] = 0.1  # Small initial horizontal velocity\n",
        "\n",
        "    for t in range(1, len(time_steps)):\n",
        "        # Apply control strategy\n",
        "        engine_on = control_strategy(t, height[t-1], vel_y[t-1])\n",
        "        thrust = engine_power if engine_on else 0\n",
        "\n",
        "        # Apply physics\n",
        "        wind_effect = wind_force * wind_direction\n",
        "        vel_x[t] = vel_x[t-1] + wind_effect\n",
        "        pos_x[t] = pos_x[t-1] + vel_x[t]\n",
        "\n",
        "        vel_y[t] = vel_y[t-1] + gravity + thrust\n",
        "        height[t] = max(0, height[t-1] + vel_y[t])\n",
        "\n",
        "        # Track fuel usage\n",
        "        fuel_used[t] = fuel_used[t-1] + (1 if engine_on else 0)\n",
        "\n",
        "        # Check if landed\n",
        "        if height[t] == 0:\n",
        "            # Fill the rest of the arrays with the final values\n",
        "            height[t:] = 0\n",
        "            vel_y[t:] = 0\n",
        "            pos_x[t:] = pos_x[t]\n",
        "            vel_x[t:] = 0\n",
        "            fuel_used[t:] = fuel_used[t]\n",
        "            break\n",
        "\n",
        "    return height, vel_y, pos_x, vel_x, fuel_used\n",
        "\n",
        "# Different control strategies\n",
        "def aggressive_strategy(t, h, v):\n",
        "    # Uses engine aggressively when falling\n",
        "    return v < -0.2\n",
        "\n",
        "def conservative_strategy(t, h, v):\n",
        "    # Uses engine more gently, allowing more fall before correcting\n",
        "    return v < -0.4 or (h < 2 and v < -0.1)\n",
        "\n",
        "def optimal_strategy(t, h, v):\n",
        "    # Calculates ideal thrust to achieve soft landing\n",
        "    stopping_distance = -v**2 / (2 * gravity)\n",
        "    return h <= stopping_distance * 1.1 or (h < 1 and v < -0.1)\n",
        "\n",
        "def bang_bang_strategy(t, h, v):\n",
        "    # Alternates between full thrust and no thrust\n",
        "    if t % 10 < 5:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Run simulations\n",
        "h1, v1, x1, vx1, f1 = simulate_trajectory(aggressive_strategy)\n",
        "h2, v2, x2, vx2, f2 = simulate_trajectory(conservative_strategy)\n",
        "h3, v3, x3, vx3, f3 = simulate_trajectory(optimal_strategy)\n",
        "h4, v4, x4, vx4, f4 = simulate_trajectory(bang_bang_strategy)\n",
        "\n",
        "# Plot height vs time\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(time_steps, h1, 'r-', label='Aggressive')\n",
        "plt.plot(time_steps, h2, 'g-', label='Conservative')\n",
        "plt.plot(time_steps, h3, 'b-', label='Optimal')\n",
        "plt.plot(time_steps, h4, 'y-', label='Bang-Bang')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Height')\n",
        "plt.title('Lander Height vs Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot vertical velocity vs time\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(time_steps, v1, 'r-', label='Aggressive')\n",
        "plt.plot(time_steps, v2, 'g-', label='Conservative')\n",
        "plt.plot(time_steps, v3, 'b-', label='Optimal')\n",
        "plt.plot(time_steps, v4, 'y-', label='Bang-Bang')\n",
        "plt.axhline(y=-0.2, color='k', linestyle='--', label='Safe Velocity')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Vertical Velocity')\n",
        "plt.title('Vertical Velocity vs Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot horizontal position vs time (affected by wind)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(time_steps, x1, 'r-', label='Aggressive')\n",
        "plt.plot(time_steps, x2, 'g-', label='Conservative')\n",
        "plt.plot(time_steps, x3, 'b-', label='Optimal')\n",
        "plt.plot(time_steps, x4, 'y-', label='Bang-Bang')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Horizontal Position')\n",
        "plt.title('Wind Drift Effect (Horizontal Position)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot fuel usage\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(time_steps, f1, 'r-', label='Aggressive')\n",
        "plt.plot(time_steps, f2, 'g-', label='Conservative')\n",
        "plt.plot(time_steps, f3, 'b-', label='Optimal')\n",
        "plt.plot(time_steps, f4, 'y-', label='Bang-Bang')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Fuel Used (Thrust Time)')\n",
        "plt.title('Fuel Consumption by Strategy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('environment_dynamics.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Environment dynamics graph created and saved as 'environment_dynamics.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFySTR0jHcnK",
        "outputId": "8b59cfa8-54d1-4d23-8606-6f9ce9d28bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment dynamics graph created and saved as 'environment_dynamics.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "# Parameters from the environment\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400\n",
        "SCALE = 30.0\n",
        "TERRAIN_CHUNKS = 25\n",
        "TERRAIN_HEIGHT = VIEWPORT_H / SCALE / 4\n",
        "TERRAIN_STEP = 1.0 / TERRAIN_CHUNKS\n",
        "\n",
        "# Create terrain using similar logic to the environment\n",
        "chunk_x = np.linspace(0, VIEWPORT_W / SCALE, TERRAIN_CHUNKS + 1)\n",
        "height_data = []\n",
        "\n",
        "# Generate base heights with random noise\n",
        "np.random.seed(42)  # For reproducibility\n",
        "base_heights = np.random.uniform(0, TERRAIN_HEIGHT, TERRAIN_CHUNKS)\n",
        "\n",
        "# Apply smoothing\n",
        "window_size = 3\n",
        "padded = np.pad(base_heights, (window_size//2, window_size//2), mode='edge')\n",
        "smoothed_heights = np.array([np.mean(padded[i:i+window_size]) for i in range(len(base_heights))])\n",
        "\n",
        "# Create landing pads\n",
        "landing_pad_1_idx = TERRAIN_CHUNKS // 4\n",
        "landing_pad_2_idx = 3 * TERRAIN_CHUNKS // 4\n",
        "pad_height = TERRAIN_HEIGHT/4\n",
        "pad_width = 3\n",
        "\n",
        "# Flatten landing pads\n",
        "for i in range(landing_pad_1_idx - pad_width//2, landing_pad_1_idx + pad_width//2 + 1):\n",
        "    if 0 <= i < len(smoothed_heights):\n",
        "        smoothed_heights[i] = pad_height\n",
        "\n",
        "for i in range(landing_pad_2_idx - pad_width//2, landing_pad_2_idx + pad_width//2 + 1):\n",
        "    if 0 <= i < len(smoothed_heights):\n",
        "        smoothed_heights[i] = pad_height\n",
        "\n",
        "# Calculate safe zones\n",
        "pad1_left = (landing_pad_1_idx - pad_width//2) * TERRAIN_STEP * VIEWPORT_W / SCALE\n",
        "pad1_right = (landing_pad_1_idx + pad_width//2) * TERRAIN_STEP * VIEWPORT_W / SCALE\n",
        "\n",
        "pad2_left = (landing_pad_2_idx - pad_width//2) * TERRAIN_STEP * VIEWPORT_W / SCALE\n",
        "pad2_right = (landing_pad_2_idx + pad_width//2) * TERRAIN_STEP * VIEWPORT_W / SCALE\n",
        "\n",
        "safe_zones = [(pad1_left, pad1_right), (pad2_left, pad2_right)]\n",
        "\n",
        "# Terrain polygon\n",
        "terrain_x = np.repeat(chunk_x[:-1], 2)\n",
        "terrain_x = np.append(terrain_x, chunk_x[-1])\n",
        "terrain_y = np.repeat(smoothed_heights, 2)\n",
        "terrain_y = np.append(terrain_y, smoothed_heights[-1])\n",
        "\n",
        "# Create terrain array for plotting\n",
        "terrain_filled_x = np.concatenate([terrain_x, [chunk_x[-1], chunk_x[0]]])\n",
        "terrain_filled_y = np.concatenate([terrain_y, [0, 0]])\n",
        "\n",
        "# Plot terrain\n",
        "plt.fill(terrain_filled_x, terrain_filled_y, color='#787878', alpha=0.8)\n",
        "\n",
        "# Highlight safe landing zones\n",
        "for i, (left, right) in enumerate(safe_zones):\n",
        "    plt.fill_between([left, right], [0], [pad_height], color='green', alpha=0.4)\n",
        "    plt.plot([left, right], [pad_height, pad_height], 'w-', linewidth=2)\n",
        "    plt.text((left + right) / 2, pad_height + 0.1, f\"Zone {i+1}\",\n",
        "             horizontalalignment='center', color='white', fontweight='bold')\n",
        "\n",
        "# Draw simulated lander initial positions\n",
        "lander1_x = (safe_zones[0][0] + safe_zones[0][1]) / 2 + np.random.uniform(-1, 1)\n",
        "lander2_x = (safe_zones[1][0] + safe_zones[1][1]) / 2 + np.random.uniform(-1, 1)\n",
        "lander_y = VIEWPORT_H / SCALE * 0.9\n",
        "\n",
        "plt.scatter([lander1_x, lander2_x], [lander_y, lander_y], color=['white', 'yellow'],\n",
        "            s=100, marker='v', edgecolors='black', linewidth=1, zorder=10,\n",
        "            label='Lander Initial Positions')\n",
        "\n",
        "# Draw wind arrows\n",
        "arrow_y = VIEWPORT_H / SCALE * 0.7\n",
        "for x in np.linspace(2, VIEWPORT_W / SCALE - 2, 8):\n",
        "    plt.arrow(x, arrow_y, 0.5, 0, head_width=0.2, head_length=0.1,\n",
        "              fc='cyan', ec='blue', alpha=0.6)\n",
        "\n",
        "plt.text(VIEWPORT_W / SCALE / 2, arrow_y + 0.5, \"Wind Direction\",\n",
        "         horizontalalignment='center', color='cyan', fontweight='bold')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X Position')\n",
        "plt.ylabel('Height')\n",
        "plt.title('Dual Lunar Lander Terrain and Landing Zones')\n",
        "\n",
        "# Set axis limits\n",
        "plt.xlim(0, VIEWPORT_W / SCALE)\n",
        "plt.ylim(0, VIEWPORT_H / SCALE * 0.95)\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add a legend\n",
        "legend_elements = [\n",
        "    Rectangle((0, 0), 1, 1, color='green', alpha=0.4, label='Safe Landing Zones'),\n",
        "    Rectangle((0, 0), 1, 1, color='#787878', alpha=0.8, label='Terrain')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('terrain_visualization.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Terrain visualization created and saved as 'terrain_visualization.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUOmT_4_HcU6",
        "outputId": "2f8fe30d-b970-4391-f56c-44da817bf402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terrain visualization created and saved as 'terrain_visualization.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define the custom colormap for policy visualization\n",
        "colors = [(0.8, 0.8, 0.8), (0.3, 0.3, 1.0), (1.0, 0.6, 0.0), (0.0, 0.8, 0.0)]\n",
        "n_bins = 4\n",
        "cmap_name = 'action_cmap'\n",
        "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "\n",
        "# Create a grid of state values for visualization\n",
        "height_range = np.linspace(0, 10, 100)\n",
        "velocity_range = np.linspace(-4, 1, 100)\n",
        "X, Y = np.meshgrid(height_range, velocity_range)\n",
        "\n",
        "# Simulate different policy regions\n",
        "# (0: No action, 1: Main engine, 2: Left engine, 3: Right engine)\n",
        "\n",
        "# Optimal policy for vertical control (simplified)\n",
        "def optimal_policy(height, velocity):\n",
        "    # When to use main engine based on velocity and height\n",
        "    if velocity < -2:\n",
        "        # Always fire if falling too fast\n",
        "        return 1\n",
        "    elif velocity < -1:\n",
        "        # Fire if falling moderately fast and getting close to ground\n",
        "        return 1 if height < 5 else 0\n",
        "    elif velocity < -0.5:\n",
        "        # Fire if falling slowly but very close to ground\n",
        "        return 1 if height < 2 else 0\n",
        "    else:\n",
        "        # No need to fire if moving upward or very slow downward\n",
        "        return 0\n",
        "\n",
        "# Simulate horizontal control policy\n",
        "def horizontal_policy(pos_x, target_x, vel_x):\n",
        "    dist = target_x - pos_x\n",
        "\n",
        "    # Need to move right\n",
        "    if dist > 0.5 and vel_x < 0.5:\n",
        "        return 3  # Right engine\n",
        "    # Need to move left\n",
        "    elif dist < -0.5 and vel_x > -0.5:\n",
        "        return 2  # Left engine\n",
        "    # Need to slow down right movement\n",
        "    elif vel_x > 0.3:\n",
        "        return 2  # Left engine\n",
        "    # Need to slow down left movement\n",
        "    elif vel_x < -0.3:\n",
        "        return 3  # Right engine\n",
        "    # Otherwise, don't use side engines\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Create policy matrices\n",
        "vertical_policy = np.zeros_like(X)\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "        vertical_policy[i, j] = optimal_policy(X[i, j], Y[i, j])\n",
        "\n",
        "# Horizontal policy visualization\n",
        "pos_range = np.linspace(-10, 10, 100)\n",
        "vel_range = np.linspace(-2, 2, 100)\n",
        "X2, Y2 = np.meshgrid(pos_range, vel_range)\n",
        "target_x = 0  # Target position at center\n",
        "\n",
        "horizontal_policy = np.zeros_like(X2)\n",
        "for i in range(X2.shape[0]):\n",
        "    for j in range(X2.shape[1]):\n",
        "        horizontal_policy[i, j] = horizontal_policy(X2[i, j], target_x, Y2[i, j])\n",
        "\n",
        "# Plot vertical control policy\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.contourf(X, Y, vertical_policy, levels=4, cmap=cm, alpha=0.8)\n",
        "plt.colorbar(ticks=[0, 1, 2, 3], label='Action')\n",
        "plt.contour(X, Y, vertical_policy, levels=[0.5, 1.5, 2.5], colors='k', linestyles='-')\n",
        "plt.xlabel('Height')\n",
        "plt.ylabel('Vertical Velocity')\n",
        "plt.title('Vertical Control Policy')\n",
        "\n",
        "# Annotate policy regions\n",
        "plt.text(5, -3, \"Use Main Engine\", fontsize=10, ha='center')\n",
        "plt.text(8, -0.5, \"No Action\", fontsize=10, ha='center')\n",
        "\n",
        "# Plot horizontal control policy\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.contourf(X2, Y2, horizontal_policy, levels=4, cmap=cm, alpha=0.8)\n",
        "plt.colorbar(ticks=[0, 1, 2, 3], label='Action')\n",
        "plt.contour(X2, Y2, horizontal_policy, levels=[0.5, 1.5, 2.5], colors='k', linestyles='-')\n",
        "plt.xlabel('Horizontal Position')\n",
        "plt.ylabel('Horizontal Velocity')\n",
        "plt.title('Horizontal Control Policy')\n",
        "\n",
        "# Annotate policy regions\n",
        "plt.text(-5, 0, \"Use Right Engine\", fontsize=10, ha='center')\n",
        "plt.text(5, 0, \"Use Left Engine\", fontsize=10, ha='center')\n",
        "plt.text(0, 1.5, \"No Action\", fontsize=10, ha='center')\n",
        "\n",
        "# Simulate action probabilities for a trained PPO agent (conceptual)\n",
        "def ppo_action_probs(height, vel_y, pos_x, vel_x):\n",
        "    # Simulate probabilistic policy learned by PPO\n",
        "    # This is a simplified approximation of what a learned policy might look like\n",
        "\n",
        "    # Base probabilities\n",
        "    probs = np.array([0.25, 0.25, 0.25, 0.25])\n",
        "\n",
        "    # Adjust based on vertical dynamics\n",
        "    if vel_y < -1.0:  # Falling fast\n",
        "        probs[1] += 0.4  # Increase main engine probability\n",
        "    elif height < 2.0 and vel_y < 0:  # Close to ground and falling\n",
        "        probs[1] += 0.5  # Heavily prefer main engine\n",
        "\n",
        "    # Adjust based on horizontal dynamics\n",
        "    if pos_x < -0.5:  # Too far left\n",
        "        probs[3] += 0.3  # Increase right engine probability\n",
        "    elif pos_x > 0.5:  # Too far right\n",
        "        probs[2] += 0.3  # Increase left engine probability\n",
        "\n",
        "    # Normalize\n",
        "    return probs / probs.sum()\n",
        "\n",
        "# Create a grid of example states\n",
        "states = [\n",
        "    {\"name\": \"High & Falling Fast\", \"height\": 9, \"vel_y\": -3, \"pos_x\": 0, \"vel_x\": 0},\n",
        "    {\"name\": \"Near Ground\", \"height\": 1, \"vel_y\": -0.5, \"pos_x\": 0, \"vel_x\": 0},\n",
        "    {\"name\": \"Drifting Right\", \"height\": 5, \"vel_y\": -1, \"pos_x\": 2, \"vel_x\": 0.5},\n",
        "    {\"name\": \"Ascending\", \"height\": 4, \"vel_y\": 1, \"pos_x\": 0, \"vel_x\": 0},\n",
        "    {\"name\": \"Safe Landing Speed\", \"height\": 0.5, \"vel_y\": -0.1, \"pos_x\": 0, \"vel_x\": 0}\n",
        "]\n",
        "\n",
        "# Plot action probabilities for example states\n",
        "plt.subplot(2, 1, 2)\n",
        "bar_width = 0.15\n",
        "index = np.arange(4)  # 4 actions\n",
        "colors = ['gray', 'blue', 'orange', 'green']\n",
        "action_names = ['No Action', 'Main Engine', 'Left Engine', 'Right Engine']\n",
        "\n",
        "for i, state in enumerate(states):\n",
        "    probs = ppo_action_probs(state[\"height\"], state[\"vel_y\"], state[\"pos_x\"], state[\"vel_x\"])\n",
        "    offset = (i - len(states)/2 + 0.5) * bar_width\n",
        "    plt.bar(index + offset, probs, bar_width, label=state[\"name\"], color=colors, alpha=0.7)\n",
        "\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Action Probability')\n",
        "plt.title('PPO Action Probabilities for Different States')\n",
        "plt.xticks(index, action_names)\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('action_policy_analysis.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Action policy analysis graph created and saved as 'action_policy_analysis.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "d8Bfsl27HcRW",
        "outputId": "d8569d13-805c-4905-fd78-a8d4ec0cca35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.ndarray' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-d8fec71ff074>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mhorizontal_policy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhorizontal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Plot vertical control policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3[extra]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2Bs777qsQQs",
        "outputId": "2f06b77a-0652-49fd-8629-0199f43f18e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable_baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (5.29.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIh5xvZBshZT",
        "outputId": "46c5ead8-8d6a-4eba-c740-a7867dab872c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJCYIBl6w4pD",
        "outputId": "dee1202d-75e1-414b-fed5-a4ed72f4b01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supersuit in /usr/local/lib/python3.11/dist-packages (3.9.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.0.0)\n",
            "Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.2.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWQBKwtaU6Uc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}